<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>LLM —— Tokenize | 一直进步 做喜欢的</title><meta name="keywords" content="Machine Learning,Deep Learning,LLM"><meta name="author" content="贪钱算法还我头发"><meta name="copyright" content="贪钱算法还我头发"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="LLM&#x2F;VLLM词元化技术介绍">
<meta property="og:type" content="article">
<meta property="og:title" content="LLM —— Tokenize">
<meta property="og:url" content="https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/index.html">
<meta property="og:site_name" content="一直进步 做喜欢的">
<meta property="og:description" content="LLM&#x2F;VLLM词元化技术介绍">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/cover.jpeg">
<meta property="article:published_time" content="2025-08-03T11:50:59.000Z">
<meta property="article:modified_time" content="2025-08-03T14:42:11.514Z">
<meta property="article:author" content="贪钱算法还我头发">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Deep Learning">
<meta property="article:tag" content="LLM">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/cover.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'LLM —— Tokenize',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-08-03 22:42:11'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><link rel="stylesheet" href="/css/custom.css?v1"><link rel="stylesheet" href="//at.alicdn.com/t/font_2264842_b004iy0kk2b.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">62</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2025/08/03/LLM-Tokenize/cover.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一直进步 做喜欢的</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">LLM —— Tokenize</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2025-08-03T11:50:59.000Z" title="Created 2025-08-03 19:50:59">2025-08-03</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-08-03T14:42:11.514Z" title="Updated 2025-08-03 22:42:11">2025-08-03</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Machine-Learning-and-Deep-Learning/">Machine Learning and Deep Learning</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">2.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>8min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="LLM —— Tokenize"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2025/08/03/LLM-Tokenize/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><h2 id="词元化技术简介"><a href="#词元化技术简介" class="headerlink" title="词元化技术简介"></a>词元化技术简介</h2><p>词元化（Tokenization）设计的目的是将原始文本分割为模型可识别的词元序列作为大语言模型的输入。传统基于词汇的分词（如中文分词）存在结果不一致性，比如某些中文分词中相同的输入可能产生不同的分词结果，导致词表庞大且含大量低频词，还可能面临<strong>未登录词（OOV）</strong>问题。分词器的设计需要注意以下几点：</p>
<ul>
<li><strong>具备无损重构特性</strong>：分词结果必须能准确还原原始文本。  </li>
<li><strong>高压缩率（Compression Ratio）</strong>：分词处理后词元数越少越好。压缩比计算公式为$$\text{压缩率} = \frac{\text{原始文本UTF-8字节数}}{\text{分词后的词元数}}$$</li>
<li><strong>领域适配性</strong>：需要适实际需求。比如通用分词器（如GPT-2）对多语言/多领域语料效果差（如LLaMA处理中文效率低）；通用BPE可能错误分割数字（如 <code>7,481 → [7, 481]</code>），需设计数字专用分词规则。 </li>
</ul>
<p>现有分词方法如<strong>字符级分词</strong>（如ELMo）以单个字符为最小单位。主流的LLM分词方法<strong>子词分词</strong>（Subword Tokenization）平衡了词表大小与OOV问题。具体实现时可以使用分词工具库SentencePiece（支持 BPE/ULM 算法），无需预分词直接处理原始文本，且可以统一处理多语言数据。下面介绍三种主流的子词分词方法。</p>
<h2 id="LLM主流子词分词方法"><a href="#LLM主流子词分词方法" class="headerlink" title="LLM主流子词分词方法"></a>LLM主流子词分词方法</h2><h3 id="BPE分词（Byte-Pair-Encoding）"><a href="#BPE分词（Byte-Pair-Encoding）" class="headerlink" title="BPE分词（Byte Pair Encoding）"></a>BPE分词（Byte Pair Encoding）</h3><p>1994年用于数据压缩，后适配至NLP领域。BPE初始化一个包含所有字符（如字母、边界符）的基础词表，然后统计相邻词元对的共现频率，迭代合并最高频词元对（如 <code>(&quot;o&quot;, &quot;o&quot;) → &quot;oo&quot;</code>），直至达到预设词表大小。Hugging Face中BPE的代码关键函数包括：  </p>
<ul>
<li><code>extract_frequencies()</code>：统计词频。  </li>
<li><code>frequency_of_pairs()</code>：计算词元对频率。  </li>
<li><code>merge_vocab()</code>：合并词元对并更新词表。  </li>
</ul>
<p>一种BPE的拓展是<strong>字节级BPE（Byte-level BPE）</strong>，基础单位为字节（基本词表大小=256）。这种方法解决了OOV问题，支持细粒度分割（如GPT-2、BART、LLaMA采用）。注意需要用<strong>Unicode标准化</strong>（如NFKC）预处理语料，避免特殊字符重复编码。</p>
<h3 id="WordPiece分词"><a href="#WordPiece分词" class="headerlink" title="WordPiece分词"></a>WordPiece分词</h3><p>WordPiece是谷歌内部算法，2018年被BERT采用。与BPE的区别是 1采用了不同的合并标准，训练一个language model对词元评分，然后选择使训练数据似然性最大化的词元对；2采用了不同的评分公式，不是选择最频繁的词对，WordPiece评分公式为： $$\text{得分} = \frac{\text{词对频率}}{\text{第一个词频率} \times \text{第二个词频率}}$$</p>
<h3 id="Unigram分词"><a href="#Unigram分词" class="headerlink" title="Unigram分词"></a>Unigram分词</h3><p>Unigram整体流程是相反的，从大初始词表开始迭代删除词元直至目标大小。基于<strong>一元语言模型</strong>评估删除词元对语料似然的影响。采用<strong>期望最大化（EM）算法</strong>：<strong>E步</strong>用动态规划算法维特比算法找最优分词，<strong>M步</strong>更新一元词概率，重复直至词表收敛。T5、mBART、ALBERT都使用了这种分词方式。</p>
<h2 id="VLLM分词技术简介"><a href="#VLLM分词技术简介" class="headerlink" title="VLLM分词技术简介"></a>VLLM分词技术简介</h2><p>在视觉大语言模型（Vision-Language Large Models, VLLMs）中，Tokenization 需同时处理文本和视觉输入，VLLM 的分词流程分为三部分：</p>
<ol>
<li><strong>文本分词</strong>（Text Tokenization）：沿用传统LLM分词技术，但需与视觉Token协同。子词分词主流方案包括<strong>BPE</strong>（GPT-4V, LLaVA和<strong>SentencePiece</strong>（PaLM-E, Gemini）。需要添加一些特殊Token如图像标记符<code>&lt;image&gt;</code> <code>[IMG]</code>和区域描述符<code>&lt;region_1&gt;</code> <code>&lt;bbox&gt;</code>等。词汇表大小通常为 <strong>32K-128K</strong>。</li>
<li><strong>图像分词</strong>（Image Tokenization）：将连续像素转换为离散Token序列，分为<strong>基于神经网络的特征提取器</strong>（如ViT patch embedding）和<strong>离散化表示学习</strong>（如VQ-VAE）两种类型。</li>
<li><strong>跨模态对齐</strong>（Cross-modal Alignment）：将文本Token与图像Token融合为统一表示。</li>
</ol>
<h3 id="图像分词方法"><a href="#图像分词方法" class="headerlink" title="图像分词方法"></a>图像分词方法</h3><h4 id="基于神经网络的特征提取器"><a href="#基于神经网络的特征提取器" class="headerlink" title="基于神经网络的特征提取器"></a>基于神经网络的特征提取器</h4><table>
<thead>
<tr>
<th>方法</th>
<th>原理</th>
<th>特点与模型示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>ViT Patch embedding</strong></td>
<td>将图像分割为固定大小patch线性投影为Token</td>
<td>• 序列长度：$H/16 \times W/16$ <br>• 代表模型：Flamingo patch为<code>16×16</code>, LLaVA patch为<code>14×14</code>, <strong>BLIP-2</strong></td>
</tr>
<tr>
<td><strong>CNN 特征图</strong></td>
<td>用ResNet等CNN提取多层特征图，展平为Token序列</td>
<td>• 保留空间层次结构 <br>• 代表模型：<strong>BLIP</strong>, <strong>VinVL</strong></td>
</tr>
<tr>
<td><strong>多尺度特征</strong></td>
<td>融合不同层级的CNN特征（如Faster R-CNN），生成区域级Token</td>
<td>• 适合目标检测任务 <br>• 代表模型：<strong>VL-T5</strong>, <strong>OFA</strong></td>
</tr>
</tbody></table>
<p>目前主流的VLLM的视觉Tokenize方法是<strong>ViT patch embedding</strong>。这里介绍3篇经典VLLM论文：</p>
<ul>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2301.12597">BLIP-2: Bootstrapping Language-Image Pre-training (ICML 2023)</a> BLIP-2两阶段预训练框架：使用预训练的Image Encoder（CLIP训练的ViT-L/14和EVA-CLIP训练的ViT-g/14）提取图像视觉特征，Q-Former（Querying Transformer）通过Learnable Queries 跨注意力机制从冻结的ViT中提取文本最相关的视觉表示，最后使用冻结的LLM（Decoder-based和Encoder-Decoder-based）生成文本。</p>
  <div align="center">
  <img src="BLIP2-1.png" height="100%" width="100%">
  </div>
  <div align="center">
  <img src="BLIP2-2.png" height="100%" width="100%">
  </div></li>
<li><p><a target="_blank" rel="noopener" href="https://www.adept.ai/blog/fuyu-8b">Fuyu-8B: A Multimodal Architecture for AI Agents(Adept AI, 2023)</a> Fuyu8B极简架构：取消视觉Encoder，直接线性投影图像patch，图像Token与文本Token并行输入标准的仅Decoder Transformer架构。</p>
  <div align="center">
  <img src="Fuyu8B.png" height="100%" width="100%">
  </div></li>
<li><p><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2304.08485">LLaVA: Large Language and Vision Assistant (NeurIPS 2023)</a> 使用一个简单的线性层将预训练CLIP视觉Encoder ViT-L/14特征映射为embedding，与语言指令的embedding共同输入LLM（Vicuna）得到最终的输出。关于LLaVA系列模型解读可以看<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/692398098">【知乎】LLaVA系列多模态大模型总结</a></p>
  <div align="center">
  <img src="LLaVA.png" height="100%" width="100%">
  </div></li>
</ul>
<h4 id="离散化表示学习"><a href="#离散化表示学习" class="headerlink" title="离散化表示学习"></a>离散化表示学习</h4><table>
<thead>
<tr>
<th>方法</th>
<th>原理</th>
<th>特点与模型示例</th>
</tr>
</thead>
<tbody><tr>
<td><strong>VQ-VAE</strong></td>
<td>学习一个离散码本（Codebook），将图像块映射到最近邻的离散码向量</td>
<td>• 生成压缩Token（如8×8→1 Token） <br>• 代表模型：DALL-E码本大小为<code>8192</code>, Parti码本大小为<code>16384</code></td>
</tr>
<tr>
<td><strong>VQ-GAN</strong></td>
<td>改进VQ-VAE，用GAN提升重建质量，生成更精细的视觉Token</td>
<td>• 支持高分辨率图像 <br>• 代表模型：<strong>CogView</strong>, <strong>NÜWA</strong></td>
</tr>
<tr>
<td><strong>Masked Token建模</strong></td>
<td>随机掩盖部分图像Token，训练模型重建（类似BERT的MLM）</td>
<td>• 提升表征鲁棒性 <br>• 代表模型：<strong>BEiT</strong>, <strong>PeCo</strong></td>
</tr>
</tbody></table>
<p>图像的Tokenize基础框架从VQ-VAE开始，发展出一系列提升重建质量和分辨率的扩展架构包括VQ-VAE2、RQ-VAE和FSQ等，同时也发展出生成式对抗化的形式。</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1312.6114"><strong>VAE</strong>（Variational Autoencoder）</a>：学习连续潜空间 $z \sim \mathcal{N}(\mu, \sigma^2)$，编码器 $q_\phi(z|x)$ 与解码器 $p_\theta(x|z)$，但是连续向量不适配Transformer的离散自回归生成。</li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1711.00937"><strong>VQ-VAE</strong>（Vector Quantized VAE）</a>：将隐变量$z$量化为可学习的码本 $\mathcal{C} = {e_1, e_2, …, e_K} \in \mathbb{R}^{K \times D}$，离散索引 $k = \text{argmin}_i | z_e - e_i |_2$。训练目标为$\mathcal{L} = |x - \hat{x}|^2 + | \text{sg}[z_e] - e_k |_2^2 + \beta | z_e - \text{sg}[e_k] |_2^2$（sg：梯度停止操作，$\beta$ 通常取0.25）。VQ-VAE生成 <strong>256x256</strong> 图像仅需 <strong>32x32=1024 Token</strong>（压缩率64倍），可适配Transformer自回归生成。  <div align="center">
  <img src="VQ-VAE.png" height="100%" width="100%">
  </div></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/1906.00446"><strong>VQ-VAE2</strong></a>：设计了层级结构，底层编码局部纹理，高层编码全局结构。高低层共享同一码本（$K=8192$）。首次实现了 <strong>1024x1024</strong> 高保真图像生成（BigGAN级质量）。  <div align="center">
  <img src="VQ-VAE2.png" height="100%" width="100%">
  </div></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2203.01941"><strong>RQ-VAE</strong>（Recurrent Quantized VAE）</a>：使用递归量化，即将单次VQ迭代扩展为多步残差量化$<br>z^{(0)} = E(x), \quad r^{(t)} = z^{(t-1)} - q(z^{(t-1)}), \quad z^{(t)} = E_r(r^{(t)})$。这样设计后码本利用率提升 <strong>3倍</strong>（相同重建质量下码本减小）    <div align="center">
  <img src="RQ-VAE.png" height="100%" width="100%">
  </div></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2309.15505"><strong>FSQ</strong>（Finite Scalar Quantization）</a>：直接量化潜向量为整数：$z_q = \text{round}(z_e / \Delta) \cdot \Delta$，码本隐含在量化步长 $\Delta$ 中。无需码本搜索，训练速度 <strong>快2.5倍</strong>，且支持任意维度离散化。$ \mathcal{L}_{FSQ} = | x - G(z_q) |^2 + \lambda | z_e - z_q |^2 $    <div align="center">
  <img src="FSQ.png" height="100%" width="100%">
  </div></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2012.09841"><strong>VQ-GAN</strong></a>：将VQ-VAE解码器升级为GAN，实现了重建的高频细节保留。相对VQ-VAE的像素级MSE损失，VQ-GAN引入Patch判别器 $D$ 和感知损失，目标函数为$ L_{VQGAN} = L_{VQ}+\lambda_{GAN} L_{GAN}+\lambda_{Percep} L_{Percep}$。VQ-VAE的码本更新仅使用了L2损失，VQ-GAN使用对抗梯度驱动。  <div align="center">
  <img src="VQ-GAN.png" height="100%" width="100%">
  </div></li>
</ul>
<blockquote>
<p>关于离散化表示学习的相关知识可以看：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/2433292582">【知乎】一文详解 codebook 技术史（从 VAE 到 VQ/RQ-VAE 到 FSQ）</a></li>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/699508908">【知乎】离散视觉tokenizer</a></li>
</ul>
</blockquote>
<h4 id="两种方法对比"><a href="#两种方法对比" class="headerlink" title="两种方法对比"></a>两种方法对比</h4><table>
<thead>
<tr>
<th><strong>维度</strong></th>
<th><strong>基于神经网络的特征提取器</strong></th>
<th><strong>离散化表示学习</strong></th>
</tr>
</thead>
<tbody><tr>
<td><strong>核心思想</strong></td>
<td>通过神经网络提取连续特征向量，输出高维浮点数向量（连续空间）</td>
<td>将图像映射到离散码本的索引，输出整数索引序列（离散空间）</td>
</tr>
<tr>
<td><strong>信息保留能力</strong></td>
<td>细节保留依赖网络参数量，高层语义和位置编码明确</td>
<td>量化损失细节，语义信息需要解码器重建，空间关系模糊</td>
</tr>
<tr>
<td><strong>多模态协同能力</strong></td>
<td>和文本融合天然适配（拼接或cross attention），需额外设计生成头适配跨模态生成</td>
<td>与文本融合需投影到连续空间，可直接复用语言模型解码</td>
</tr>
<tr>
<td><strong>计算效率</strong></td>
<td>编码速度适中，无需解码，序列一般较长</td>
<td>码本近邻搜索导致编码较慢，且需要解码器生成</td>
</tr>
<tr>
<td><strong>训练稳定性</strong></td>
<td>梯度稳定，端到端可微</td>
<td>码本更新存在梯度截断问题</td>
</tr>
<tr>
<td><strong>适用场景</strong></td>
<td>连续向量保留丰富语义信息，适用于图像描述生成、VQA等语义理解任务</td>
<td>因离散Token天然适配语言模型架构，适用自回归生成式预测和文生图、图像编辑等图像生成任务。VQ-VAE的序列压缩技术可用于将推荐广告领域中广告离散化为一串索引表示。</td>
</tr>
</tbody></table>
<h3 id="跨模态对齐策略"><a href="#跨模态对齐策略" class="headerlink" title="跨模态对齐策略"></a>跨模态对齐策略</h3><p>在进入LLM时代之前，<a target="_blank" rel="noopener" href="https://arxiv.org/abs/2102.03334#">ViLT</a>的paper中总结了多模态的框架如下（多模态论文总结详见之前的笔记<a href="https://xfliu1998.github.io/2022/09/17/Papers-Summary/">Papers Summary</a>）。关于VLLM中的跨模态对齐策略后续会详细学习。</p>
<div align="center">
  <img src="MultiModel.png" height="100%" width="100%">
</div>

</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">贪钱算法还我头发</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/">https://xfliu1998.github.io/2025/08/03/LLM-Tokenize/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Machine-Learning/">Machine Learning</a><a class="post-meta__tags" href="/tags/Deep-Learning/">Deep Learning</a><a class="post-meta__tags" href="/tags/LLM/">LLM</a></div><div class="post_share"><div class="social-share" data-image="/2025/08/03/LLM-Tokenize/cover.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2025/09/18/LLM-SFT/"><img class="prev-cover" src="/2025/09/18/LLM-SFT/cover.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">LLM —— SFT</div></div></a></div><div class="next-post pull-right"><a href="/2025/07/26/LLM-Position-Embedding/"><img class="next-cover" src="/2025/07/26/LLM-Position-Embedding/cover.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">LLM —— Position Embedding</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/10/24/LLM-RL4LLM/" title="LLM —— RL4LLM"><img class="cover" src="/2025/10/24/LLM-RL4LLM/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-10-24</div><div class="title">LLM —— RL4LLM</div></div></a></div><div><a href="/2025/09/18/LLM-SFT/" title="LLM —— SFT"><img class="cover" src="/2025/09/18/LLM-SFT/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-09-18</div><div class="title">LLM —— SFT</div></div></a></div><div><a href="/2025/07/26/LLM-Position-Embedding/" title="LLM —— Position Embedding"><img class="cover" src="/2025/07/26/LLM-Position-Embedding/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-07-26</div><div class="title">LLM —— Position Embedding</div></div></a></div><div><a href="/2025/06/21/LLM-Parameter-Calculation/" title="LLM —— Parameter Calculation"><img class="cover" src="/2025/06/21/LLM-Parameter-Calculation/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-06-21</div><div class="title">LLM —— Parameter Calculation</div></div></a></div><div><a href="/2025/05/05/LLM-MoE/" title="LLM —— MoE"><img class="cover" src="/2025/05/05/LLM-MoE/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-05-05</div><div class="title">LLM —— MoE</div></div></a></div><div><a href="/2025/04/06/LLM-KV-Cache/" title="LLM —— KV Cache"><img class="cover" src="/2025/04/06/LLM-KV-Cache/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-04-06</div><div class="title">LLM —— KV Cache</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">贪钱算法还我头发</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">62</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xfliu1998"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xfliu1998" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuxiaofei_7@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/fan-xu-15-35/posts" target="_blank" title="Zhihu"><i class="fa fa-address-card"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">👋你好呀，欢迎围观～搭建这个小站源于一个朴素的愿望：对抗遗忘，沉淀思考。期待在代码与逻辑的世界里探索技术的深度与广度，永远保持热情与好奇。</div></div><div class="card-widget" id="newYear"><div class="item-headline"><i></i><span></span></div><div class="item-content"><div id="newYear-main"><div class="mask"></div> <p class="title"></p> <div class="newYear-time"></div> <p class="today" style="text-align: right;"></p> </div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E8%AF%8D%E5%85%83%E5%8C%96%E6%8A%80%E6%9C%AF%E7%AE%80%E4%BB%8B"><span class="toc-number">1.</span> <span class="toc-text">词元化技术简介</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#LLM%E4%B8%BB%E6%B5%81%E5%AD%90%E8%AF%8D%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">LLM主流子词分词方法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BPE%E5%88%86%E8%AF%8D%EF%BC%88Byte-Pair-Encoding%EF%BC%89"><span class="toc-number">2.1.</span> <span class="toc-text">BPE分词（Byte Pair Encoding）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#WordPiece%E5%88%86%E8%AF%8D"><span class="toc-number">2.2.</span> <span class="toc-text">WordPiece分词</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Unigram%E5%88%86%E8%AF%8D"><span class="toc-number">2.3.</span> <span class="toc-text">Unigram分词</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VLLM%E5%88%86%E8%AF%8D%E6%8A%80%E6%9C%AF%E7%AE%80%E4%BB%8B"><span class="toc-number">3.</span> <span class="toc-text">VLLM分词技术简介</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9B%BE%E5%83%8F%E5%88%86%E8%AF%8D%E6%96%B9%E6%B3%95"><span class="toc-number">3.1.</span> <span class="toc-text">图像分词方法</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8"><span class="toc-number">3.1.1.</span> <span class="toc-text">基于神经网络的特征提取器</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%A6%BB%E6%95%A3%E5%8C%96%E8%A1%A8%E7%A4%BA%E5%AD%A6%E4%B9%A0"><span class="toc-number">3.1.2.</span> <span class="toc-text">离散化表示学习</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%A4%E7%A7%8D%E6%96%B9%E6%B3%95%E5%AF%B9%E6%AF%94"><span class="toc-number">3.1.3.</span> <span class="toc-text">两种方法对比</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%B7%A8%E6%A8%A1%E6%80%81%E5%AF%B9%E9%BD%90%E7%AD%96%E7%95%A5"><span class="toc-number">3.2.</span> <span class="toc-text">跨模态对齐策略</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/2025/08/03/LLM-Tokenize/cover.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 贪钱算法还我头发</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '1b0c10ce649501ea4a72',
      clientSecret: '741b5e861137e3d5a482bba272c8201b78da6cb0',
      repo: 'xfliu1998.github.io',
      owner: 'xfliu1998',
      admin: ['xfliu1998'],
      id: '363710f38585b0b3daefc112baa3fa84',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: true,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/script.js?v1"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><script async data-pjax src="/js/newYear.js"></script><script async src="//at.alicdn.com/t/font_2264842_b004iy0kk2b.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='all'|| 'all' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Machine-Learning-and-Deep-Learning/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习与深度学习 (18)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Structures-and-Algorithms/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">😼 数据结构与算法 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Search-Advertisement-Recommendation-Causal/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗂️ 搜索/广告/推荐/因果 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Analysis-and-Processing/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数据分析与处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Reading-Notes/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 阅读笔记 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Daily/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 日常随笔 (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://xfliu1998.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-NLP/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Reading-about-NLP/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-NLP/" alt="">Papers Reading about NLP</a><div class="blog-slider__text">自然语言处理论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-NLP/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/10/01/9-3D-Construction/" alt=""><img width="48" height="48" src="2022/10/01/9-3D-Construction/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-10-01</span><a class="blog-slider__title" href="2022/10/01/9-3D-Construction/" alt="">3D Construction</a><div class="blog-slider__text">三维重建基础</div><a class="blog-slider__button" href="2022/10/01/9-3D-Construction/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/01/11/SfM-SLAM/" alt=""><img width="48" height="48" src="2023/01/11/SfM-SLAM/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-11</span><a class="blog-slider__title" href="2023/01/11/SfM-SLAM/" alt="">SfM &amp; SLAM</a><div class="blog-slider__text">SfM和SLAM系统</div><a class="blog-slider__button" href="2023/01/11/SfM-SLAM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/03/25/Papers-Ideas/" alt=""><img width="48" height="48" src="2023/03/25/Papers-Ideas/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-25</span><a class="blog-slider__title" href="2023/03/25/Papers-Ideas/" alt="">Papers Ideas</a><div class="blog-slider__text">大模型时代下的科研思路</div><a class="blog-slider__button" href="2023/03/25/Papers-Ideas/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Summary/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Summary/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Summary/" alt="">Papers Summary</a><div class="blog-slider__text">论文总结笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Summary/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-CV/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Reading-about-CV/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-CV/" alt="">Papers Reading about CV</a><div class="blog-slider__text">计算机视觉论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-CV/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/06/05/Interview-Experience/" alt=""><img width="48" height="48" src="2023/06/05/Interview-Experience/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-05</span><a class="blog-slider__title" href="2023/06/05/Interview-Experience/" alt="">Interview Experience</a><div class="blog-slider__text">面经八股</div><a class="blog-slider__button" href="2023/06/05/Interview-Experience/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/08/16/Papers-Reading-about-LLM/" alt=""><img width="48" height="48" src="2024/08/16/Papers-Reading-about-LLM/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-08-16</span><a class="blog-slider__title" href="2024/08/16/Papers-Reading-about-LLM/" alt="">Papers Reading about LLM</a><div class="blog-slider__text">LLM论文阅读笔记</div><a class="blog-slider__button" href="2024/08/16/Papers-Reading-about-LLM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/01/21/Learning-Framework/" alt=""><img width="48" height="48" src="2022/01/21/Learning-Framework/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-21</span><a class="blog-slider__title" href="2022/01/21/Learning-Framework/" alt="">学习大纲</a><div class="blog-slider__text">目录</div><a class="blog-slider__button" href="2022/01/21/Learning-Framework/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<container><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></container>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.fomal.cc/api?xfliu1998",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xfliu1998')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>