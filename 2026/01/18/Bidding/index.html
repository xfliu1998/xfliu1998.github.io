<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Ad Bidding | 一直进步 做喜欢的</title><meta name="keywords" content="Search, Ads &amp; Reco"><meta name="author" content="贪钱算法还我头发"><meta name="copyright" content="贪钱算法还我头发"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="广告出价算法：从传统式到生成式">
<meta property="og:type" content="article">
<meta property="og:title" content="Ad Bidding">
<meta property="og:url" content="https://xfliu1998.github.io/2026/01/18/Bidding/index.html">
<meta property="og:site_name" content="一直进步 做喜欢的">
<meta property="og:description" content="广告出价算法：从传统式到生成式">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xfliu1998.github.io/2026/01/18/Bidding/cover.jpeg">
<meta property="article:published_time" content="2026-01-18T09:00:00.000Z">
<meta property="article:modified_time" content="2026-01-18T10:59:30.185Z">
<meta property="article:author" content="贪钱算法还我头发">
<meta property="article:tag" content="Search, Ads &amp; Reco">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xfliu1998.github.io/2026/01/18/Bidding/cover.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xfliu1998.github.io/2026/01/18/Bidding/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Ad Bidding',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2026-01-18 18:59:30'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><link rel="stylesheet" href="/css/custom.css?v1"><link rel="stylesheet" href="//at.alicdn.com/t/font_2264842_b004iy0kk2b.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">66</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2026/01/18/Bidding/cover.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一直进步 做喜欢的</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Ad Bidding</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2026-01-18T09:00:00.000Z" title="Created 2026-01-18 17:00:00">2026-01-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2026-01-18T10:59:30.185Z" title="Updated 2026-01-18 18:59:30">2026-01-18</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Search-Ads-Reco/">Search, Ads &amp; Reco</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">8.5k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>30min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Ad Bidding"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2026/01/18/Bidding/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p>👋计算广告中常见的出价模式包括CPM、CPA（Cost Per Acquisition，每次获取成本）、CPC等，相关介绍可以看👉<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1938485355159480262">【王喆】「深度学习计算广告」讲透互联网广告中的出价模式</a>。互联网中最常用的两种出价分别是MCB和oCPM，其中oCPM是主要的出价方式。本文将详细介绍关于MCB和oCPM的算法建模，以及目前工业界广告出价算法中传统算法（PID、期望最优）和生成式算法（DT、扩散模型）的落地应用。</p>
<h2 id="问题建模"><a href="#问题建模" class="headerlink" title="问题建模"></a>问题建模</h2><h3 id="MCB"><a href="#MCB" class="headerlink" title="MCB"></a>MCB</h3><p><strong>最大转化出价（Multiple Constrained Bidding, MCB）</strong>是一种预算约束下的多目标优化问题。广告主提供总预算 $B$ 和可选目标CPA，如每次转化目标成本为50元，不需要设置出价，所以也叫nobid，系统目标是在预算 $B$ 内最大化转化量 $Conv$，若设置需同时满足 $ CPA \leq CPA_{target} $。</p>
<p>MCB出价的建模如下：假设一个有 $N$ 个曝光机会顺序到达的场景，第 $i$ 个机会中广告主出价 $b$ 超过其他广告主则赢得曝光并产生成本 $c$。目标是总预算 $B$ 和广告主设置的第 $j$ 个约束上限 $C$ 下最大化赢得曝光总价值：<br>$$\sum_i o_i v_i$$</p>
<p>其中 $v$ 表示曝光价值，$o$ 是指广告主是否赢得曝光 $i$ 的二进制变量。约束条件如下，$p$ 可以是任何性能指标，如转化次数。<br>$$ \sum_i o_i c_i \leq B $$</p>
<p>$$ \frac{\sum_i c_{ij} o_i}{\sum_i p_{ij} o_i} \leq C_j $$</p>
<p><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/epdf/10.1145/3447548.3467199">A unified solution to constrained bidding in online display advertising</a> 中计算出MCB的出价最优解可以用最优出价参数 $\lambda_j$ 表示：<br>$$<br>b_i = \lambda_0 v_i + \sum_{j=1}^J \lambda_j p_{ij} C_j<br>$$</p>
<h3 id="oCPM"><a href="#oCPM" class="headerlink" title="oCPM"></a>oCPM</h3><p><strong>优化千次展示成本（Optimized Cost per Mille, oCMP）</strong>是一种以转化为目标的智能出价策略，广告主设定目标转化成本CPA，系统通过调整每次展示的出价，使得平均转化成本趋近于 $ CPA_{target} $，同时最大化转化量。</p>
<p>oCPM出价建模如下：理论出价可以通过出价系数 $\alpha_i$ 来校正预测偏差和竞争环境变化。<br>$$<br>b_i = CPA_{target} \cdot \hat{p}_{cvr}(i) \cdot \alpha_i<br>$$</p>
<p>oCPM涉及到深度学习模型对转化率 $ \hat{p}_{cvr}(i) $ 的预测和出价机制中 $ \alpha_i $ 的调整。不论是MCB还是oCPM，都需要注意预算平滑，将总预算分配到各个时间段避免提前花完或剩余过多。对于两种出价方式的选择：如果广告主优先考虑转化量，且对成本容忍度较高，或处于冷启动阶段选MCB。如果广告主有明确的成本要求，且已积累一定数据处于稳定期，系统能准确预测转化率选oCPM。在实际投放中，两种策略可结合使用，前期用MCB快速获取数据，后期转为oCPM精细控制成本。</p>
<h2 id="传统算法"><a href="#传统算法" class="headerlink" title="传统算法"></a>传统算法</h2><h3 id="PID"><a href="#PID" class="headerlink" title="PID"></a>PID</h3><p>PID是比例（Proportional）、积分（Integral）、微分（Derivative）控制的缩写，是一种经典的反馈控制算法。在oCPM出价中PID控制器通过调整出价公式中的 $\alpha$ 系数，使得实际转化成本CPA尽可能接近目标转化成本。</p>
<p>在oCPM出价场景中，我们设定目标 $ CPA_g $，然后实时监控实际 $CPA_a$。PID控制器根据误差 $e(t) = CPA_a(t) - CPA_g$ 来调整 $\alpha$ 系数，从而调整出价。在实际应用中通常会对误差进行归一化处理，使用相对误差：$e(t) = (CPA_a(t) - CPA_g) / CPA_g$。接下来，我们详细解释PID的三个分量如何影响 $\alpha$ 的调整：</p>
<ul>
<li>P 控制：与当前误差成比例，即调整量与当前误差成正比。如果成本偏高 e&gt;0 则降低 $\alpha$；如果成本偏低 e&lt;0 则提高 $\alpha$。比例控制能够快速响应误差，但单独使用可能无法消除稳态误差，即实际CPA与目标CPA之间的持久差距，且可能引起震荡。</li>
<li>I 控制：与误差的累积和成比例。积分控制可以消除稳态误差，因为它会不断累积误差并产生调整量，直到误差为零。但是积分控制可能使系统反应迟缓，并且可能引起超调，即实际CPA在目标值附近波动过大。</li>
<li>D 控制：与误差的变化率成比例。微分控制可以预测误差的未来趋势，从而提前进行抑制，有助于减少超调和震荡，提高系统的稳定性。</li>
</ul>
<p>在离散时间系统中，PID控制器的输出可以表示为：<br>$$\alpha(t) = Kp * e(t) + Ki * Σ e(τ) + Kd * [e(t) - e(t-1)]$$</p>
<p>但在oCPM出价中通常使用乘法调整，或者也可以采用加法调整，但乘法调整更符合出价公式的乘法结构。<br>$$\alpha(t) = \alpha(t-1) * (1 + Kp * e(t) + Ki * Σ e(τ) + Kd * [e(t) - e(t-1)])$$</p>
<p>实际应用中，往往只使用PI控制，去掉微分项，因为微分项对噪声敏感，而在广告系统中，转化成本本身波动较大，微分项可能引入不稳定性。</p>
<p>我们来看一个具体的调整过程：假设我们每隔一段时间比如1小时调整一次 $\alpha$。我们记录当前时间段内的实际 $CPA_a$ 和累计误差。</p>
<ul>
<li>S1: 计算当前误差 $e(t) = (CPA_a - CPA_g) / CPA_g$</li>
<li>S2: 计算误差的累积和：$sum_e(t) = sum_e(t-1) + e(t)$</li>
<li>S3: 计算误差的变化：$\delta_e(t) = e(t) - e(t-1)$</li>
<li>S4: 计算调整量：$adjust = Kp * e(t) + Ki * sum_e(t) + Kd * \delta_e(t)$</li>
<li>S5: 更新$\alpha$：$\alpha(t) = \alpha(t-1) * (1 + adjust)$，需要对 $\alpha$ 进行限幅，如限制在[0.5, 2.0]之间，避免出价过高或过低。</li>
</ul>
<p>PID出价的一些问题：</p>
<ul>
<li>延迟反馈：转化可能发生在点击之后很久，因此实际CPA的计算会有延迟。需要使用部分数据或预测数据来估计当前CPA。</li>
<li>非稳态：市场环境包括竞争、用户行为等不断变化，目标CPA可能也需要调整。</li>
<li>预算约束：在控制成本的同时还要考虑预算花光的速度，因此需要将预算考虑进调整中。</li>
</ul>
<h3 id="期望最优调价"><a href="#期望最优调价" class="headerlink" title="期望最优调价"></a>期望最优调价</h3><p>在广告投放系统中，成本控制是核心目标之一，尤其是在按CPA模式下。PID在转化量充足时表现良好，但在低转化场景下面临显著挑战：</p>
<ul>
<li>随机偏差放大：转化量少时，成本波动性高，PID依赖的历史误差信号本身噪声大；</li>
<li>局部优化局限：仅基于已发生数据调整，缺乏对全天投放过程的整体规划；</li>
<li>多目标难以平衡：成本、跑量、稳定性等多目标协同优化复杂；</li>
<li>参数敏感与泛化难：不同行业、账户、时段需差异化参数，调优成本高。</li>
</ul>
<p>期望最优调价策略的核心思想是：在任意决策时刻，基于已知信息与未来预估，求解使全天成本期望最优的出价系数。基于历史消耗与转化（含回流预估）<code>cost_hist</code>, <code>conv_hist</code> 以及未来消耗与转化的预估<code>cost_future</code>, <code>conv_future</code>，全天成本可表示为：<br>$$<br>CPA_{final} = \frac{\text{cost}<em>{\text{hist}} + \text{cost}</em>{future}}{\text{conv}<em>{\text{hist}} + \text{conv}</em>{\text{future}}}<br>$$</p>
<p>MPC未来消耗预估依赖两个关键函数：</p>
<ul>
<li>消耗时间分布函数 <code>F(t)</code>：表示在出价系数为1时，从0点到时刻 <code>t</code> 的累计消耗占全天总消耗的比例。可通过历史数据拟合，常用二次函数或分段线性函数。</li>
<li>出价系数与消耗关系模型：假设消耗与出价系数的 <code>k ≈ 2.5</code> 次方成正比：<br>$$<br>\frac{\text{cost}(\lambda_1)}{\text{cost}(\lambda_2)} = \left( \frac{\lambda_1}{\lambda_2} \right)^k<br>$$</li>
</ul>
<p>(1) 未来消耗预估：<br>$$<br>\text{cost}<em>{\text{future}}(\lambda)  = \frac{\text{cost}</em>{\text{hist}}}{\lambda_{\text{hist}}^k} \cdot \frac{1 - F(t)}{F(t)} \cdot \lambda^k<br>$$</p>
<p>(2) 未来成本预估：假设转化成本与出价成正比即<br>$$<br>CPA_{\text{future}}(\lambda)  = \frac{\lambda}{\lambda_{\text{hist}}} \cdot CPA_{\text{hist}} = \frac{\lambda}{\lambda_{\text{hist}}} \cdot \frac{cost_{hist}}{conv_{hist}}<br>$$</p>
<p>(3) 未来转化量预估：<br>$$<br>\text{conv}<em>{\text{future}}(\lambda) = \frac{\text{cost}</em>{\text{future}}}{CPA_{\text{future}}}<br>$$</p>
<p>期望最优调价系数求解：</p>
<ul>
<li>解析法：联立上式最小化成本偏差，对方程求导最终得到一个关于<code>λ</code>的k次方程，需要数值求解</li>
<li>遍历求解（最常用）：在 <code>λ</code> 的可行域如 <code>[0.2, 2.0]</code>内遍历搜索求解</li>
<li>考虑概率分布的优化目标：考虑未来转化的随机性，即未来真实转化量 <code>conv_real</code> 服从泊松分布，参数为 <code>conv_future</code>。则需要最大化达成概率<br>$<br>P\left(0.8 \cdot CPA_{\text{target}} \leq CPA_{\text{real}} \leq 1.2 \cdot CPA_{\text{target}}\right)<br>$，同样适用遍历求解。</li>
</ul>
<h2 id="强化学习算法——IQL"><a href="#强化学习算法——IQL" class="headerlink" title="强化学习算法——IQL"></a>强化学习算法——IQL</h2><p>传统广告出价算法的问题是其<strong>决策的局部性与静态性</strong>。它们通常将连续的竞价过程割裂为孤立的决策点，只优化单次竞价的即时收益，实际上单步决策最优不等于全局决策最优，传统出价算法忽略了出价作为一个序列决策问题的本质——当前的出价策略会影响未来的流量竞争态势、预算消耗节奏和用户转化路径。同时，这些方法高度依赖预设规则和手工调参，难以自动适应市场竞争、用户行为等环境的动态变化，在面对多目标复杂权衡和转化奖励延迟时显得僵化且短视。</p>
<p>强化学习RL优势在于其<strong>序列优化与自适应学习</strong>的能力。它将出价建模为一个马尔可夫决策过程（MDP），智能体通过与环境即竞价市场的交互，学习一个直接最大化长期累积奖励的策略。这使得RL能自然地进行跨时间步的全局优化，例如智能地规划预算消耗、为潜在高价值用户提前布局。更重要的是，RL策略能根据实时反馈数据自主演化，自动平衡成本与转化量的矛盾，并适应竞争格局的波动，从而实现一种动态、精细且前瞻性的出价智能，这是传统静态规则系统无法企及的。</p>
<h4 id="问题定义"><a href="#问题定义" class="headerlink" title="问题定义"></a>问题定义</h4><p>首先，我们将广告出价问题建模为一个MDP：</p>
<ul>
<li>状态空间 $S$：状态 $s_t$ 包含广告请求的上下文信息，即广告投放状态，包括预算类、消耗类、时间进度、历史成本和出价等。也可以考虑行业、产品、转化目标对应投放状态。</li>
<li>动作空间 $A$：动作定义为下一时刻的出价调整系数 $\alpha_t$，假设以eCPM形式出价，实际出价:<br>$$bid_t = CPA_{target} \times pCVR_t \times pCTR_t \times 1000 \times \alpha_t$$</li>
<li>奖励函数 $r(s_t, a_t)$：通常设计为多目标组合，比如是否转化、是否在目标CPA范围内、预算消耗进度的加权。简单点也可以直接将 $\sum{pctr*pcvr}$ 作为奖励，ROI类再乘pltv。</li>
<li>状态转移：由广告竞价环境决定，包括竞拍结果、用户反馈等。<br>$$s_{t+1} \sim P(s_{t+1}|s_t, a_t)$$</li>
</ul>
<p>我们的目标是学习一个策略 $π(a∣s)$，使得在给定预算约束下最大化累计奖励。</p>
<h4 id="IQL简介"><a href="#IQL简介" class="headerlink" title="IQL简介"></a>IQL简介</h4><p>在广告出价中，我们无法在线与环境大量交互，因为探索可能浪费广告预算，因此使用离线强化学习。IQL是一种离线RL算法，它可以在不与环境交互的情况下，仅使用离线数据集中已有的状态-动作对，不直接学习Q函数，通过期望回归（Expectile Regression）隐式地学习Q函数的上界来避免对未见过的动作的过高估计。结合优势加权行为克隆（Advantage-Weighted Regression, AWR）学习策略避免了分布偏移问题。</p>
<p>IQL的优化目标包括三个部分：</p>
<ul>
<li>状态价值函数 $V(s)$ 的学习：通过期望回归来隐式地学习Q函数的上分位数。</li>
<li>Q函数 $Q(s,a)$ 学习：使用标准的时序差分目标，但用 $V(s′)$ 代替 $max_{a′}Q(s<br>′,a′)$。</li>
<li>策略 $π(a∣s)$ 的学习：通过优势加权行为克隆从数据集中提取策略，即优化策略以最大化期望Q值，同时保持与行为策略的相似性。</li>
</ul>
<p>状态价值函数 $V(s)$ 的学习：不直接学习Q函数的最大值，而是学习一个介于期望和最大值之间的量。定义状态价值函数 $V(s)$ 的损失函数为：<br>$$L_V(ψ) = \mathbb{E}<em>{(s,a)∼D​} [L_2^τ​ (Q</em>{\hat{θ}}(s,a) − V_ψ​(s))]$$</p>
<p>其中，$L_2^τ$​ 是期望回归损失函数，定义为：<br>$$L_2^τ​ (u)=∣τ−1_{u&lt;0}​ ∣u^2$$</p>
<p>这里，$τ∈(0.5,1]$ 是一个超参数。当 $τ=0.5$ 时，就是最小二乘，$V(s)$ 会学习到 $Q(s,a)$ 的条件期望；当 $τ$ 接近1时，$V(s)$ 会学习到 $Q(s,a)$ 的条件上分位数，即更大的Q值。</p>
<p>直观理解：我们希望通过 $V(s)$ 来估计在状态 $s$ 下，数据集中存在的动作所能达到的Q值的上界。由于只使用数据集中存在的动作，我们避免了外推误差。</p>
<p>Q函数 $Q(s,a)$ 的学习采用标准的贝尔曼方程，但使用 $V(s′)$ 作为目标值，而不是 $max⁡_{a′}Q(s′,a′)$。损失函数为：<br>$$L_Q(θ) = \mathbb{E}_{(s,a,r,s′)∼D}[(r+γV_ψ(s′)−Q_θ(s,a))^2]$$</p>
<p>这里的目标值 $r+γV_ψ(s′)$ 只依赖于状态 $s′$ ，而不依赖于动作 $a′$ 。这意味着我们不需要在 $s′$ 处最大化Q函数，从而避免了在离线数据中因采取不同于数据收集策略的动作而导致的误差。</p>
<p>策略 $π(a∣s)$ 的学习通过优势加权行为克隆进行。首先定义优势函数：<br>$$A_{\hat{θ}}(s,a)=Q_{\hat{θ}}(s,a)−V_ψ(s)$$</p>
<p>这里的优势函数使用了之前学到的Q函数和V函数。策略的学习目标是最大化加权对数似然：<br>$$L_π(ϕ)=\mathbb{E}<em>{(s,a)∼D}[exp⁡(βA</em>{\hat{θ}}(s,a)) \space log \space ⁡π_ϕ(a∣s)]$$</p>
<p>其中，$β&gt;0$ 是温度参数。这个损失函数的意义是：对于数据集中每个状态-动作对，我们根据其优势值来加权。优势值越大，即该动作相对于状态价值的优势越大，在更新策略时给予的权重就越大。这样，策略会倾向于选择数据集中那些优势高的动作。</p>
<h4 id="IQL在广告出价中的建模过程"><a href="#IQL在广告出价中的建模过程" class="headerlink" title="IQL在广告出价中的建模过程"></a>IQL在广告出价中的建模过程</h4><p>数据收集：收集离线数据。数据来自历史广告日志，每条记录包括：状态$s$（特征如上所述），动作 $a$（历史策略使用的出价调整系数），奖励 $r$（根据广告效果计算的即时奖励），以及下一个状态。</p>
<p>训练过程：</p>
<ol>
<li>初始化：随机初始化Q网络参数 $θ$，V网络参数 $ψ$，策略网络参数 $ϕ$。</li>
<li>迭代更新：<ul>
<li>从数据集中采样一批数据 $(s,a,r,s′)$</li>
<li>计算Q值：$Q_θ(s,a)$</li>
<li>更新V网络：最小化 $L_V(ψ)$</li>
<li>更新Q网络：最小化 $L_Q(θ)$</li>
<li>更新策略网络：最小化 $L_π(ϕ)$</li>
</ul>
</li>
<li>重复直到收敛。</li>
</ol>
<p>策略部署：训练完成后，我们得到策略网络 $π_ϕ(a∣s)$。在线上出价时，对于每个广告请求，我们提取状态 $s$，然后使用策略网络给出动作 $a$，即出价调整系数，最后计算出价。</p>
<h4 id="为什么IQL适合建模出价问题？"><a href="#为什么IQL适合建模出价问题？" class="headerlink" title="为什么IQL适合建模出价问题？"></a>为什么IQL适合建模出价问题？</h4><ul>
<li>离线学习：广告出价系统通常有大量的历史数据，但在线交互学习成本高。在线RL需要在线交互，可能在探索过程中付出高昂代价。IQL可以从历史数据中学习，无需在线探索，降低了风险。</li>
<li>适合连续动作空间：广告出价问题中，出价系数是连续值。IQL通过优势加权行为克隆可以处理连续动作空间，而其他一些离线RL算法如BCQ、CQL也支持连续动作空间，但IQL在实验中被证明在连续控制任务上表现良好。</li>
<li>利用历史策略：许多离线RL算法如CQL、BRAC需要估计行为策略 $π$。IQL可以基于历史策略如PID的数据，利用高奖励轨迹学习到比历史策略更优的策略，从而提升出价效果。</li>
<li>稳定性：IQL通过期望回归的tau参数学习Q函数，隐式地将策略约束在数据分布附近，可以缓解分布偏移问题，tau参数可以控制保守程度，tau小 → 更保守，适应慢。tau大 → 更激进，适应快，通过使用V函数作为目标，避免了在离线数据中需要最大化Q函数时可能产生的对未见动作的高估，避免了外推误差，提高了学习的稳定性。</li>
</ul>
<h2 id="生成式出价"><a href="#生成式出价" class="headerlink" title="生成式出价"></a>生成式出价</h2><h3 id="Decision-Transformer-based"><a href="#Decision-Transformer-based" class="headerlink" title="Decision Transformer based"></a>Decision Transformer based</h3><h4 id="DT"><a href="#DT" class="headerlink" title="DT"></a>DT</h4><p>DT 摒弃了传统的马尔可夫决策过程框架，不通过价值函数或策略梯度来学习，而是将强化学习问题视为一个序列建模问题。输入是一个由历史“状态-动作-回报”三元组拼接而成的序列。像语言模型预测下一个词一样，自回归地预测序列中的下一个动作。通过剩余回报（Return To Go, RTG）这个条件变量来控制策略的“激进”或“保守”程度。RTG表示从当前时刻到轨迹结束，预期还能获得多少累计奖励。下面我们详细描述DT出价的建模过程：</p>
<p>我们的目标是训练一个 DT 模型 $DT_θ$ 使其能够根据历史轨迹和指定的“RTG”条件，生成合适的出价调整动作。</p>
<p>首先从线上日志中获取轨迹。每条轨迹由多个时间步的数据组成。序列长度通常被截断到一个固定的 $K$ 步如最近20步，以处理长序列并保持计算效率。对于每个时间步 $t$，我们需要构建输入序列，这个序列是一个按时间顺序拼接的“三元组”流：$$R_t, s_t, a_t, R_{t-1}, s_{t-1}, a_{t-1}, …, R_1, s_1, a_1$$。</p>
<p>模型结构使用带因果注意力掩码的标准的 Transformer Decoder 结构，确保当前时刻的预测只能看到过去的信息。输入编码序列中的每个 $R$, $s$, $a$ 都会被投影到统一的embedding空间，并加上位置编码以保留时间顺序信息。在输入序列后，模型的目标是预测出该时刻的正确动作，通常使用均方误差损失。模型学习到的是：在历史上下文且要求未来总回报达到 $R_t$ 的条件下，当前状态下应该采取什么动作。通过输入不同的 RTG 条件可以引导模型产生不同激进程度的出价行为，条件设得高模型会更“努力”去争取高价值曝光。</p>
<div align="center">
  <img src="DT.png" height=100% width=100%>
</div>

<h4 id="GAS-Generative-Auto-bidding-with-Post-training-Search"><a href="#GAS-Generative-Auto-bidding-with-Post-training-Search" class="headerlink" title="GAS: Generative Auto-bidding with Post-training Search"></a>GAS: Generative Auto-bidding with Post-training Search</h4><p>GAS中提到DT出价的问题：</p>
<ul>
<li>条件不匹配：DT依赖的“剩余回报”条件在数据中可能噪声很大，不能准确反映动作的真实价值。例如，一个好的动作后跟一个糟糕的未来动作可能导致的剩余回报很低。</li>
<li>偏好固化：一个DT模型通常只擅长一种奖励函数定义的偏好，通常被训练甚至偏向于模仿主流偏好，难以灵活适应广告主多样的 KPI 要求。</li>
</ul>
<p>GAS在一个预训练的基础策略模型DT上，训练多个Q网络，在搜索过程中，这些评判器负责给每个经过扰动base action得来的候选动作打分，采用Q-voting选择最终的动作，从而绕过了 DT 中可能不准确的“剩余回报”条件，直接根据目标偏好进行决策。</p>
<div align="center">
  <img src="GAS.png" height=100% width=100%>
</div>

<p>训练阶段：</p>
<ol>
<li>DT训练：使用离线数据训练基础策略模型 $$a_t \sim DT_θ(s_t, history, R_t)$$</li>
<li>QT训练：对于每种偏好，我们训练一组M个针对特定偏好的评判器 $${QT_{φ_k}}_{k=1:M}$$</li>
</ol>
<p>每个评判器是一个Transformer-based的Q网络，输入为长度为 $L$ 的历史状态-动作序列和当前状态-动作对，注意这里包含当前时刻t的状态和动作。输出为当前状态-动作对的Q值。QT采用IQL来训练，以避免对分布外动作的高估。每个评判器都是独立训练的，使用不同的随机初始化，从而得到略有差异的Q值估计。IQL引入一个额外的价值网络$V_ψ$，并通过期望回归损失来学习：<br>  $$L_V(ψ) = E_{(s,a)<del>D} [L_2^τ(Q_φ(s,a) - V_ψ(s))]$$<br>  $$L_2^τ(u) = |τ - I(u&lt;0)| u^2$$<br>其中 $τ$ 是超参数。Q网络的损失为：<br>  $$L_Q(φ) = E_{(s,a,s’)</del>D} [(r(s,a) + γ V_ψ(s’) - Q_φ(s,a))^2]$$</p>
<p>推理阶段在每一个时间步t，执行以下步骤：</p>
<ol>
<li>使用基础策略模型生成基础动作 $$a_t = DT_θ(history, s_t, R_t)$$</li>
<li>在基础动作周围通过随机扰动生成N个候选动作。$a_t^i = a_t * ε_i$, 其中$ε_i ~ Uniform(0.9, 1.1), i=1,…,N-1$。候选动作集合：${a_t^1, a_t^2, …, a_t^{N-1}, a_t}$</li>
<li>对于每个候选动作$a_t^i$，使用对应偏好的多个评判器（QT）计算Q值，然后通过Q-voting机制汇总得到每个动作的得分。<ul>
<li>对于每个评判器$QT_{φ_k}$，计算候选动作的Q值：$$Q_k^i = QT_{φ_k}(s_t, a_t^i; history)$$</li>
<li>对每个评判器，将N个候选动作的Q值进行min-max归一化，得到该评判器对每个动作的投票分数：$$v_k(a_t^i) = (Q_k^i - min_{n} Q_k^n) / (max_{n} Q_k^n - min_{n} Q_k^n)$$</li>
<li>对所有评判器，计算每个动作的总投票分数：$$v(a_t^i) = Σ_{k=1}^{M} v_k(a_t^i)$$</li>
</ul>
</li>
<li>选择总投票分数最高的动作作为最终执行的动作：$$a_t^* = argmax_{a_t^i} v(a_t^i)$$</li>
</ol>
<h4 id="GAVE-Generative-Auto-Bidding-with-Value-Guided-Explorations"><a href="#GAVE-Generative-Auto-Bidding-with-Value-Guided-Explorations" class="headerlink" title="GAVE: Generative Auto-Bidding with Value-Guided Explorations"></a>GAVE: Generative Auto-Bidding with Value-Guided Explorations</h4><p>GAVE解决的是离线自动出价中的三个难点：复杂目标、行为崩溃和OOD风险。因此GAVE基于DT引入3个模块：</p>
<ol>
<li>基于分数的RTG模块：解决目标对齐问题。传统方法用简单奖励，但实际广告有CPA约束，所以GAVE把约束变成可微分的评分函数，让模型直接优化业务指标，这样训练和评估就不会脱节。</li>
<li>动作探索模块：离线数据有限，模型容易学得保守。GAVE不是随机探索，而是用缩放系数生成新动作，再用RTG比较新旧动作的好坏，通过权重平衡更新，既探索又保持稳定。它不像强化学习那样依赖环境交互，而是在离线状态下安全地试探。</li>
<li>可学习的价值函数：针对OOD风险。探索的动作可能不靠谱，GAVE用期望回归学习RTG的上界，引导探索靠近潜在最优区域，避免跑偏。</li>
</ol>
<div align="center">
  <img src="GAVE.png" height=100% width=100%>
</div>

<h5 id="基于分数的RTG计算"><a href="#基于分数的RTG计算" class="headerlink" title="基于分数的RTG计算"></a>基于分数的RTG计算</h5><p>为处理CPA等约束，GAVE使用约束评分函数而非简单的价值累加，其中 $ S_t $ 是时间步t的累积约束评分，$ \gamma $ 是惩罚系数（论文中γ=2），$ r_t $ 是从时间步t到结束的剩余RTG。RTG反映了未来可能获得的评分，指导模型优化。<br>$$<br>\begin{aligned}<br>S_t &amp;= \mathbb{P}(CPA_t; C) \cdot \sum_{i}^{I_t} x_i v_i \<br>\text{其中} \quad \mathbb{P}(CPA_t; C) &amp;= \min\left{\left(\frac{C}{CPA_t}\right)^{\gamma}, 1\right} \<br>CPA_t &amp;= \frac{\sum_{i}^{I_t} x_i c_i}{\sum_{i}^{I_t} x_i v_i} \<br>r_t &amp;= S_T - S_{t-1}<br>\end{aligned}<br>$$</p>
<h5 id="动作探索机制"><a href="#动作探索机制" class="headerlink" title="动作探索机制"></a>动作探索机制</h5><p>在训练过程中，GAVE不仅预测动作$a_t$，还生成一个探索动作$\tilde{a}_t$。首先，模型预测一个系数 $β_t$ 用于缩放原始动作 $a_t$，得到探索动作$\tilde{a}_t = β_t a_t$。系数$β_t$通过一个全连接层和缩放函数 $σ$ 得到，限制在(0.5, 1.5)之间，确保探索动作不会偏离太远。<br>$$<br>\begin{aligned}<br>\hat{\beta}<em>t &amp;= \sigma\left(FC</em>{\beta}\left(DT(\text{Input}_t)\right)\right) \<br>\tilde{a}_t &amp;= \hat{\beta}_t a_t \<br>\quad \sigma(x) &amp;= \text{Sigmoid}(x) + 0.5<br>\end{aligned}<br>$$</p>
<p>然后，模型使用基于RTG的评估方法来比较探索动作 $\tilde{a}<em>t$ 和原始动 $a_t$。具体地，模型预测执行 $\tilde{a}<em>t$ 后的RTG值 $\tilde{r}</em>{t+1}$，并与执行 $a_t$ 后的预测RTG值 $\hat{r}</em>{t+1}$ 比较。权重 $w_t$ 用于平衡更新方向。</p>
<ul>
<li>模型预测三个关键值：<br>$$<br>(\hat{\beta}_t, \hat{a}<em>t, \hat{V}</em>{t+1}) = \text{GAVE}(\text{Input}_t)<br>$$</li>
<li>同时预测两个RTG值：<br>$$<br>\begin{aligned}<br>\hat{r}_{t+1} &amp;= \text{GAVE}(\text{Input}<em>t, a_t) \quad &amp;(原始动作的RTG) \<br>\tilde{r}</em>{t+1} &amp;= \text{GAVE}(\text{Input}_t, \tilde{a}_t) \quad &amp;\text{(探索动作的RTG)}<br>\end{aligned}<br>$$</li>
<li>更新权重计算：<br>$$<br>w_t = Sigmoid\left(a_t \cdot (\tilde{r}<em>{t+1} - \hat{r}</em>{t+1})\right)<br>$$</li>
</ul>
<p>动作损失 $L_a$ 由两部分组成：一部分是预测动作 $\hat{a}_t$ 与原始动作 $a_t$ 的均方误差，另一部分是 $\hat{a}_t$ 与探索动作$\tilde{a}_t$ 的均方误差，由权重 $w_t$ 平衡。这样，如果探索动作更好（$w_t$ &gt; 0.5），更新会更倾向于探索动作，否则倾向于原始动作。</p>
<h5 id="可学习的价值函数"><a href="#可学习的价值函数" class="headerlink" title="可学习的价值函数"></a>可学习的价值函数</h5><p>为了指导探索动作向更优的方向发展，GAVE引入了一个可学习的价值函数表示在最优动作下的RTG上界。由于直接计算这个价值函数困难，模型通过期望回归来学习这个价值函数的估计$\hat{V}<em>{t+1}$。损失函数 $L_e$ 中 $τ$ 设置为0.99以学习上界。然后，通过损失函数 $L_o$ 来引导探索动作的RTG接近价值函数的估计值，从而间接使探索动作接近最优动作。其中 $\hat{V}</em>{t+1}’$ 表示梯度冻结的价值函数估计</p>
<ul>
<li>价值函数定义：<br>$$<br>V_{t+1} = \arg\max_{a_t \in \mathbb{A}} r_{t+1}<br>$$</li>
<li>通过期望回归学习：<br>$$<br>\begin{aligned}<br>L_e &amp;= \frac{1}{M+1} \sum_{t-M}^{t} L_2^{\tau}(r_{t+1} - \hat{V}_{t+1}) \<br>L_2^{\tau}(y) &amp;= |\tau - \mathbb{1}(y &lt; 0)| \cdot y^2<br>\end{aligned}<br>$$</li>
<li>探索动作的价值引导损失：<br>$$<br>L_o = \frac{1}{M+1} \sum_{t-M}^{t} (\tilde{r}<em>{t+1} - \hat{V}</em>{t+1}’)^2<br>$$</li>
</ul>
<h5 id="损失函数与优化"><a href="#损失函数与优化" class="headerlink" title="损失函数与优化"></a>损失函数与优化</h5><p>GAVE的总损失函数是多个损失项的加权和，其中 $w_t’$ 是冻结梯度的权重，平衡向原始动作和探索动作的更新。<br>$$<br>L_{total} = \alpha_1 L_r + \alpha_2 L_a + \alpha_3 L_e + \alpha_4 L_o<br>$$</p>
<ul>
<li>RTG预测损失：<br>$$<br>L_r = \frac{1}{M+1} \sum_{t-M}^{t} (\hat{r}<em>{t+1} - r</em>{t+1})^2<br>$$</li>
<li>动作预测损失：<br>$$<br>L_a = \frac{1}{M+1} \sum_{t-M}^{t} \left[(1-w_t’) \cdot (\hat{a}_t - a_t)^2 + w_t’ \cdot (\hat{a}_t - \tilde{a}_t’)^2\right]<br>$$</li>
</ul>
<p>算法流程总结：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 构建输入序列：历史RTG-状态-动作三元组</span><br><span class="line">2. 前向传播：</span><br><span class="line">   - 预测系数β_t、动作a_t、价值V_&#123;t+1&#125;</span><br><span class="line">   - 预测原始动作RTG：r_&#123;t+1&#125;</span><br><span class="line">   - 生成探索动作：˜a_t = β_t * a_t</span><br><span class="line">   - 预测探索动作RTG：˜r_&#123;t+1&#125;</span><br><span class="line">3. 计算权重：w_t = Sigmoid(a_t · (˜r_&#123;t+1&#125; - r_&#123;t+1&#125;))</span><br><span class="line">4. 计算损失：</span><br><span class="line">   - L_r: RTG预测准确性</span><br><span class="line">   - L_a: 动作预测（平衡原始与探索动作）</span><br><span class="line">   - L_e: 价值函数学习（期望回归）</span><br><span class="line">   - L_o: 价值引导（使探索RTG接近上界）</span><br><span class="line">5. 反向传播更新参数</span><br></pre></td></tr></table></figure>

<h3 id="Diffusion-based"><a href="#Diffusion-based" class="headerlink" title="Diffusion based"></a>Diffusion based</h3><h4 id="AIGB-Generative-Auto-bidding-via-Conditional-Diffusion-Modeling"><a href="#AIGB-Generative-Auto-bidding-via-Conditional-Diffusion-Modeling" class="headerlink" title="AIGB: Generative Auto-bidding via Conditional Diffusion Modeling"></a>AIGB: Generative Auto-bidding via Conditional Diffusion Modeling</h4><p>AIGB是一种基于扩散模型的自动出价生成方法。DT基于马尔可夫决策过程，即假设下一个状态仅依赖于当前状态和动作，但在实际广告环境中，历史状态序列对当前决策影响显著。扩散模型的非自回归生成，直接建模整个轨迹的联合分布，最大化似然估计等价于一个非马尔可夫决策过程的最大化回报问题。这意味着DiffBid不依赖MDP假设，能更好地处理广告环境中的随机性和稀疏回报，避免了DT自回归生成带来的误差累积。</p>
<div align="center">
  <img src="AIGB.png" height=100% width=100%>
</div>

<p>AIGB将自动出价问题视为条件轨迹生成问题，给定广告主的目标（如最大化GMV、控制CPC、平滑消耗等），直接生成满足条件的最优状态轨迹，再通过逆动力学模型生成对应的出价参数。问题定义：给定轨迹 $\tau = (s_1, a_1, r_1, …, s_T)$ 目标是最优化条件似然：<br>$$<br>\max_{\theta} \mathbb{E}_{\tau \sim D} \left[ \log p_\theta(x_0(\tau) \mid y(\tau)) \right]<br>$$<br>其中 $x_0(\tau) = (s_1, …, s_T)$ 为状态轨迹，$y(\tau)$ 为条件（如总回报 $R(\tau)$、约束满足指标等）。</p>
<ul>
<li>前向扩散过程：逐步向轨迹添加高斯噪声，其中 $\beta_k$ 为噪声调度参数（常用cosine schedule）。当 $K \to \infty$ 时，$x_K(\tau)$ 趋近于高斯噪声。<br>$$<br>q(x_k(\tau) \mid x_{k-1}(\tau)) = \mathcal{N}(x_k(\tau); \sqrt{1 - \beta_k} x_{k-1}(\tau), \beta_k I)<br>$$</li>
<li>反向生成过程：通过神经网络逐步去噪，生成符合条件 $y(\tau)$ 的轨迹，其中 $\hat{\epsilon}<em>k$ 是通过分类器无关引导组合的条件与无条件噪声估计。<br>$$<br>p_\theta(x</em>{k-1}(\tau) \mid x_k(\tau), y(\tau)) = \mathcal{N}(x_{k-1}(\tau) \mid \mu_\theta(x_k(\tau), y(\tau), k), \Sigma_\theta(x_k(\tau), k))<br>$$<br>$$<br>\mu_\theta(x_k, y, k) = \frac{1}{\sqrt{\alpha_k}} \left( x_k - \frac{\beta_k}{\sqrt{1 - \bar{\alpha}_k}} \hat{\epsilon}_k \right)<br>$$<br>$$<br>\hat{\epsilon}_k = \epsilon_\theta(x_k, k) + \omega \left( \epsilon_\theta(x_k, y, k) - \epsilon_\theta(x_k, k) \right)<br>$$</li>
<li>逆动力学动作生成：给定当前状态 $s_{t-L:t}$ 和预测状态 $s’<em>{t+1}$，通过逆动力学模型生成出价参数：<br>$$<br>\hat{a}<em>t = f_\phi(s</em>{t-L:t}, s’</em>{t+1})<br>$$</li>
<li>训练目标是联合训练扩散模型和逆动力学模型：<br>$$<br>\mathcal{L}(\theta, \phi) = \mathbb{E}<em>{k, \tau} \left[ | \epsilon - \epsilon_\theta(x_k, y, k) |^2 \right] + \mathbb{E}</em>{(s_{t-L:t}, a_t, s_{t+1})} \left[ | a_t - f_\phi(s_{t-L:t}, s_{t+1}) |^2 \right]<br>$$</li>
</ul>
<h4 id="CBD-Generative-Auto-Bidding-in-Large-Scale-Competitive-Auctions-via-Diffusion-Completer-Aligner"><a href="#CBD-Generative-Auto-Bidding-in-Large-Scale-Competitive-Auctions-via-Diffusion-Completer-Aligner" class="headerlink" title="CBD: Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner"></a>CBD: Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner</h4><p>CBD在AIGB的基础上进行了改进，主要解决生成不确定性问题。改进点包括：</p>
<ul>
<li>训练阶段：将扩散模型的训练重构为 <strong>“补全任务”</strong>，模拟大语言模型的“问答”模式：给定历史状态（Query），生成未来状态（Answer），以增强状态间的动态连贯性。引入一个额外的随机变量t，将扩散模型的训练重构为学习一个“完成器”（completer），即给定前t个状态，完成剩余的状态序列，类似 masked-trajectory 建模，这样增强了生成的轨迹的动态合法性？</li>
<li>推理阶段：由于扩散模型的生成过程是随机的，生成的轨迹可能不完全符合广告主的目标（条件）。CBD使用一个轨迹级别的回报模型作为“对齐器”（aligner），对生成的轨迹进行梯度微调，使其更符合广告主的目标。</li>
</ul>
<div align="center">
  <img src="CBD.png" height=100% width=100%>
</div>

<p><strong>Completer</strong>：引入随机变量 $t \sim [0, T-1]$ 表示观测历史长度。训练目标变为给定前 $t$ 个状态生成剩余状态。其中 $\tilde{x}<em>k(\tau, t) = [s_0, …, s_t, s</em>{t+1}^k, …, s_T^k]$ 为部分观测、部分噪声的轨迹。$| \cdot |<em>{t+1:T}^2$ 表示仅对 $t+1$ 到 $T$ 位置计算损失。这样迫使模型在噪声中“补全”未来状态，增强动态合法性。<br>  $$<br>  \max</em>{\theta} \mathbb{E}<em>{\tau \sim D} [\log p_\theta(s</em>{t+1:T} \mid s_{0:t}, y(\tau))]<br>  $$<br>  $$<br>  \mathcal{L}<em>c(\theta) = \mathbb{E}</em>{k, x_0, t, \epsilon} \left| \epsilon - \epsilon_\theta(\tilde{x}<em>k(\tau, t), k, y(\tau)) \right|</em>{t+1:T}^2<br>  $$</p>
<p><strong>Aligner</strong>：首先通过扩散模型生成未来状态组合为初始轨迹 $\tilde{x}<em>0(\tau, t) = [s_0, …, s_t, \tilde{s}</em>{t+1}, …, \tilde{s}_T]$。然后训练轨迹级回报模型 $R_\varphi$ 用于预测轨迹的累积奖励。<br>  $$<br>  \mathcal{L}<em>r(\varphi) = \mathbb{E}</em>{(x_0, y) \sim D} | R_\varphi(x_0(\tau)) - y(\tau) |^2<br>  $$</p>
<p>梯度微调生成状态：若生成轨迹的预测回报 $R_\varphi$ 高于目标 $y(\tau)$，则向下调整状态，减少激进出价。若低于目标，则向上调整状态，增加出价积极性。<br>  $$<br>  \tilde{s}’<em>{t+1:T} \leftarrow \tilde{s}</em>{t+1:T} - \lambda \nabla_{\tilde{s}_{t+1:T}} | R_\varphi(\tilde{x}_0(\tau, t)) - y(\tau) |^2<br>  $$</p>
<hr>
<p>计算广告的出价算法演进，正从“确定性规则”迈向“不确定性智能”。传统方法无论PID式反馈还是期望最优的全局规划，都是在预设的因果框架内寻找最优解。而强化学习与生成式模型的兴起则承认了真实竞价场域的本质：它是一个由海量智能体交互构成、充满随机反馈和长期依赖的复杂序列决策环境。因此技术的焦点从“如何更准地预测单次价值”转向“如何在一个动态博弈中规划全局”。大模型时代将为此注入更深刻的“理解”与“生成”力量。生成式出价规划智能体将不再仅是调整参数的优化器，而可能进化为一个能够深度理解广告主模糊的自然语言意图、洞悉市场竞争态势，并同步生成协同策略的“决策大脑”。</p>
<h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul>
<li><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/1938485355159480262">【王喆】「深度学习计算广告」讲透互联网广告中的出价模式</a></li>
<li><a target="_blank" rel="noopener" href="https://dl.acm.org/doi/epdf/10.1145/3447548.3467199">A unified solution to constrained bidding in online display advertising</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2106.01345">Decision Transformer: Reinforcement Learning via Sequence Modeling</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2412.17018">GAS: Generative Auto-bidding with Post-training Search</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2504.14587">Generative Auto-Bidding with Value-Guided Explorations</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2212.09748">Scalable Diffusion Models with Transformers</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2405.16141">AIGB: Generative Auto-bidding via Conditional Diffusion Modeling</a></li>
<li><a target="_blank" rel="noopener" href="https://arxiv.org/abs/2509.03348">Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner</a></li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">贪钱算法还我头发</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xfliu1998.github.io/2026/01/18/Bidding/">https://xfliu1998.github.io/2026/01/18/Bidding/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Search-Ads-Reco/">Search, Ads &amp; Reco</a></div><div class="post_share"><div class="social-share" data-image="/2026/01/18/Bidding/cover.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="next-post pull-full"><a href="/2025/12/07/Rank-Model/"><img class="next-cover" src="/2025/12/07/Rank-Model/cover.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Rank Model</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2025/12/07/Rank-Model/" title="Rank Model"><img class="cover" src="/2025/12/07/Rank-Model/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-12-07</div><div class="title">Rank Model</div></div></a></div><div><a href="/2025/11/26/LLM-Ad&Rec/" title="LLM —— LLM4Ad&Rec"><img class="cover" src="/2025/11/26/LLM-Ad&Rec/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-26</div><div class="title">LLM —— LLM4Ad&Rec</div></div></a></div><div><a href="/2025/11/16/LLM-GRs/" title="LLM —— GRs"><img class="cover" src="/2025/11/16/LLM-GRs/cover.jpg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2025-11-16</div><div class="title">LLM —— GRs</div></div></a></div><div><a href="/2024/03/31/Causal-Inference/" title="Causal Inference"><img class="cover" src="/2024/03/31/Causal-Inference/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-03-31</div><div class="title">Causal Inference</div></div></a></div><div><a href="/2022/01/18/5.7-RS-case/" title="Recommendation System case"><img class="cover" src="/2022/01/18/5.7-RS-case/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-18</div><div class="title">Recommendation System case</div></div></a></div><div><a href="/2022/01/18/5.2-RS-Algorithm/" title="Recommendation System Algorithm"><img class="cover" src="/2022/01/18/5.2-RS-Algorithm/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2022-01-18</div><div class="title">Recommendation System Algorithm</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">贪钱算法还我头发</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">66</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xfliu1998"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xfliu1998" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuxiaofei_7@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/fan-xu-15-35/posts" target="_blank" title="Zhihu"><i class="fa fa-address-card"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">👋你好呀，欢迎围观～搭建这个小站源于一个朴素的愿望：对抗遗忘，沉淀思考。期待在代码与逻辑的世界里探索技术的深度与广度，永远保持热情与好奇。</div></div><div class="card-widget" id="newYear"><div class="item-headline"><i></i><span></span></div><div class="item-content"><div id="newYear-main"><div class="mask"></div> <p class="title"></p> <div class="newYear-time"></div> <p class="today" style="text-align: right;"></p> </div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%BB%BA%E6%A8%A1"><span class="toc-number">1.</span> <span class="toc-text">问题建模</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#MCB"><span class="toc-number">1.1.</span> <span class="toc-text">MCB</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#oCPM"><span class="toc-number">1.2.</span> <span class="toc-text">oCPM</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BC%A0%E7%BB%9F%E7%AE%97%E6%B3%95"><span class="toc-number">2.</span> <span class="toc-text">传统算法</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#PID"><span class="toc-number">2.1.</span> <span class="toc-text">PID</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9C%9F%E6%9C%9B%E6%9C%80%E4%BC%98%E8%B0%83%E4%BB%B7"><span class="toc-number">2.2.</span> <span class="toc-text">期望最优调价</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95%E2%80%94%E2%80%94IQL"><span class="toc-number">3.</span> <span class="toc-text">强化学习算法——IQL</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98%E5%AE%9A%E4%B9%89"><span class="toc-number">3.0.1.</span> <span class="toc-text">问题定义</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IQL%E7%AE%80%E4%BB%8B"><span class="toc-number">3.0.2.</span> <span class="toc-text">IQL简介</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#IQL%E5%9C%A8%E5%B9%BF%E5%91%8A%E5%87%BA%E4%BB%B7%E4%B8%AD%E7%9A%84%E5%BB%BA%E6%A8%A1%E8%BF%87%E7%A8%8B"><span class="toc-number">3.0.3.</span> <span class="toc-text">IQL在广告出价中的建模过程</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88IQL%E9%80%82%E5%90%88%E5%BB%BA%E6%A8%A1%E5%87%BA%E4%BB%B7%E9%97%AE%E9%A2%98%EF%BC%9F"><span class="toc-number">3.0.4.</span> <span class="toc-text">为什么IQL适合建模出价问题？</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E7%94%9F%E6%88%90%E5%BC%8F%E5%87%BA%E4%BB%B7"><span class="toc-number">4.</span> <span class="toc-text">生成式出价</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Decision-Transformer-based"><span class="toc-number">4.1.</span> <span class="toc-text">Decision Transformer based</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DT"><span class="toc-number">4.1.1.</span> <span class="toc-text">DT</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GAS-Generative-Auto-bidding-with-Post-training-Search"><span class="toc-number">4.1.2.</span> <span class="toc-text">GAS: Generative Auto-bidding with Post-training Search</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GAVE-Generative-Auto-Bidding-with-Value-Guided-Explorations"><span class="toc-number">4.1.3.</span> <span class="toc-text">GAVE: Generative Auto-Bidding with Value-Guided Explorations</span></a><ol class="toc-child"><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%9F%BA%E4%BA%8E%E5%88%86%E6%95%B0%E7%9A%84RTG%E8%AE%A1%E7%AE%97"><span class="toc-number">4.1.3.1.</span> <span class="toc-text">基于分数的RTG计算</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8A%A8%E4%BD%9C%E6%8E%A2%E7%B4%A2%E6%9C%BA%E5%88%B6"><span class="toc-number">4.1.3.2.</span> <span class="toc-text">动作探索机制</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E5%8F%AF%E5%AD%A6%E4%B9%A0%E7%9A%84%E4%BB%B7%E5%80%BC%E5%87%BD%E6%95%B0"><span class="toc-number">4.1.3.3.</span> <span class="toc-text">可学习的价值函数</span></a></li><li class="toc-item toc-level-5"><a class="toc-link" href="#%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-number">4.1.3.4.</span> <span class="toc-text">损失函数与优化</span></a></li></ol></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Diffusion-based"><span class="toc-number">4.2.</span> <span class="toc-text">Diffusion based</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#AIGB-Generative-Auto-bidding-via-Conditional-Diffusion-Modeling"><span class="toc-number">4.2.1.</span> <span class="toc-text">AIGB: Generative Auto-bidding via Conditional Diffusion Modeling</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#CBD-Generative-Auto-Bidding-in-Large-Scale-Competitive-Auctions-via-Diffusion-Completer-Aligner"><span class="toc-number">4.2.2.</span> <span class="toc-text">CBD: Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="toc-number">5.</span> <span class="toc-text">参考文献</span></a></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/2026/01/18/Bidding/cover.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2026  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 贪钱算法还我头发</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '1b0c10ce649501ea4a72',
      clientSecret: '741b5e861137e3d5a482bba272c8201b78da6cb0',
      repo: 'xfliu1998.github.io',
      owner: 'xfliu1998',
      admin: ['xfliu1998'],
      id: '2125fa667041a802c2a98de5cf5494b2',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: true,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/script.js?v1"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><script async data-pjax src="/js/newYear.js"></script><script async src="//at.alicdn.com/t/font_2264842_b004iy0kk2b.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='all'|| 'all' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Machine-Learning-and-Deep-Learning/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习与深度学习 (18)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Structures-and-Algorithms/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">😼 数据结构与算法 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Analysis-and-Processing/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数据分析与处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Reading-Notes/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 阅读笔记 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Daily/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 日常随笔 (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item" style="visibility: hidden"></div><a class="magnet_link_more"  href="https://xfliu1998.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/03/25/Papers-Ideas/" alt=""><img width="48" height="48" src="2023/03/25/Papers-Ideas/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-25</span><a class="blog-slider__title" href="2023/03/25/Papers-Ideas/" alt="">Papers Ideas</a><div class="blog-slider__text">大模型时代下的科研思路</div><a class="blog-slider__button" href="2023/03/25/Papers-Ideas/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Summary/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Summary/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Summary/" alt="">Papers Summary</a><div class="blog-slider__text">论文总结笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Summary/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/11/16/LLM-GRs/" alt=""><img width="48" height="48" src="2025/11/16/LLM-GRs/cover.jpg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-11-16</span><a class="blog-slider__title" href="2025/11/16/LLM-GRs/" alt="">LLM —— GRs</a><div class="blog-slider__text">生成式在推荐广告领域的落地</div><a class="blog-slider__button" href="2025/11/16/LLM-GRs/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/11/26/LLM-Ad&amp;Rec/" alt=""><img width="48" height="48" src="2025/11/26/LLM-Ad&amp;Rec/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-11-26</span><a class="blog-slider__title" href="2025/11/26/LLM-Ad&amp;Rec/" alt="">LLM —— LLM4Ad&amp;Rec</a><div class="blog-slider__text">LLM在广告推荐领域的应用范式</div><a class="blog-slider__button" href="2025/11/26/LLM-Ad&amp;Rec/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2025/12/07/Rank-Model/" alt=""><img width="48" height="48" src="2025/12/07/Rank-Model/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2025-12-07</span><a class="blog-slider__title" href="2025/12/07/Rank-Model/" alt="">Rank Model</a><div class="blog-slider__text">精排模型——从特征交叉建模到深度兴趣序列建模</div><a class="blog-slider__button" href="2025/12/07/Rank-Model/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2026/01/18/Bidding/" alt=""><img width="48" height="48" src="2026/01/18/Bidding/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2026-01-18</span><a class="blog-slider__title" href="2026/01/18/Bidding/" alt="">Ad Bidding</a><div class="blog-slider__text">广告出价算法：从传统式到生成式</div><a class="blog-slider__button" href="2026/01/18/Bidding/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/01/21/Learning-Framework/" alt=""><img width="48" height="48" src="2022/01/21/Learning-Framework/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-21</span><a class="blog-slider__title" href="2022/01/21/Learning-Framework/" alt="">学习大纲</a><div class="blog-slider__text">目录</div><a class="blog-slider__button" href="2022/01/21/Learning-Framework/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<container><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></container>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.fomal.cc/api?xfliu1998",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xfliu1998')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>