<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Hadoop | 一直进步 做喜欢的</title><meta name="keywords" content="python"><meta name="author" content="贪钱算法还我头发"><meta name="copyright" content="贪钱算法还我头发"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="推荐系统学习笔记三——Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="Hadoop">
<meta property="og:url" content="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/index.html">
<meta property="og:site_name" content="一直进步 做喜欢的">
<meta property="og:description" content="推荐系统学习笔记三——Hadoop">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6">
<meta property="article:published_time" content="2022-01-18T13:43:40.000Z">
<meta property="article:modified_time" content="2024-03-31T12:12:45.634Z">
<meta property="article:author" content="贪钱算法还我头发">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Hadoop',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-31 20:12:45'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><link rel="stylesheet" href="/css/custom.css?v1"><link rel="stylesheet" href="//at.alicdn.com/t/font_2264842_b004iy0kk2b.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">52</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一直进步 做喜欢的</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Hadoop</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-01-18T13:43:40.000Z" title="Created 2022-01-18 21:43:40">2022-01-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-03-31T12:12:45.634Z" title="Updated 2024-03-31 20:12:45">2024-03-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Search-Advertisement-Recommendation-Causal/">Search / Advertisement / Recommendation / Causal</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">8k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>29min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Hadoop"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2022/01/18/5.3-Hadoop/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>推荐系统学习笔记目录</strong></p>
<ol>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.1-Recommendation-System-Introduction/">推荐系统介绍</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.2-RS-Algorithm/">推荐算法</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/">Hadoop</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.4-Hive/">Hive &amp; HBase</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/">Spark core</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/">Spark SQL &amp; Spark streaming</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.7-RS-case/">推荐系统案例</a></li>
</ol>
<h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="什么是Hadoop"><a href="#什么是Hadoop" class="headerlink" title="什么是Hadoop"></a>什么是Hadoop</h3><ul>
<li><p>Hadoop名字的由来</p>
<ul>
<li>作者：Doug cutting</li>
<li>Hadoop项目作者的孩子给一个棕黄色的大象样子的填充玩具的命名<br><img src="image-hadoop1.png"></li>
</ul>
</li>
<li><p>Hadoop的概念:</p>
<ul>
<li>Apache™ Hadoop®  是一个开源的, <strong>可靠的</strong>(reliable), <strong>可扩展</strong>的(scalable)<strong>分布式计算框架</strong><ul>
<li>允许使用简单的编程模型跨计算机集群分布式处理大型数据集</li>
<li><strong>可扩展</strong>: 从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储</li>
<li><strong>可靠的</strong>: 不依靠硬件来提供高可用性(high-availability)，而是在应用层检测和处理故障，从而在计算机集群之上提供高可用服务</li>
</ul>
</li>
</ul>
</li>
<li><p>Hadoop能做什么?</p>
<ul>
<li>搭建大型数据仓库</li>
<li>PB级数据的存储 处理 分析 统计等业务<ul>
<li>搜索引擎</li>
<li>日志分析</li>
<li>数据挖掘</li>
<li>商业智能(Business Intelligence，简称：BI)<br>商业智能通常被理解为将企业中现有的数据(订单、库存、交易账目、客户和供应商等数据)转化为知识，帮助企业做出明智的业务经营决策的工具。从技术层面上讲，是数据仓库、数据挖掘等技术的综合运用。</li>
</ul>
</li>
</ul>
</li>
<li><p>Hadoop发展史</p>
<ul>
<li>2003-2004年 Google发表了三篇论文<ul>
<li>GFS：Google的分布式文件系统Google File System </li>
<li><a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/MapReduce">MapReduce</a>: Simplified Data Processing on Large Clusters </li>
<li>BigTable：一个大型的分布式数据库</li>
</ul>
</li>
<li>2006年2月Hadoop成为Apache的独立开源项目( Doug Cutting等人实现了DFS和MapReduce机制)。</li>
<li>2006年4月— 标准排序(10 GB每个节点)在188个节点上运行47.9个小时。 </li>
<li>2008年4月— 赢得世界最快1TB数据排序在900个节点上用时209秒。 </li>
<li>2008年— <strong>淘宝开始投入研究基于Hadoop的系统–云梯</strong>。云梯总容量约9.3PB，共有1100台机器，每天处理18000道作业，扫描500TB数据。 </li>
<li>2009年3月— <strong>Cloudera推出CDH（Cloudera’s Dsitribution Including Apache Hadoop）</strong></li>
<li>2009年5月— Yahoo的团队使用Hadoop对1 TB的数据进行排序只花了62秒时间。 </li>
<li>2009年7月— <strong>Hadoop Core项目更名为Hadoop Common;</strong> </li>
<li>2009年7月— <strong>MapReduce和Hadoop Distributed File System (HDFS)成为Hadoop项目的独立子项目。</strong></li>
<li>2012年11月— Apache Hadoop 1.0 Available</li>
<li>2018年4月— Apache Hadoop 3.1 Available</li>
<li>搜索引擎时代<ul>
<li>有保存大量网页的需求(单机  集群)</li>
<li>词频统计 word count  PageRank</li>
</ul>
</li>
<li>数据仓库时代<ul>
<li>FaceBook推出Hive</li>
<li>曾经进行数分析与统计时, 仅限于数据库,受数据量和计算能力的限制, 我们只能对最重要的数据进行统计和分析(决策数据,财务相关)</li>
<li>Hive可以在Hadoop上运行SQL操作, 可以把运行日志, 应用采集数据,数据库数据放到一起分析</li>
</ul>
</li>
<li>数据挖掘时代<ul>
<li>啤酒尿不湿</li>
<li>关联分析</li>
<li>用户画像/物品画像</li>
</ul>
</li>
<li>机器学习时代  广义大数据<ul>
<li>大数据提高数据存储能力, 为机器学习提供燃料</li>
<li>alpha go</li>
<li>siri 小爱 天猫精灵</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Hadoop核心组件"><a href="#Hadoop核心组件" class="headerlink" title="Hadoop核心组件"></a>Hadoop核心组件</h3><ul>
<li><p>Hadoop是所有搜索引擎的共性问题的廉价解决方案</p>
<ul>
<li>如何存储持续增长的海量网页:  单节点 V.S. 分布式存储</li>
<li>如何对持续增长的海量网页进行排序: 超算 V.S. 分布式计算</li>
<li>HDFS 解决分布式存储问题</li>
<li>MapReduce 解决分布式计算问题</li>
</ul>
</li>
<li><p><strong>Hadoop Common</strong>: The common utilities that support the other Hadoop modules.(hadoop的核心组件)</p>
</li>
<li><p><strong>Hadoop Distributed File System (HDFS™)</strong>: A distributed file system that provides high-throughput access to application data.(分布式文件系统)</p>
<ul>
<li>源自于Google的GFS论文, 论文发表于2003年10月</li>
<li>HDFS是GFS的开源实现</li>
<li>HDFS的特点:扩展性&amp;容错性&amp;海量数量存储</li>
<li>将文件切分成指定大小的数据块, 并在多台机器上保存多个副本</li>
<li>数据切分、多副本、容错等操作对用户是透明的</li>
</ul>
</li>
<li><p>下面这张图是数据块多份复制存储的示意</p>
<ul>
<li>图中对于文件 /users/sameerp/data/part-0，其复制备份数设置为2, 存储的BlockID分别为1、3。</li>
<li>Block1的两个备份存储在DataNode0和DataNode2两个服务器上</li>
<li>Block3的两个备份存储在DataNode4和DataNode6两个服务器上</li>
</ul>
</li>
</ul>
<p><img src="hadoop-hdfs1.png"></p>
<ul>
<li><p><strong>Hadoop MapReduce</strong>: A YARN-based system for parallel processing of large data sets.</p>
<ul>
<li>分布式计算框架</li>
<li>源于Google的MapReduce论文，论文发表于2004年12月</li>
<li>MapReduce是GoogleMapReduce的开源实现</li>
<li>MapReduce特点:扩展性&amp;容错性&amp;海量数据离线处理</li>
</ul>
<p><img src="image-mapreduce.png"></p>
</li>
<li><p><strong>Hadoop YARN</strong>: A framework for job scheduling and cluster resource management.(资源调度系统)</p>
<ul>
<li><p>YARN: Yet Another Resource Negotiator</p>
</li>
<li><p>负责整个集群资源的管理和调度</p>
</li>
<li><p>YARN特点:扩展性&amp;容错性&amp;多框架资源统一调度</p>
<p><img src="image-yarn.png"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Hadoop优势"><a href="#Hadoop优势" class="headerlink" title="Hadoop优势"></a>Hadoop优势</h3><ul>
<li>高可靠<ul>
<li>数据存储: 数据块多副本</li>
<li>数据计算: 某个节点崩溃, 会自动重新调度作业计算</li>
</ul>
</li>
<li>高扩展性<ul>
<li>存储/计算资源不够时，可以横向的线性扩展机器</li>
<li>一个集群中可以包含数以千计的节点</li>
<li>集群可以使用廉价机器，成本低</li>
</ul>
</li>
<li>Hadoop生态系统成熟</li>
</ul>
<h2 id="分布式文件系统-HDFS"><a href="#分布式文件系统-HDFS" class="headerlink" title="分布式文件系统 HDFS"></a>分布式文件系统 HDFS</h2><h3 id="HDFS的使用"><a href="#HDFS的使用" class="headerlink" title="HDFS的使用"></a>HDFS的使用</h3><ul>
<li><p>启动HDFS</p>
<ul>
<li>来到$HADOOP_HOME/sbin目录下</li>
<li>执行start-dfs.sh</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop00 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure>

<ul>
<li>可以看到 namenode和 datanode启动的日志信息</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [hadoop00]</span><br><span class="line">hadoop00: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop00.out</span><br><span class="line">localhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop00.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop00.out</span><br></pre></td></tr></table></figure>

<ul>
<li>通过jps命令查看当前运行的进程</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop00 sbin]$ jps</span><br><span class="line">4416 DataNode</span><br><span class="line">4770 Jps</span><br><span class="line">4631 SecondaryNameNode</span><br><span class="line">4251 NameNode</span><br></pre></td></tr></table></figure>

<ul>
<li>可以看到 NameNode DataNode 以及 SecondaryNameNode 说明启动成功</li>
</ul>
</li>
<li><p>通过可视化界面查看HDFS的运行情况</p>
<ul>
<li>通过浏览器查看 主机ip:50070端口 </li>
</ul>
<p><img src="hadoop-state.png" alt="1551174774098"></p>
<ul>
<li>Overview界面查看整体情况</li>
</ul>
<p><img src="hadoop-state1.png" alt="1551174978741"></p>
<ul>
<li><p>Datanodes界面查看datanode的情况</p>
<p><img src="hadoop-state2.png" alt="1551175081051"></p>
</li>
</ul>
</li>
</ul>
<h3 id="HDFS-shell操作"><a href="#HDFS-shell操作" class="headerlink" title="HDFS shell操作"></a>HDFS shell操作</h3><ul>
<li><p>调用文件系统(FS)Shell命令应使用 bin/hadoop fs <args>的形式</p>
<ul>
<li><p>ls<br>使用方法：hadoop fs -ls <args><br>如果是文件，则按照如下格式返回文件信息：<br>文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID<br>如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下：<br>目录名 <dir> 修改日期 修改时间 权限 用户ID 组ID<br>示例：<br>hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2 hdfs://host:port/user/hadoop/dir1 /nonexistentfile<br>返回值：<br>成功返回0，失败返回-1。 </p>
</li>
<li><p>text<br>使用方法：hadoop fs -text <src><br>将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream。</p>
</li>
<li><p>mv<br>使用方法：hadoop fs -mv URI [URI …] <dest><br>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。<br>示例：</p>
<ul>
<li>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</li>
<li>hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
</li>
<li><p>put<br>使用方法：hadoop fs -put <localsrc> … <dst><br>从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。</p>
<ul>
<li>hadoop fs -put localfile /user/hadoop/hadoopfile</li>
<li>hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</li>
<li>hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile</li>
<li>hadoop fs -put - hdfs://host:port/hadoop/hadoopfile<br>从标准输入中读取输入。</li>
</ul>
<p>返回值：成功返回0，失败返回-1。</p>
</li>
<li><p>rm<br>使用方法：hadoop fs -rm URI [URI …]<br>删除指定的文件。只删除非空目录和文件。请参考rmr命令了解递归删除。<br>示例：</p>
<ul>
<li>hadoop fs -rm hdfs://host:port/file /user/hadoop/emptydir<br>返回值：<br>成功返回0，失败返回-1。</li>
</ul>
</li>
</ul>
</li>
<li><p><a target="_blank" rel="noopener" href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a></p>
</li>
</ul>
<h3 id="HDFS-shell操作练习"><a href="#HDFS-shell操作练习" class="headerlink" title="HDFS shell操作练习"></a>HDFS shell操作练习</h3><ul>
<li><p>在centos 中创建 test.txt  </p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch test.txt</span><br></pre></td></tr></table></figure></li>
<li><p>在centos中为test.txt 添加文本内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi test.txt</span><br></pre></td></tr></table></figure></li>
<li><p>在HDFS中创建 hadoop001/test 文件夹</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /hadoop001/test</span><br></pre></td></tr></table></figure></li>
<li><p>把text.txt文件上传到HDFS中</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put test.txt /hadoop001/test/</span><br></pre></td></tr></table></figure></li>
<li><p>查看hdfs中 hadoop001/test/test.txt 文件内容</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /hadoop001/test/test.txt</span><br></pre></td></tr></table></figure></li>
<li><p>将hdfs中 hadoop001/test/test.txt文件下载到centos</p>
 <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /hadoop001/test/test.txt test.txt</span><br></pre></td></tr></table></figure></li>
<li><p>删除HDFS中 hadoop001/test/</p>
<p> hadoop fs -rm -r /hadoop001</p>
</li>
</ul>
<h3 id="HDFS环境搭建"><a href="#HDFS环境搭建" class="headerlink" title="HDFS环境搭建"></a>HDFS环境搭建</h3><ul>
<li><p>下载jdk 和 hadoop 放到 ~/software目录下 然后解压到 ~/app目录下</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf 压缩包名字 -C ~/app/</span><br></pre></td></tr></table></figure></li>
<li><p>配置环境变量</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bash_profile</span><br><span class="line">export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export HADOOP_HOME=/home/hadoop/app/hadoop......</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 保存退出后</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure></li>
<li><p>进入到解压后的hadoop目录 修改配置文件</p>
<ul>
<li><p>配置文件作用</p>
<ul>
<li>core-site.xml  指定hdfs的访问方式</li>
<li>hdfs-site.xml  指定namenode 和 datanode 的数据存储位置</li>
<li>mapred-site.xml 配置mapreduce</li>
<li>yarn-site.xml  配置yarn</li>
</ul>
</li>
<li><p>修改hadoop-env.sh</p>
</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd etc/hadoop</span><br><span class="line">vi hadoop-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 找到下面内容添加java home</span></span><br><span class="line">export_JAVA_HOME=/home/hadoop/app/jdk1.8.0_91</span><br></pre></td></tr></table></figure>

<ul>
<li>修改 core-site.xml 在 <configuration>节点中添加</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.default.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop000:8020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>修改hdfs-site.xml 在 configuration节点中添加</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/app/tmp/dfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/home/hadoop/app/tmp/dfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>修改 mapred-site.xml </li>
<li>默认没有这个 从模板文件复制 </li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure>

<p>​    在mapred-site.xml  的configuration 节点中添加</p>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<ul>
<li>修改yarn-site.xml configuration 节点中添加</li>
</ul>
<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li>
<li><p>来到hadoop的bin目录</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hadoop namenode -format (这个命令只运行一次)</span><br></pre></td></tr></table></figure></li>
<li><p>启动hdfs 进入到  sbin</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-dfs.sh</span><br></pre></td></tr></table></figure></li>
<li><p>启动启动yarn 在sbin中</p>
</li>
</ul>
<h2 id="YARN"><a href="#YARN" class="headerlink" title="YARN"></a>YARN</h2><h3 id="什么是YARN"><a href="#什么是YARN" class="headerlink" title="什么是YARN"></a>什么是YARN</h3><ul>
<li>Yet Another Resource Negotiator, 另一种资源协调者</li>
<li>通用资源管理系统</li>
<li>为上层应用提供统一的资源管理和调度，为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处</li>
</ul>
<h3 id="YARN产生背景"><a href="#YARN产生背景" class="headerlink" title="YARN产生背景"></a>YARN产生背景</h3><ul>
<li><p>通用资源管理系统</p>
<ul>
<li>Hadoop数据分布式存储（数据分块，冗余存储）</li>
<li>当多个MapReduce任务要用到相同的hdfs数据， 需要进行资源调度管理</li>
<li>Hadoop1.x时并没有YARN，MapReduce 既负责进行计算作业又处理服务器集群资源调度管理</li>
</ul>
</li>
<li><p>服务器集群资源调度管理和MapReduce执行过程耦合在一起带来的问题</p>
<ul>
<li><p>Hadoop早期, 技术只有Hadoop, 这个问题不明显</p>
</li>
<li><p>随着大数据技术的发展，Spark Storm … 计算框架都要用到服务器集群资源 </p>
</li>
<li><p>如果没有通用资源管理系统，只能为多个集群分别提供数据</p>
<ul>
<li> 资源利用率低 运维成本高</li>
</ul>
<p><img src="image-yarn2.png"></p>
</li>
<li><p>Yarn (Yet Another Resource Negotiator) 另一种资源调度器</p>
<ul>
<li>Mesos 大数据资源管理产品</li>
</ul>
</li>
</ul>
</li>
<li><p>不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度</p>
<p><img src="hadoop-yarn3.png"></p>
</li>
</ul>
<h3 id="YARN的架构和执行流程"><a href="#YARN的架构和执行流程" class="headerlink" title="YARN的架构和执行流程"></a>YARN的架构和执行流程</h3><ul>
<li>ResourceManager: RM 资源管理器<br>​    整个集群同一时间提供服务的RM只有一个，负责集群资源的统一管理和调度<br>​    处理客户端的请求： submit, kill<br>​    监控我们的NM，一旦某个NM挂了，那么该NM上运行的任务需要告诉我们的AM来如何进行处理</li>
<li>NodeManager: NM 节点管理器<br>​    整个集群中有多个，负责自己本身节点资源管理和使用<br>​    定时向RM汇报本节点的资源使用情况<br>​    接收并处理来自RM的各种命令：启动Container<br>​    处理来自AM的命令</li>
<li>ApplicationMaster: AM<br>​    每个应用程序对应一个：MR、Spark，负责应用程序的管理<br>​    为应用程序向RM申请资源（core、memory），分配给内部task<br>​    需要与NM通信：启动/停止task，task是运行在container里面，AM也是运行在container里面</li>
<li>Container 容器: 封装了CPU、Memory等资源的一个容器,是一个任务运行环境的抽象</li>
<li>Client: 提交作业 查询作业的运行进度,杀死作业</li>
</ul>
<p><img src="yarn4.png"></p>
<ol>
<li>Client提交作业请求</li>
<li>ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个Container(容器)，并将 ApplicationMaster 分发到这个容器上面</li>
<li>在启动的Container中创建ApplicationMaster</li>
<li>ApplicationMaster启动后向ResourceManager注册进程,申请资源</li>
<li>ApplicationMaster申请到资源后，向对应的NodeManager申请启动Container,将要执行的程序分发到NodeManager上</li>
<li>Container启动后，执行对应的任务</li>
<li>Tast执行完毕之后，向ApplicationMaster返回结果</li>
<li>ApplicationMaster向ResourceManager 请求kill</li>
</ol>
<h3 id="YARN环境搭建"><a href="#YARN环境搭建" class="headerlink" title="YARN环境搭建"></a>YARN环境搭建</h3><p>1）mapred-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;mapreduce.framework.name&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;yarn&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<p>2）yarn-site.xml</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&lt;property&gt;</span><br><span class="line">    &lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;</span><br><span class="line">    &lt;value&gt;mapreduce_shuffle&lt;/value&gt;</span><br><span class="line">&lt;/property&gt;</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>启动YARN相关的进程<br>sbin/start-yarn.sh<br>4）验证<br>​    jps<br>​        ResourceManager<br>​        NodeManager<br>​    <a href="http://192,168.199.188:8088">http://192,168.199.188:8088</a><br>5）停止YARN相关的进程<br>​    sbin/stop-yarn.sh</li>
</ol>
<h2 id="分布式处理框架-MapReduce"><a href="#分布式处理框架-MapReduce" class="headerlink" title="分布式处理框架 MapReduce"></a>分布式处理框架 MapReduce</h2><h3 id="什么是MapReduce"><a href="#什么是MapReduce" class="headerlink" title="什么是MapReduce"></a>什么是MapReduce</h3><ul>
<li>源于Google的MapReduce论文(2004年12月)</li>
<li>Hadoop的MapReduce是Google论文的开源实现</li>
<li>MapReduce优点: 海量数据离线处理&amp;易开发</li>
<li>MapReduce缺点: 实时流式计算</li>
</ul>
<h3 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h3><ul>
<li><p>MapReduce分而治之的思想</p>
<ul>
<li>数钱实例：一堆钞票，各种面值分别是多少<ul>
<li>单点策略<ul>
<li>一个人数所有的钞票，数出各种面值有多少张</li>
</ul>
</li>
<li>分治策略<ul>
<li>每个人分得一堆钞票，数出各种面值有多少张</li>
<li>汇总，每个人负责统计一种面值</li>
</ul>
</li>
<li>解决数据可以切割进行计算的应用</li>
</ul>
</li>
</ul>
</li>
<li><p>MapReduce编程分Map和Reduce阶段</p>
<ul>
<li>将作业拆分成Map阶段和Reduce阶段</li>
<li>Map阶段 Map Tasks 分：把复杂的问题分解为若干”简单的任务”</li>
<li>Reduce阶段: Reduce Tasks 合：reduce</li>
</ul>
</li>
<li><p>MapReduce编程执行步骤</p>
<ul>
<li>准备MapReduce的输入数据</li>
<li>准备Mapper数据</li>
<li>Shuffle</li>
<li>Reduce处理</li>
<li>结果输出</li>
</ul>
</li>
<li><p><strong>编程模型</strong></p>
<ul>
<li><p>借鉴函数式编程方式</p>
</li>
<li><p>用户只需要实现两个函数接口：</p>
<ul>
<li><p>Map(in_key,in_value)</p>
<p>—&gt;(out_key,intermediate_value) list</p>
</li>
<li><p>Reduce(out_key,intermediate_value) list</p>
<p>—&gt;out_value list</p>
</li>
</ul>
</li>
<li><p>Word Count 词频统计案例</p>
<p><img src="image-mapreduce.png"></p>
</li>
</ul>
</li>
</ul>
<h3 id="Hadoop-Streaming-实现wordcount-（实验-了解）"><a href="#Hadoop-Streaming-实现wordcount-（实验-了解）" class="headerlink" title="Hadoop Streaming 实现wordcount （实验 了解）"></a>Hadoop Streaming 实现wordcount （实验 了解）</h3><ul>
<li><p>Mapper</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line"><span class="comment">#输入为标准输入stdin</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment">#删除开头和结尾的空行</span></span><br><span class="line">    line = line.strip()</span><br><span class="line">    <span class="comment">#以默认空格分隔单词到words列表</span></span><br><span class="line">    words = line.split()</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> words:</span><br><span class="line">        <span class="comment">#输出所有单词，格式为“单词 1”以便作为Reduce的输入</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;%s %s&quot;</span>%(word,<span class="number">1</span>))</span><br></pre></td></tr></table></figure></li>
<li><p>Reducer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"></span><br><span class="line">current_word = <span class="literal">None</span></span><br><span class="line">current_count = <span class="number">0</span></span><br><span class="line">word = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#获取标准输入，即mapper.py的标准输出</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> sys.stdin:</span><br><span class="line">    <span class="comment">#删除开头和结尾的空行</span></span><br><span class="line">    line = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#解析mapper.py输出作为程序的输入，以tab作为分隔符</span></span><br><span class="line">    word,count = line.split()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#转换count从字符型到整型</span></span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        count = <span class="built_in">int</span>(count)</span><br><span class="line">    <span class="keyword">except</span> ValueError:</span><br><span class="line">        <span class="comment">#count非数字时，忽略此行</span></span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#要求mapper.py的输出做排序（sort）操作，以便对连续的word做判断</span></span><br><span class="line">    <span class="keyword">if</span> current_word == word:</span><br><span class="line">        current_count += count</span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        <span class="comment">#出现了一个新词</span></span><br><span class="line">        <span class="comment">#输出当前word统计结果到标准输出</span></span><br><span class="line">        <span class="keyword">if</span> current_word :</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">&#x27;%s\t%s&#x27;</span> % (current_word,current_count))</span><br><span class="line">        <span class="comment">#开始对新词的统计</span></span><br><span class="line">        current_count = count</span><br><span class="line">        current_word = word</span><br><span class="line"></span><br><span class="line"><span class="comment">#输出最后一个word统计</span></span><br><span class="line"><span class="keyword">if</span> current_word == word:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">&quot;%s\t%s&quot;</span>% (current_word,current_count))</span><br></pre></td></tr></table></figure>

<p>cat xxx.txt|python3 map.py|sort|python3 red.py</p>
<p>得到最终的输出</p>
<p>注：hadoop-streaming会主动将map的输出数据进行字典排序</p>
</li>
<li><p>通过Hadoop Streaming 提交作业到Hadoop集群</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">STREAM_JAR_PATH=&quot;/root/bigdata/hadoop/share/hadoop/tools/lib/hadoop-streaming-2.9.1.jar&quot;    # hadoop streaming jar包所在位置</span><br><span class="line">INPUT_FILE_PATH_1=&quot;/The_Man_of_Property.txt&quot;  #要进行词频统计的文档在hdfs中的路径</span><br><span class="line">OUTPUT_PATH=&quot;/output&quot;                         #MR作业后结果的存放路径</span><br><span class="line"></span><br><span class="line">hadoop fs -rm -r -skipTrash $OUTPUT_PATH    # 输出路径如果之前存在 先删掉否则会报错</span><br><span class="line"></span><br><span class="line">hadoop jar $STREAM_JAR_PATH \   </span><br><span class="line">		-input $INPUT_FILE_PATH_1 \ # 指定输入文件位置</span><br><span class="line">		-output $OUTPUT_PATH \      #指定输出结果位置</span><br><span class="line">		-mapper &quot;python map.py&quot; \   #指定mapper执行的程序</span><br><span class="line">		-reducer &quot;python red.py&quot; \  # 指定reduce阶段执行的程序</span><br><span class="line">		-file ./map.py \            # 通过-file 把python源文件分发到集群的每一台机器上  </span><br><span class="line">		-file ./red.py</span><br></pre></td></tr></table></figure></li>
<li><p>到Hadoop集群查看运行结果</p>
<p><img src="mr_result.png"></p>
</li>
</ul>
<h3 id="利用MRJob编写和运行MapReduce代码"><a href="#利用MRJob编写和运行MapReduce代码" class="headerlink" title="利用MRJob编写和运行MapReduce代码"></a>利用MRJob编写和运行MapReduce代码</h3><p><strong>mrjob 简介</strong></p>
<ul>
<li>使用python开发在Hadoop上运行的程序, mrjob是最简单的方式</li>
<li>mrjob程序可以在本地测试运行也可以部署到Hadoop集群上运行</li>
<li>如果不想成为hadoop专家, 但是需要利用Hadoop写MapReduce代码,mrJob是很好的选择</li>
</ul>
<p><strong>mrjob 安装</strong></p>
<ul>
<li>使用pip安装<ul>
<li>pip install mrjob</li>
</ul>
</li>
</ul>
<p><strong>mrjob实现WordCount</strong></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">from mrjob.job import MRJob</span><br><span class="line"></span><br><span class="line">class MRWordFrequencyCount(MRJob):</span><br><span class="line"></span><br><span class="line">    def mapper(self, _, line):</span><br><span class="line">        yield &quot;chars&quot;, len(line)</span><br><span class="line">        yield &quot;words&quot;, len(line.split())</span><br><span class="line">        yield &quot;lines&quot;, 1</span><br><span class="line"></span><br><span class="line">    def reducer(self, key, values):</span><br><span class="line">        yield key, sum(values)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">if __name__ == &#x27;__main__&#x27;:</span><br><span class="line">    MRWordFrequencyCount.run()</span><br></pre></td></tr></table></figure>

<p><strong>运行WordCount代码</strong></p>
<p>打开命令行, 找到一篇文本文档, 敲如下命令:</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python mr_word_count.py my_file.txt</span><br></pre></td></tr></table></figure>

<h3 id="运行MRJOB的不同方式"><a href="#运行MRJOB的不同方式" class="headerlink" title="运行MRJOB的不同方式"></a>运行MRJOB的不同方式</h3><p>1、内嵌(-r inline)方式</p>
<p>特点是调试方便，启动单一进程模拟任务执行状态和结果，默认(-r inline)可以省略，输出文件使用 &gt; output-file 或-o output-file，比如下面两种运行方式是等价的</p>
<p>python word_count.py -r inline input.txt &gt; output.txt<br>python word_count.py input.txt &gt; output.txt</p>
<p>2、本地(-r local)方式</p>
<p>用于本地模拟Hadoop调试，与内嵌(inline)方式的区别是启动了多进程执行每一个任务。如：</p>
<p>python word_count.py -r local input.txt &gt; output1.txt</p>
<p>3、Hadoop(-r hadoop)方式</p>
<p>用于hadoop环境，支持Hadoop运行调度控制参数，如：</p>
<p>1)指定Hadoop任务调度优先级(VERY_HIGH|HIGH),如：–jobconf mapreduce.job.priority=VERY_HIGH。</p>
<p>2)Map及Reduce任务个数限制，如：–jobconf mapreduce.map.tasks=2  –jobconf mapreduce.reduce.tasks=5</p>
<p>python word_count.py -r hadoop hdfs:///test.txt -o  hdfs:///output</p>
<h3 id="mrjob-实现-topN统计（实验）"><a href="#mrjob-实现-topN统计（实验）" class="headerlink" title="mrjob 实现 topN统计（实验）"></a>mrjob 实现 topN统计（实验）</h3><p>统计数据中出现次数最多的前n个数据</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob,MRStep</span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopNWords</span>(<span class="params">MRJob</span>):</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span>(<span class="params">self, _, line</span>):</span></span><br><span class="line">        <span class="keyword">if</span> line.strip() != <span class="string">&quot;&quot;</span>:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().split():</span><br><span class="line">                <span class="keyword">yield</span> word,<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#介于mapper和reducer之间，用于临时的将mapper输出的数据进行统计</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combiner</span>(<span class="params">self, word, counts</span>):</span></span><br><span class="line">        <span class="keyword">yield</span> word,<span class="built_in">sum</span>(counts)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer_sum</span>(<span class="params">self, word, counts</span>):</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="literal">None</span>,(<span class="built_in">sum</span>(counts),word)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#利用heapq将数据进行排序，将最大的2个取出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">top_n_reducer</span>(<span class="params">self,_,word_cnts</span>):</span></span><br><span class="line">        <span class="keyword">for</span> cnt,word <span class="keyword">in</span> heapq.nlargest(<span class="number">2</span>,word_cnts):</span><br><span class="line">            <span class="keyword">yield</span> word,cnt</span><br><span class="line">    </span><br><span class="line">	<span class="comment">#实现steps方法用于指定自定义的mapper，comnbiner和reducer方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">steps</span>(<span class="params">self</span>):</span></span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            MRStep(mapper=self.mapper,</span><br><span class="line">                   combiner=self.combiner,</span><br><span class="line">                   reducer=self.reducer_sum),</span><br><span class="line">            MRStep(reducer=self.top_n_reducer)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    TopNWords.run()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>


<h3 id="MRJOB-文件合并"><a href="#MRJOB-文件合并" class="headerlink" title="MRJOB 文件合并"></a>MRJOB 文件合并</h3><p><strong>需求描述</strong></p>
<ul>
<li>两个文件合并 类似于数据库中的两张表合并</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">uid uname</span><br><span class="line">01 user1 </span><br><span class="line">02 user2</span><br><span class="line">03 user3</span><br><span class="line">uid orderid order_price</span><br><span class="line">01   01     80</span><br><span class="line">01   02     90</span><br><span class="line">02   03    82</span><br><span class="line">02   04    95</span><br></pre></td></tr></table></figure>



<p><strong>mrjob 实现</strong></p>
<p>实现对两个数据表进行join操作，显示效果为每个用户的所有订单信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;01:user1&quot;	&quot;01:80,02:90&quot;</span><br><span class="line">&quot;02:user2&quot;	&quot;03:82,04:95&quot;</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserOrderJoin</span>(<span class="params">MRJob</span>):</span></span><br><span class="line">    SORT_VALUES = <span class="literal">True</span></span><br><span class="line">    <span class="comment"># 二次排序参数：http://mrjob.readthedocs.io/en/latest/job.html</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span>(<span class="params">self, _, line</span>):</span></span><br><span class="line">        fields = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(fields) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># user data</span></span><br><span class="line">            source = <span class="string">&#x27;A&#x27;</span></span><br><span class="line">            user_id = fields[<span class="number">0</span>]</span><br><span class="line">            user_name = fields[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span>  user_id,[source,user_name] <span class="comment"># 01 [A,user1]</span></span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(fields) == <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># order data</span></span><br><span class="line">            source =<span class="string">&#x27;B&#x27;</span></span><br><span class="line">            user_id = fields[<span class="number">0</span>]</span><br><span class="line">            order_id = fields[<span class="number">1</span>]</span><br><span class="line">            price = fields[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">yield</span> user_id,[source,order_id,price] <span class="comment">#01 [&#x27;B&#x27;,01,80][&#x27;B&#x27;,02,90]</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span>(<span class="params">self,user_id,values</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        每个用户的订单列表</span></span><br><span class="line"><span class="string">        &quot;01:user1&quot;	&quot;01:80,02:90&quot;</span></span><br><span class="line"><span class="string">        &quot;02:user2&quot;	&quot;03:82,04:95&quot;</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        :param user_id:</span></span><br><span class="line"><span class="string">        :param values:[A,user1]  [&#x27;B&#x27;,01,80]</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        values = [v <span class="keyword">for</span> v <span class="keyword">in</span> values]</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(values)&gt;<span class="number">1</span> :</span><br><span class="line">            user_name = values[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">            order_info = [<span class="string">&#x27;:&#x27;</span>.join([v[<span class="number">1</span>],v[<span class="number">2</span>]]) <span class="keyword">for</span> v <span class="keyword">in</span> values[<span class="number">1</span>:]] <span class="comment">#[01:80,02:90]</span></span><br><span class="line">            <span class="keyword">yield</span> <span class="string">&#x27;:&#x27;</span>.join([user_id,user_name]),<span class="string">&#x27;,&#x27;</span>.join(order_info)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    UserOrderJoin.run()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>

<p>实现对两个数据表进行join操作，显示效果为每个用户所下订单的订单总量和累计消费金额</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&quot;01:user1&quot;	[2, 170]</span><br><span class="line">&quot;02:user2&quot;	[2, 177]</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">UserOrderJoin</span>(<span class="params">MRJob</span>):</span></span><br><span class="line">    <span class="comment"># 二次排序参数：http://mrjob.readthedocs.io/en/latest/job.html</span></span><br><span class="line">    SORT_VALUES = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span>(<span class="params">self, _, line</span>):</span></span><br><span class="line">        fields = line.strip().split(<span class="string">&#x27;\t&#x27;</span>)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(fields) == <span class="number">2</span>:</span><br><span class="line">            <span class="comment"># user data</span></span><br><span class="line">            source = <span class="string">&#x27;A&#x27;</span></span><br><span class="line">            user_id = fields[<span class="number">0</span>]</span><br><span class="line">            user_name = fields[<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">yield</span>  user_id,[source,user_name]</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">len</span>(fields) == <span class="number">3</span>:</span><br><span class="line">            <span class="comment"># order data</span></span><br><span class="line">            source =<span class="string">&#x27;B&#x27;</span></span><br><span class="line">            user_id = fields[<span class="number">0</span>]</span><br><span class="line">            order_id = fields[<span class="number">1</span>]</span><br><span class="line">            price = fields[<span class="number">2</span>]</span><br><span class="line">            <span class="keyword">yield</span> user_id,[source,order_id,price]</span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span>(<span class="params">self,user_id,values</span>):</span></span><br><span class="line">        <span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">        统计每个用户的订单数量和累计消费金额</span></span><br><span class="line"><span class="string">        :param user_id:</span></span><br><span class="line"><span class="string">        :param values:</span></span><br><span class="line"><span class="string">        :return:</span></span><br><span class="line"><span class="string">        &#x27;&#x27;&#x27;</span></span><br><span class="line">        values = [v <span class="keyword">for</span> v <span class="keyword">in</span> values]</span><br><span class="line">        user_name = <span class="literal">None</span></span><br><span class="line">        order_cnt = <span class="number">0</span></span><br><span class="line">        order_sum = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(values)&gt;<span class="number">1</span>:</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> values:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(v) ==  <span class="number">2</span> :</span><br><span class="line">                    user_name = v[<span class="number">1</span>]</span><br><span class="line">                <span class="keyword">elif</span> <span class="built_in">len</span>(v) == <span class="number">3</span>:</span><br><span class="line">                    order_cnt += <span class="number">1</span></span><br><span class="line">                    order_sum += <span class="built_in">int</span>(v[<span class="number">2</span>])</span><br><span class="line">            <span class="keyword">yield</span> <span class="string">&quot;:&quot;</span>.join([user_id,user_name]),(order_cnt,order_sum)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    UserOrderJoin().run()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()	</span><br></pre></td></tr></table></figure>


<h3 id="MapReduce原理详解"><a href="#MapReduce原理详解" class="headerlink" title="MapReduce原理详解"></a>MapReduce原理详解</h3><p><strong>单机程序计算流程</strong></p>
<p>输入数据—&gt;读取数据—&gt;处理数据—&gt;写入数据—&gt;输出数据</p>
<p><strong>Hadoop计算流程</strong></p>
<p>input data：输入数据</p>
<p>InputFormat：对数据进行切分，格式化处理</p>
<p>map：将前面切分的数据做map处理(将数据进行分类，输出(k,v)键值对数据)</p>
<p>shuffle&amp;sort:将相同的数据放在一起，并对数据进行排序处理</p>
<p>reduce：将map输出的数据进行hash计算，对每个map数据进行统计计算</p>
<p>OutputFormat：格式化输出数据</p>
<p><img src="mp3.png"></p>
<p><img src="mp4.png"></p>
<p><img src="mp5.png"></p>
<p><img src="mp6.png"></p>
<p><img src="mp1.png"></p>
<p>map：将数据进行处理</p>
<p>buffer in memory：达到80%数据时，将数据锁在内存上，将这部分输出到磁盘上</p>
<p>partitions：在磁盘上有很多”小的数据”，将这些数据进行归并排序。</p>
<p>merge on disk：将所有的”小的数据”进行合并。</p>
<p>reduce：不同的reduce任务，会从map中对应的任务中copy数据</p>
<p>​        在reduce中同样要进行merge操作</p>
<h3 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h3><ul>
<li>MapReduce架构 1.X<ul>
<li>JobTracker:负责接收客户作业提交，负责任务到作业节点上运行，检查作业的状态</li>
<li>TaskTracker：由JobTracker指派任务，定期向JobTracker汇报状态，在每一个工作节点上永远只会有一个TaskTracker</li>
</ul>
</li>
</ul>
<p><img src="image-MapReduce4.png"></p>
<ul>
<li><p>MapReduce2.X架构</p>
<ul>
<li>ResourceManager：负责资源的管理，负责提交任务到NodeManager所在的节点运行，检查节点的状态</li>
<li>NodeManager：由ResourceManager指派任务，定期向ResourceManager汇报状态</li>
</ul>
<p><img src="image-MapReduce5.png"></p>
</li>
</ul>
<h2 id="hadoop概念扩展"><a href="#hadoop概念扩展" class="headerlink" title="hadoop概念扩展"></a>hadoop概念扩展</h2><h3 id="Hadoop生态系统"><a href="#Hadoop生态系统" class="headerlink" title="Hadoop生态系统"></a>Hadoop生态系统</h3><p><strong>狭义的Hadoop VS 广义的Hadoop</strong></p>
<ul>
<li>广义的Hadoop：指的是Hadoop生态系统，Hadoop生态系统是一个很庞大的概念，hadoop是其中最重要最基础的一个部分，生态系统中每一子系统只解决某一个特定的问题域（甚至可能更窄），不搞统一型的全能系统，而是小而精的多个小系统；</li>
</ul>
<p><img src="hadoop-%E7%94%9F%E6%80%81.png"></p>
<p>Hive:数据仓库</p>
<p>R:数据分析</p>
<p>Mahout:机器学习库</p>
<p>pig：脚本语言，跟Hive类似</p>
<p>Oozie:工作流引擎，管理作业执行顺序</p>
<p>Zookeeper:用户无感知，主节点挂掉选择从节点作为主的</p>
<p>Flume:日志收集框架</p>
<p>Sqoop:数据交换框架，例如：关系型数据库与HDFS之间的数据交换</p>
<p>Hbase : 海量数据中的查询，相当于分布式文件系统中的数据库</p>
<p>Spark: 分布式的计算框架基于内存</p>
<ul>
<li>spark core</li>
<li>spark sql</li>
<li>spark streaming 准实时 不算是一个标准的流式计算</li>
<li>spark ML spark MLlib</li>
</ul>
<p>Kafka: 消息队列</p>
<p>Storm: 分布式的流式计算框架  python操作storm </p>
<p>Flink: 分布式的流式计算框架</p>
<p><strong>Hadoop生态系统的特点</strong></p>
<ul>
<li><p>开源、社区活跃</p>
</li>
<li><p>囊括了大数据处理的方方面面</p>
</li>
<li><p>成熟的生态圈</p>
</li>
</ul>
<h3 id="HDFS-读写流程-amp-高可用"><a href="#HDFS-读写流程-amp-高可用" class="headerlink" title="HDFS 读写流程 &amp; 高可用"></a>HDFS 读写流程 &amp; 高可用</h3><ul>
<li><p>HDFS读写流程</p>
<p><img src="hdfs_read_write/a.jpg"></p>
<p><img src="hdfs_read_write/b.jpg"></p>
<p><img src="hdfs_read_write/c.jpg"></p>
<p><img src="hdfs_read_write/d.jpg"></p>
<ul>
<li><p>客户端向NameNode发出写文件请求。</p>
</li>
<li><p>检查是否已存在文件、检查权限。若通过检查，直接先将操作写入EditLog，并返回输出流对象。<br>（注：WAL，write ahead log，先写Log，再写内存，因为EditLog记录的是最新的HDFS客户端执行所有的写操作。如果后续真实写操作失败了，由于在真实写操作之前，操作就被写入EditLog中了，故EditLog中仍会有记录，我们不用担心后续client读不到相应的数据块，因为在第5步中DataNode收到块后会有一返回确认信息，若没写成功，发送端没收到确认信息，会一直重试，直到成功）</p>
</li>
<li><p>client端按128MB的块切分文件。</p>
</li>
<li><p>client将NameNode返回的分配的可写的DataNode列表和Data数据一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。client每向第一个DataNode写入一个packet，这个packet便会直接在pipeline里传给第二个、第三个…DataNode。<br>（注：并不是写好一个块或一整个文件后才向后分发）</p>
</li>
<li><p>每个DataNode写完一个块后，会返回确认信息。<br>（注：并不是每写完一个packet后就返回确认信息，个人觉得因为packet中的每个chunk都携带校验信息，没必要每写一个就汇报一下，这样效率太慢。正确的做法是写完一个block块后，对校验信息进行汇总分析，就能得出是否有块写错的情况发生）</p>
</li>
<li><p>写完数据，关闭输输出流。</p>
</li>
<li><p>发送完成信号给NameNode。</p>
<p>（注：发送完成信号的时机取决于集群是强一致性还是最终一致性，强一致性则需要所有DataNode写完后才向NameNode汇报。最终一致性则其中任意一个DataNode写完后就能单独向NameNode汇报，HDFS一般情况下都是强调强一致性） </p>
</li>
</ul>
</li>
<li><p>HDFS如何实现高可用(HA)</p>
<ul>
<li>数据存储故障容错<ul>
<li>磁盘介质在存储过程中受环境或者老化影响,数据可能错乱</li>
<li>对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum)</li>
<li>读取数据的时候, 重新计算读取出来的数据校验和, 校验不正确抛出异常, 从其它DataNode上读取备份数据</li>
</ul>
</li>
<li>磁盘故障容错<ul>
<li>DataNode 监测到本机的某块磁盘损坏</li>
<li>将该块磁盘上存储的所有 BlockID 报告给 NameNode</li>
<li>NameNode 检查这些数据块在哪些DataNode上有备份,</li>
<li>通知相应DataNode, 将数据复制到其他服务器上</li>
</ul>
</li>
<li>DataNode故障容错<ul>
<li>通过心跳和NameNode保持通讯</li>
<li>超时未发送心跳, NameNode会认为这个DataNode已经宕机</li>
<li>NameNode查找这个DataNode上有哪些数据块, 以及这些数据在其它DataNode服务器上的存储情况</li>
<li>从其它DataNode服务器上复制数据</li>
</ul>
</li>
<li>NameNode故障容错<ul>
<li>主从热备 secondary namenode</li>
<li>zookeeper配合 master节点选举</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Hadoop发行版的选择"><a href="#Hadoop发行版的选择" class="headerlink" title="Hadoop发行版的选择"></a>Hadoop发行版的选择</h3><ul>
<li><p>Apache Hadoop</p>
<ul>
<li>开源社区版</li>
<li>最新的Hadoop版本都是从Apache Hadoop发布的</li>
<li>Hadoop Hive Flume  版本不兼容的问题 jar包  spark scala  Java-&gt;.class-&gt;.jar -&gt;JVM</li>
</ul>
</li>
<li><p>CDH: Cloudera Distributed Hadoop</p>
<ul>
<li><p>Cloudera 在社区版的基础上做了一些修改</p>
</li>
<li><p><a target="_blank" rel="noopener" href="http://archive.cloudera.com/cdh5/cdh/5/">http://archive.cloudera.com/cdh5/cdh/5/</a></p>
<p><img src="cdh.png"></p>
</li>
<li><p>hadoop-2.6.0-cdh-5.7.0 和 Flume*****-cdh5.7.0 cdh版本一致 的各个组件配合是有不会有兼容性问题</p>
</li>
<li><p>CDH版本的这些组件 没有全部开源</p>
</li>
</ul>
</li>
<li><p>HDP: Hortonworks Data Platform</p>
</li>
</ul>
<h3 id="大数据产品与互联网产品结合"><a href="#大数据产品与互联网产品结合" class="headerlink" title="大数据产品与互联网产品结合"></a>大数据产品与互联网产品结合</h3><ul>
<li>分布式系统执行任务瓶颈: 延迟高 MapReduce 几分钟 Spark几秒钟</li>
<li>互联网产品要求<ul>
<li>毫秒级响应(1秒以内完成)</li>
<li>需要通过大数据实现 统计分析 数据挖掘 关联推荐 用户画像</li>
</ul>
</li>
<li>大数据平台<ul>
<li>整合网站应用和大数据系统之间的差异, 将应用产生的数据导入到大数据系统, 经过处理计算后再导出给应用程序使用</li>
</ul>
</li>
<li>互联网大数据平台架构:</li>
</ul>
<p><img src="bigdata_arcit.png"></p>
<ul>
<li><p>数据采集</p>
<ul>
<li>App/Web 产生的数据&amp;日志同步到大数据系统</li>
<li>数据库同步:Sqoop  日志同步:Flume 打点: Kafka</li>
<li>不同数据源产生的数据质量可能差别很大<ul>
<li>数据库 也许可以直接用</li>
<li>日志 爬虫 大量的清洗,转化处理 </li>
</ul>
</li>
</ul>
</li>
<li><p>数据处理</p>
<ul>
<li>大数据存储与计算的核心</li>
<li>数据同步后导入HDFS</li>
<li>MapReduce Hive Spark 读取数据进行计算 结果再保存到HDFS</li>
<li>MapReduce Hive Spark 离线计算, HDFS 离线存储<ul>
<li>离线计算通常针对(某一类别)全体数据, 比如 历史上所有订单</li>
<li>离线计算特点: 数据规模大, 运行时间长</li>
</ul>
</li>
<li>流式计算<ul>
<li>淘宝双11 每秒产生订单数 监控宣传</li>
<li>Storm(毫秒) SparkStreaming(秒)</li>
</ul>
</li>
</ul>
</li>
<li><p>数据输出与展示</p>
<ul>
<li>HDFS需要把数据导出交给应用程序, 让用户实时展示  ECharts<ul>
<li>淘宝卖家量子魔方</li>
</ul>
</li>
<li>给运营和决策层提供各种统计报告, 数据需要写入数据库<ul>
<li>很多运营管理人员, 上班后就会登陆后台数据系统</li>
</ul>
</li>
</ul>
</li>
<li><p>任务调度系统</p>
<ul>
<li>将上面三个部分整合起来</li>
</ul>
</li>
</ul>
<h3 id="大数据应用–数据分析"><a href="#大数据应用–数据分析" class="headerlink" title="大数据应用–数据分析"></a>大数据应用–数据分析</h3><ul>
<li><p>通过数据分析指标监控企业运营状态, 及时调整运营和产品策略,是大数据技术的关键价值之一</p>
</li>
<li><p>大数据平台(互联网企业)运行的绝大多数大数据计算都是关于数据分析的</p>
<ul>
<li>统计指标</li>
<li>关联分析,</li>
<li>汇总报告,</li>
</ul>
</li>
<li><p>运营数据是公司管理的基础</p>
<ul>
<li>了解公司目前发展的状况</li>
<li>数据驱动运营: 调节指标对公司进行管理</li>
</ul>
</li>
<li><p>运营数据的获取需要大数据平台的支持</p>
<ul>
<li>埋点采集数据</li>
<li>数据库,日志 三方采集数据</li>
<li>对数据清洗 转换 存储 </li>
<li>利用SQL进行数据统计 汇总 分析</li>
<li>得到需要的运营数据报告</li>
</ul>
</li>
<li><p>运营常用数据指标</p>
<ul>
<li><p>新增用户数  UG  user growth 用户增长</p>
<ul>
<li>产品增长性的关键指标</li>
<li>新增访问网站(新下载APP)的用户数</li>
</ul>
</li>
<li><p>用户留存率</p>
<ul>
<li>用户留存率 = 留存用户数 / 当期新增用户数</li>
<li>3日留存  5日留存 7日留存</li>
</ul>
</li>
<li><p>活跃用户数</p>
<ul>
<li>打开使用产品的用户</li>
<li>日活</li>
<li>月活</li>
<li>提升活跃是网站运营的重要目标</li>
</ul>
</li>
<li><p>PV Page View</p>
<ul>
<li>打开产品就算活跃</li>
<li>打开以后是否频繁操作就用PV衡量, 每次点击, 页面跳转都记一次PV</li>
</ul>
</li>
<li><p>GMV</p>
<ul>
<li>成交总金额(Gross Merchandise Volume) 电商网站统计营业额, 反应网站应收能力的重要指标</li>
<li>GMV相关的指标: 订单量 客单价</li>
</ul>
</li>
<li><p>转化率<br>转化率 = 有购买行为的用户数 / 总访问用户数</p>
</li>
</ul>
</li>
</ul>
<h3 id="数据分析案例"><a href="#数据分析案例" class="headerlink" title="数据分析案例"></a>数据分析案例</h3><ul>
<li><p>背景: 某电商网站, 垂直领域领头羊, 各项指标相对稳定</p>
</li>
<li><p>运营人员发现从 8 月 15 日开始，网站的订单量连续四天明显下跌</p>
</li>
<li><p>8 月 18 号早晨发现 8 月 17 号的订单量没有恢复正常，运营人员开始尝试寻找原因</p>
<ul>
<li>是否有负面报道被扩散</li>
<li>是否竞争对手在做活动</li>
<li>是否某类商品缺货</li>
<li>价格异常</li>
</ul>
</li>
<li><p>没有找到原因, 将问题交给数据分析团队</p>
<p><img src="case1.png"></p>
</li>
<li><p>数据分析师分析可能性</p>
<ul>
<li>新增用户出现问题</li>
<li>查看日活数据, 发现日活没有明显下降<ul>
<li>基本判断, 用户在访问网站的过程中,转化出了问题</li>
</ul>
</li>
</ul>
<p><img src="case2.png"></p>
</li>
<li><p>转化过程:</p>
<ul>
<li>打开APP</li>
<li>搜索关键词 浏览搜索结果列表</li>
<li>点击商品访问详情</li>
<li>有购买意向开始咨询</li>
<li>放入购物车</li>
<li>支付</li>
</ul>
<p><img src="case3.png"></p>
</li>
<li><p>订单活跃转化率 = 日订单量 / 打开用户数</p>
</li>
<li><p>搜索打开转化率 = 搜索用户数 / 打开用户数</p>
</li>
<li><p>有明显降幅的是咨询详情转化率</p>
<p><img src="case4.png"></p>
<ul>
<li>对咨询信息分类统计后发现，新用户的咨询量几乎为 0</li>
<li>于是将问题提交给技术部门调查，工程师查看 8 月 15 日当天发布记录,发现有消息队列SDK更新</li>
</ul>
</li>
</ul>
<p><strong>Hadoop企业应用案例之消费大数据</strong></p>
<p>亚马逊提前发货系统</p>
<p><strong>Hadoop企业案例之商业零售大数据</strong></p>
<p>智能推荐</p>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">贪钱算法还我头发</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/">https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/01/18/5.4-Hive/"><img class="prev-cover" src="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Hive &amp; HBase</div></div></a></div><div class="next-post pull-right"><a href="/2022/01/18/5.2-RS-Algorithm/"><img class="next-cover" src="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Recommendation System Algorithm</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/09/Technical-Summary-Deep-Learning/" title="Technical Summary —— Pytorch & Numpy"><img class="cover" src="http://img.shijue.me/21cdab996c944862bac3947c8c7197f1_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-09</div><div class="title">Technical Summary —— Pytorch & Numpy</div></div></a></div><div><a href="/2024/07/02/Technical-Summary-Necessary/" title="Technical Summary —— Common Command"><img class="cover" src="http://img.shijue.me/82cad50b40214d388d246ae4847e8bec_d.jpeg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Technical Summary —— Common Command</div></div></a></div><div><a href="/2024/04/15/Technical-Summary-BigData/" title="Technical Summary —— Big Data Processing"><img class="cover" src="http://img.shijue.me/886e06c4e9b04b8a836a1b6ec00a0b17_d.jpeg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-15</div><div class="title">Technical Summary —— Big Data Processing</div></div></a></div><div><a href="/2024/04/07/Technical-Summary-Database/" title="Technical Summary —— Database Usage"><img class="cover" src="http://img.shijue.me/078729117f75472bae4bbc684d2b714c_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-07</div><div class="title">Technical Summary —— Database Usage</div></div></a></div><div><a href="/2023/05/19/Summary-NG/" title="Summary NG"><img class="cover" src="http://img.shijue.me/0a336cc060e74ca29f9cf862eb2ee8cf_d.png!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Summary NG</div></div></a></div><div><a href="/2023/05/19/Experimental-Technique/" title="Experimental Technique"><img class="cover" src="http://img.shijue.me/c94bd493552d441ab4c647296a6913b5_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Experimental Technique</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">贪钱算法还我头发</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">52</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">14</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xfliu1998"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xfliu1998" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuxiaofei_7@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/keiven_" target="_blank" title="CSDN"><i class="fa fa-address-card"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">欢迎来这里掉头发</div></div><div class="card-widget" id="newYear"><div class="item-headline"><i></i><span></span></div><div class="item-content"><div id="newYear-main"><div class="mask"></div> <p class="title"></p> <div class="newYear-time"></div> <p class="today" style="text-align: right;"></p> </div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Hadoop"><span class="toc-number">1.</span> <span class="toc-text">Hadoop</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFHadoop"><span class="toc-number">1.1.</span> <span class="toc-text">什么是Hadoop</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6"><span class="toc-number">1.2.</span> <span class="toc-text">Hadoop核心组件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E4%BC%98%E5%8A%BF"><span class="toc-number">1.3.</span> <span class="toc-text">Hadoop优势</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F-HDFS"><span class="toc-number">2.</span> <span class="toc-text">分布式文件系统 HDFS</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%9A%84%E4%BD%BF%E7%94%A8"><span class="toc-number">2.1.</span> <span class="toc-text">HDFS的使用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-shell%E6%93%8D%E4%BD%9C"><span class="toc-number">2.2.</span> <span class="toc-text">HDFS shell操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-shell%E6%93%8D%E4%BD%9C%E7%BB%83%E4%B9%A0"><span class="toc-number">2.3.</span> <span class="toc-text">HDFS shell操作练习</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">2.4.</span> <span class="toc-text">HDFS环境搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#YARN"><span class="toc-number">3.</span> <span class="toc-text">YARN</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFYARN"><span class="toc-number">3.1.</span> <span class="toc-text">什么是YARN</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN%E4%BA%A7%E7%94%9F%E8%83%8C%E6%99%AF"><span class="toc-number">3.2.</span> <span class="toc-text">YARN产生背景</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN%E7%9A%84%E6%9E%B6%E6%9E%84%E5%92%8C%E6%89%A7%E8%A1%8C%E6%B5%81%E7%A8%8B"><span class="toc-number">3.3.</span> <span class="toc-text">YARN的架构和执行流程</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#YARN%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="toc-number">3.4.</span> <span class="toc-text">YARN环境搭建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6-MapReduce"><span class="toc-number">4.</span> <span class="toc-text">分布式处理框架 MapReduce</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFMapReduce"><span class="toc-number">4.1.</span> <span class="toc-text">什么是MapReduce</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="toc-number">4.2.</span> <span class="toc-text">MapReduce编程模型</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop-Streaming-%E5%AE%9E%E7%8E%B0wordcount-%EF%BC%88%E5%AE%9E%E9%AA%8C-%E4%BA%86%E8%A7%A3%EF%BC%89"><span class="toc-number">4.3.</span> <span class="toc-text">Hadoop Streaming 实现wordcount （实验 了解）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8MRJob%E7%BC%96%E5%86%99%E5%92%8C%E8%BF%90%E8%A1%8CMapReduce%E4%BB%A3%E7%A0%81"><span class="toc-number">4.4.</span> <span class="toc-text">利用MRJob编写和运行MapReduce代码</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BF%90%E8%A1%8CMRJOB%E7%9A%84%E4%B8%8D%E5%90%8C%E6%96%B9%E5%BC%8F"><span class="toc-number">4.5.</span> <span class="toc-text">运行MRJOB的不同方式</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#mrjob-%E5%AE%9E%E7%8E%B0-topN%E7%BB%9F%E8%AE%A1%EF%BC%88%E5%AE%9E%E9%AA%8C%EF%BC%89"><span class="toc-number">4.6.</span> <span class="toc-text">mrjob 实现 topN统计（实验）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MRJOB-%E6%96%87%E4%BB%B6%E5%90%88%E5%B9%B6"><span class="toc-number">4.7.</span> <span class="toc-text">MRJOB 文件合并</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce%E5%8E%9F%E7%90%86%E8%AF%A6%E8%A7%A3"><span class="toc-number">4.8.</span> <span class="toc-text">MapReduce原理详解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#MapReduce%E6%9E%B6%E6%9E%84"><span class="toc-number">4.9.</span> <span class="toc-text">MapReduce架构</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#hadoop%E6%A6%82%E5%BF%B5%E6%89%A9%E5%B1%95"><span class="toc-number">5.</span> <span class="toc-text">hadoop概念扩展</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F"><span class="toc-number">5.1.</span> <span class="toc-text">Hadoop生态系统</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#HDFS-%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B-amp-%E9%AB%98%E5%8F%AF%E7%94%A8"><span class="toc-number">5.2.</span> <span class="toc-text">HDFS 读写流程 &amp; 高可用</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E7%9A%84%E9%80%89%E6%8B%A9"><span class="toc-number">5.3.</span> <span class="toc-text">Hadoop发行版的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E4%BA%A7%E5%93%81%E4%B8%8E%E4%BA%92%E8%81%94%E7%BD%91%E4%BA%A7%E5%93%81%E7%BB%93%E5%90%88"><span class="toc-number">5.4.</span> <span class="toc-text">大数据产品与互联网产品结合</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BA%94%E7%94%A8%E2%80%93%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="toc-number">5.5.</span> <span class="toc-text">大数据应用–数据分析</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90%E6%A1%88%E4%BE%8B"><span class="toc-number">5.6.</span> <span class="toc-text">数据分析案例</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 贪钱算法还我头发</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '1b0c10ce649501ea4a72',
      clientSecret: '741b5e861137e3d5a482bba272c8201b78da6cb0',
      repo: 'xfliu1998.github.io',
      owner: 'xfliu1998',
      admin: ['xfliu1998'],
      id: '84cce50eb4b2cd1a10b673c916ccbf31',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: true,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/script.js?v1"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><script async data-pjax src="/js/newYear.js"></script><script async src="//at.alicdn.com/t/font_2264842_b004iy0kk2b.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='all'|| 'all' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Machine-Learning-and-Deep-Learning/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习与深度学习 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Structures-and-Algorithms/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">😼 数据结构与算法 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Search-Advertisement-Recommendation-Causal/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗂️ 搜索/广告/推荐/因果 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Analysis-and-Processing/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数据分析与处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Reading-Notes/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 阅读笔记 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Daily/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 日常随想 (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://xfliu1998.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-NLP/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-NLP/" alt="">Papers Reading about NLP</a><div class="blog-slider__text">自然语言处理论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-NLP/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/10/01/9-3D-Construction/" alt=""><img width="48" height="48" src="https://img.zcool.cn/community/017d495d0e6a9ea801205e4b7fa13c.png@1280w_1l_2o_100sh.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-10-01</span><a class="blog-slider__title" href="2022/10/01/9-3D-Construction/" alt="">3D Construction</a><div class="blog-slider__text">三维重建基础</div><a class="blog-slider__button" href="2022/10/01/9-3D-Construction/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/01/11/SfM-SLAM/" alt=""><img width="48" height="48" src="https://tse2-mm.cn.bing.net/th/id/OIP-C.V5uTTQ6LTBHc42xoBPG8hAHaEm?pid=ImgDet&amp;rs=1" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-11</span><a class="blog-slider__title" href="2023/01/11/SfM-SLAM/" alt="">SfM &amp; SLAM</a><div class="blog-slider__text">SfM和SLAM系统</div><a class="blog-slider__button" href="2023/01/11/SfM-SLAM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/03/25/Papers-Ideas/" alt=""><img width="48" height="48" src="https://tse2-mm.cn.bing.net/th/id/OIP-C.Mmv8iGEVFxSQII6QH0BG9QHaEJ?w=302&amp;h=180&amp;c=7&amp;r=0&amp;o=5&amp;dpr=2&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-25</span><a class="blog-slider__title" href="2023/03/25/Papers-Ideas/" alt="">Papers Ideas</a><div class="blog-slider__text">大模型时代下的科研思路</div><a class="blog-slider__button" href="2023/03/25/Papers-Ideas/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Summary/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Summary/" alt="">Papers Summary</a><div class="blog-slider__text">论文总结笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Summary/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-CV/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-CV/" alt="">Papers Reading about CV</a><div class="blog-slider__text">计算机视觉论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-CV/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/06/05/Interview-Experience/" alt=""><img width="48" height="48" src="http://img.shijue.me/00964c481ad34d78acbf148d2b391c9e_d.jpg!dp6" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-05</span><a class="blog-slider__title" href="2023/06/05/Interview-Experience/" alt="">Interview Experience</a><div class="blog-slider__text">面经八股</div><a class="blog-slider__button" href="2023/06/05/Interview-Experience/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/01/21/Learning-Framework/" alt=""><img width="48" height="48" src="http://img.shijue.me/78ae8b05a73444cd9643a8312abc0d43.jpg!dp6" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-21</span><a class="blog-slider__title" href="2022/01/21/Learning-Framework/" alt="">Learning Framework</a><div class="blog-slider__text">学习大纲</div><a class="blog-slider__button" href="2022/01/21/Learning-Framework/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<container><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></container>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.fomal.cc/api?xfliu1998",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xfliu1998')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>