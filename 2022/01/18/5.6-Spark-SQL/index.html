<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark SQL &amp; Spark streaming | 一直进步 做喜欢的</title><meta name="keywords" content="python"><meta name="author" content="贪钱算法还我头发"><meta name="copyright" content="贪钱算法还我头发"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="推荐系统学习笔记六——Spark SQL &amp; Spark streaming">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark SQL &amp; Spark streaming">
<meta property="og:url" content="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/index.html">
<meta property="og:site_name" content="一直进步 做喜欢的">
<meta property="og:description" content="推荐系统学习笔记六——Spark SQL &amp; Spark streaming">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6">
<meta property="article:published_time" content="2022-01-18T13:44:59.000Z">
<meta property="article:modified_time" content="2024-03-31T12:12:56.325Z">
<meta property="article:author" content="贪钱算法还我头发">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark SQL & Spark streaming',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2024-03-31 20:12:56'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><link rel="stylesheet" href="/css/custom.css?v1"><link rel="stylesheet" href="//at.alicdn.com/t/font_2264842_b004iy0kk2b.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">56</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一直进步 做喜欢的</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark SQL &amp; Spark streaming</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-01-18T13:44:59.000Z" title="Created 2022-01-18 21:44:59">2022-01-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2024-03-31T12:12:56.325Z" title="Updated 2024-03-31 20:12:56">2024-03-31</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Search-Advertisement-Recommendation-Causal/">Search / Advertisement / Recommendation / Causal</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">8.2k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>34min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark SQL &amp; Spark streaming"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2022/01/18/5.6-Spark-SQL/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>推荐系统学习笔记目录</strong></p>
<ol>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.1-Recommendation-System-Introduction/">推荐系统介绍</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.2-RS-Algorithm/">推荐算法</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/">Hadoop</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.4-Hive/">Hive &amp; HBase</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/">Spark core</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/">Spark SQL &amp; Spark streaming</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.7-RS-case/">推荐系统案例</a></li>
</ol>
<h1 id="Spark-SQL"><a href="#Spark-SQL" class="headerlink" title="Spark SQL"></a>Spark SQL</h1><p><strong>Spark SQL概念</strong></p>
<ul>
<li>Spark SQL is Apache Spark’s module for working with structured data.<ul>
<li>它是spark中用于处理结构化数据的一个模块</li>
</ul>
</li>
</ul>
<p><strong>Spark SQL历史</strong></p>
<ul>
<li>Hive是目前大数据领域，事实上的数据仓库标准。</li>
</ul>
<p><img src="s9.PNG" alt="s9"></p>
<ul>
<li>Shark：shark底层使用spark的基于内存的计算模型，从而让性能比Hive提升了数倍到上百倍。</li>
<li>底层很多东西还是依赖于Hive，修改了内存管理、物理计划、执行三个模块</li>
<li>2014年6月1日的时候，Spark宣布了不再开发Shark，全面转向Spark SQL的开发</li>
</ul>
<p><strong>Spark SQL优势</strong></p>
<ul>
<li>Write Less Code<br><img src="s10.PNG" alt="s10"></li>
</ul>
<ul>
<li>Performance<br><img src="s11.PNG" alt="s11"></li>
</ul>
<p>python操作RDD，转换为可执行代码，运行在java虚拟机，涉及两个不同语言引擎之间的切换，进行进程间        通信很耗费性能。</p>
<p>DataFrame</p>
<ul>
<li>是RDD为基础的分布式数据集，类似于传统关系型数据库的二维表，dataframe记录了对应列的名称和类型</li>
<li>dataFrame引入schema和off-heap(使用操作系统层面上的内存)<ul>
<li>1、解决了RDD的缺点</li>
<li>序列化和反序列化开销大</li>
<li>频繁的创建和销毁对象造成大量的GC</li>
<li>2、丢失了RDD的优点</li>
<li>RDD编译时进行类型检查</li>
<li>RDD具有面向对象编程的特性</li>
</ul>
</li>
</ul>
<p>用scala/python编写的RDD比Spark SQL编写转换的RDD慢，涉及到执行计划</p>
<ul>
<li>CatalystOptimizer：Catalyst优化器</li>
<li>ProjectTungsten：钨丝计划，为了提高RDD的效率而制定的计划</li>
<li>Code gen：代码生成器</li>
</ul>
<p><img src="s12.PNG" alt="s12"></p>
<p>直接编写RDD也可以自实现优化代码，但是远不及SparkSQL前面的优化操作后转换的RDD效率高，快1倍左右</p>
<p>优化引擎：类似mysql等关系型数据库基于成本的优化器</p>
<p>首先执行逻辑执行计划，然后转换为物理执行计划(选择成本最小的)，通过Code Generation最终生成为RDD</p>
<ul>
<li><p>Language-independent API</p>
<p>用任何语言编写生成的RDD都一样，而使用spark-core编写的RDD，不同的语言生成不同的RDD</p>
</li>
</ul>
<ul>
<li><p>Schema</p>
<p>结构化数据，可以直接看出数据的详情</p>
<p>在RDD中无法看出，解释性不强，无法告诉引擎信息，没法详细优化。</p>
</li>
</ul>
<p>**为什么要学习sparksql **</p>
<p>sparksql特性</p>
<ul>
<li> 1、易整合</li>
<li> 2、统一的数据源访问</li>
<li> 3、兼容hive</li>
<li> 4、提供了标准的数据库连接（jdbc/odbc）</li>
</ul>
<h1 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h1><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>在Spark语义中，DataFrame是一个分布式的<strong>行集合</strong>，可以想象为一个关系型数据库的表，或者一个带有列名的Excel表格。它和RDD一样，有这样一些特点：</p>
<ul>
<li>Immuatable：一旦RDD、DataFrame被创建，就不能更改，只能通过transformation生成新的RDD、DataFrame</li>
<li>Lazy Evaluations：只有action才会触发Transformation的执行</li>
<li>Distributed：DataFrame和RDD一样都是分布式的</li>
<li>dataframe和dataset统一，dataframe只是dataset[ROW]的类型别名。由于Python是弱类型语言，只能使用DataFrame</li>
</ul>
<p><strong>DataFrame vs RDD</strong></p>
<ul>
<li>RDD：分布式的对象的集合，Spark并不知道对象的详细模式信息</li>
<li>DataFrame：分布式的Row对象的集合，其提供了由列组成的详细模式信息，使得Spark SQL可以进行某些形式的执行优化。</li>
<li>DataFrame和普通的RDD的逻辑框架区别如下所示：</li>
</ul>
<p><img src="s13.PNG" alt="s13"></p>
<ul>
<li><p>左侧的RDD Spark框架本身不了解 Person类的内部结构。</p>
</li>
<li><p>右侧的DataFrame提供了详细的结构信息（schema——每列的名称，类型）</p>
</li>
<li><p>DataFrame还配套了新的操作数据的方法，DataFrame API（如df.select())和SQL(select id, name from xx_table where …)。</p>
</li>
<li><p>DataFrame还引入了off-heap,意味着JVM堆以外的内存, 这些内存直接受操作系统管理（而不是JVM）。</p>
</li>
<li><p>RDD是分布式的Java对象的集合。DataFrame是分布式的Row对象的集合。DataFrame除了提供了比RDD更丰富的算子以外，更重要的特点是提升执行效率、减少数据读取以及执行计划的优化。</p>
</li>
<li><p>DataFrame的抽象后，我们处理数据更加简单了，甚至可以用SQL来处理数据了</p>
</li>
<li><p>通过DataFrame API或SQL处理数据，会自动经过Spark 优化器（Catalyst）的优化，即使你写的程序或SQL不仅高效，也可以运行的很快。</p>
</li>
<li><p>DataFrame相当于是一个带着schema的RDD</p>
</li>
</ul>
<p><strong>Pandas DataFrame vs Spark DataFrame</strong></p>
<ul>
<li>Cluster Parallel：集群并行执行</li>
<li>Lazy Evaluations: 只有action才会触发Transformation的执行</li>
<li>Immutable：不可更改</li>
<li>Pandas rich API：比Spark SQL api丰富</li>
</ul>
<h2 id="创建DataFrame"><a href="#创建DataFrame" class="headerlink" title="创建DataFrame"></a>创建DataFrame</h2><ol>
<li><p>创建dataFrame的步骤<br>调用方法例如：spark.read.xxx方法</p>
</li>
<li><p>其他方式创建dataframe</p>
</li>
</ol>
<ul>
<li><p>createDataFrame：pandas dataframe、list、RDD</p>
</li>
<li><p>数据源：RDD、csv、json、parquet、orc、jdbc</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">  jsonDF = spark.read.json(<span class="string">&quot;xxx.json&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  jsonDF = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>).load(<span class="string">&#x27;xxx.json&#x27;</span>)</span><br><span class="line">  </span><br><span class="line">  parquetDF = spark.read.parquet(<span class="string">&quot;xxx.parquet&quot;</span>)</span><br><span class="line">  </span><br><span class="line">  jdbcDF = spark.read.<span class="built_in">format</span>(<span class="string">&quot;jdbc&quot;</span>).option(<span class="string">&quot;url&quot;</span>,<span class="string">&quot;jdbc:mysql://localhost:3306/db_name&quot;</span>).option(<span class="string">&quot;dbtable&quot;</span>,<span class="string">&quot;table_name&quot;</span>).option(<span class="string">&quot;user&quot;</span>,<span class="string">&quot;xxx&quot;</span>).option(<span class="string">&quot;password&quot;</span>,<span class="string">&quot;xxx&quot;</span>).load()</span><br><span class="line">  PNG</span><br><span class="line"></span><br><span class="line">- Transformation:延迟性操作</span><br><span class="line"></span><br><span class="line">- action：立即操作</span><br><span class="line"></span><br><span class="line">  ![s14](s14.PNG)</span><br><span class="line"></span><br><span class="line"><span class="comment">## DataFrame API实现</span></span><br><span class="line"></span><br><span class="line">**基于RDD创建**</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> Row</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&#x27;test&#x27;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"><span class="comment"># spark.conf.set(&quot;spark.sql.shuffle.partitions&quot;, 6)</span></span><br><span class="line"><span class="comment"># ================直接创建==========================</span></span><br><span class="line">l = [(<span class="string">&#x27;Ankit&#x27;</span>,<span class="number">25</span>),(<span class="string">&#x27;Jalfaizy&#x27;</span>,<span class="number">22</span>),(<span class="string">&#x27;saurabh&#x27;</span>,<span class="number">20</span>),(<span class="string">&#x27;Bala&#x27;</span>,<span class="number">26</span>)]</span><br><span class="line">rdd = sc.parallelize(l)</span><br><span class="line"><span class="comment">#为数据添加列名</span></span><br><span class="line">people = rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x: Row(name=x[<span class="number">0</span>], age=<span class="built_in">int</span>(x[<span class="number">1</span>])))</span><br><span class="line"><span class="comment">#创建DataFrame</span></span><br><span class="line">schemaPeople = spark.createDataFrame(people)</span><br></pre></td></tr></table></figure></li>
</ul>
<p><strong>从csv中读取数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================从csv读取======================</span></span><br><span class="line"><span class="comment">#加载csv类型的数据并转换为DataFrame</span></span><br><span class="line">df = spark.read.<span class="built_in">format</span>(<span class="string">&quot;csv&quot;</span>). \</span><br><span class="line">    option(<span class="string">&quot;header&quot;</span>, <span class="string">&quot;true&quot;</span>) \</span><br><span class="line">    .load(<span class="string">&quot;iris.csv&quot;</span>)</span><br><span class="line"><span class="comment">#显示数据结构</span></span><br><span class="line">df.printSchema()</span><br><span class="line"><span class="comment">#显示前10条数据</span></span><br><span class="line">df.show(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#统计总量</span></span><br><span class="line">df.count()</span><br><span class="line"><span class="comment">#列名</span></span><br><span class="line">df.columns</span><br></pre></td></tr></table></figure>

<p><strong>增加一列</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===============增加一列(或者替换) withColumn===========</span></span><br><span class="line"><span class="comment">#定义一个新的列，数据为其他某列数据的两倍</span></span><br><span class="line"><span class="comment">#如果操作的是原有列，可以替换原有列的数据</span></span><br><span class="line">df.withColumn(<span class="string">&#x27;newWidth&#x27;</span>,df.SepalWidth * <span class="number">2</span>).show()</span><br></pre></td></tr></table></figure>

<p><strong>删除一列</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==========删除一列  drop=========================</span></span><br><span class="line"><span class="comment">#删除一列</span></span><br><span class="line">df.drop(<span class="string">&#x27;cls&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<p><strong>统计信息</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================ 统计信息 describe================</span></span><br><span class="line">df.describe().show()</span><br><span class="line"><span class="comment">#计算某一列的描述信息</span></span><br><span class="line">df.describe(<span class="string">&#x27;cls&#x27;</span>).show()   </span><br></pre></td></tr></table></figure>

<p><strong>提取部分列</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ===============提取部分列 select==============</span></span><br><span class="line">df.select(<span class="string">&#x27;SepalLength&#x27;</span>,<span class="string">&#x27;SepalWidth&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<p><strong>基本统计功能</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ==================基本统计功能 distinct count=====</span></span><br><span class="line">df.select(<span class="string">&#x27;cls&#x27;</span>).distinct().count()</span><br></pre></td></tr></table></figure>

<p><strong>分组统计</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 分组统计 groupby(colname).agg(&#123;&#x27;col&#x27;:&#x27;fun&#x27;,&#x27;col2&#x27;:&#x27;fun2&#x27;&#125;)</span></span><br><span class="line">df.groupby(<span class="string">&#x27;cls&#x27;</span>).agg(&#123;<span class="string">&#x27;SepalWidth&#x27;</span>:<span class="string">&#x27;mean&#x27;</span>,<span class="string">&#x27;SepalLength&#x27;</span>:<span class="string">&#x27;max&#x27;</span>&#125;).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># avg(), count(), countDistinct(), first(), kurtosis(),</span></span><br><span class="line"><span class="comment"># max(), mean(), min(), skewness(), stddev(), stddev_pop(),</span></span><br><span class="line"><span class="comment"># stddev_samp(), sum(), sumDistinct(), var_pop(), var_samp() and variance()</span></span><br></pre></td></tr></table></figure>

<p><strong>自定义的汇总方法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 自定义的汇总方法</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line"><span class="comment">#调用函数并起一个别名</span></span><br><span class="line">df.agg(fn.count(<span class="string">&#x27;SepalWidth&#x27;</span>).alias(<span class="string">&#x27;width_count&#x27;</span>),fn.countDistinct(<span class="string">&#x27;cls&#x27;</span>).alias(<span class="string">&#x27;distinct_cls_count&#x27;</span>)).show()</span><br></pre></td></tr></table></figure>

<p><strong>拆分数据集</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#====================数据集拆成两部分 randomSplit ===========</span></span><br><span class="line"><span class="comment">#设置数据比例将数据划分为两部分</span></span><br><span class="line">trainDF, testDF = df.randomSplit([<span class="number">0.6</span>, <span class="number">0.4</span>])</span><br></pre></td></tr></table></figure>

<p><strong>采样数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================采样数据 sample===========</span></span><br><span class="line"><span class="comment">#withReplacement：是否有放回的采样</span></span><br><span class="line"><span class="comment">#fraction：采样比例</span></span><br><span class="line"><span class="comment">#seed：随机种子</span></span><br><span class="line">sdf = df.sample(<span class="literal">False</span>,<span class="number">0.2</span>,<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p><strong>查看两个数据集在类别上的差异</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#查看两个数据集在类别上的差异 subtract，确保训练数据集覆盖了所有分类</span></span><br><span class="line">diff_in_train_test = testDF.select(<span class="string">&#x27;cls&#x27;</span>).subtract(trainDF.select(<span class="string">&#x27;cls&#x27;</span>))</span><br><span class="line">diff_in_train_test.distinct().count()</span><br></pre></td></tr></table></figure>

<p><strong>交叉表</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ================交叉表 crosstab=============</span></span><br><span class="line">df.crosstab(<span class="string">&#x27;cls&#x27;</span>,<span class="string">&#x27;SepalLength&#x27;</span>).show()</span><br></pre></td></tr></table></figure>

<p><strong>udf</strong></p>
<p>udf：自定义函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#================== 综合案例 + udf================</span></span><br><span class="line"><span class="comment"># 测试数据集中有些类别在训练集中是不存在的，找到这些数据集做后续处理</span></span><br><span class="line">trainDF,testDF = df.randomSplit([<span class="number">0.99</span>,<span class="number">0.01</span>])</span><br><span class="line"></span><br><span class="line">diff_in_train_test = trainDF.select(<span class="string">&#x27;cls&#x27;</span>).subtract(testDF.select(<span class="string">&#x27;cls&#x27;</span>)).distinct().show()</span><br><span class="line"></span><br><span class="line"><span class="comment">#首先找到这些类，整理到一个列表</span></span><br><span class="line">not_exist_cls = trainDF.select(<span class="string">&#x27;cls&#x27;</span>).subtract(testDF.select(<span class="string">&#x27;cls&#x27;</span>)).distinct().rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> x :x[<span class="number">0</span>]).collect()</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义一个方法，用于检测</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">should_remove</span>(<span class="params">x</span>):</span></span><br><span class="line">    <span class="keyword">if</span> x <span class="keyword">in</span> not_exist_cls:</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span> :</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建udf，udf函数需要两个参数：</span></span><br><span class="line"><span class="comment"># Function</span></span><br><span class="line"><span class="comment"># Return type (in my case StringType())</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#在RDD中可以直接定义函数，交给rdd的transformatioins方法进行执行</span></span><br><span class="line"><span class="comment">#在DataFrame中需要通过udf将自定义函数封装成udf函数再交给DataFrame进行调用执行</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> StringType</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.functions <span class="keyword">import</span> udf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">check = udf(should_remove,StringType())</span><br><span class="line"></span><br><span class="line">resultDF = trainDF.withColumn(<span class="string">&#x27;New_cls&#x27;</span>,check(trainDF[<span class="string">&#x27;cls&#x27;</span>])).<span class="built_in">filter</span>(<span class="string">&#x27;New_cls &lt;&gt; -1&#x27;</span>)</span><br><span class="line"></span><br><span class="line">resultDF.show()</span><br></pre></td></tr></table></figure>


<h1 id="JSON数据的处理"><a href="#JSON数据的处理" class="headerlink" title="JSON数据的处理"></a>JSON数据的处理</h1><h2 id="介绍-1"><a href="#介绍-1" class="headerlink" title="介绍"></a>介绍</h2><p><strong>JSON数据</strong></p>
<ul>
<li><p>Spark SQL can automatically infer the schema of a JSON dataset and load it as a DataFrame</p>
<p>Spark SQL能够自动将JSON数据集以结构化的形式加载为一个DataFrame</p>
</li>
<li><p>This conversion can be done using SparkSession.read.json on a JSON file</p>
<p>读取一个JSON文件可以用SparkSession.read.json方法</p>
</li>
</ul>
<p><strong>从JSON到DataFrame</strong></p>
<ul>
<li><p>指定DataFrame的schema</p>
<ol>
<li>通过反射自动推断，适合静态数据</li>
<li>程序指定，适合程序运行中动态生成的数据</li>
</ol>
</li>
</ul>
<p><strong>加载json数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#使用内部的schema</span></span><br><span class="line">jsonDF = spark.read.json(<span class="string">&quot;xxx.json&quot;</span>)</span><br><span class="line">jsonDF = spark.read.<span class="built_in">format</span>(<span class="string">&#x27;json&#x27;</span>).load(<span class="string">&#x27;xxx.json&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#指定schema</span></span><br><span class="line">jsonDF = spark.read.schema(jsonSchema).json(<span class="string">&#x27;xxx.json&#x27;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>嵌套结构的JSON</strong></p>
<ul>
<li><p>重要的方法</p>
<ol>
<li>get_json_object</li>
<li>get_json</li>
<li>explode</li>
</ol>
</li>
</ul>
<h2 id="实践"><a href="#实践" class="headerlink" title="实践"></a>实践</h2><h3 id="静态json数据的读取和操作"><a href="#静态json数据的读取和操作" class="headerlink" title="静态json数据的读取和操作"></a>静态json数据的读取和操作</h3><p><strong>无嵌套结构的json数据</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line">spark =  SparkSession.builder.appName(<span class="string">&#x27;json_demo&#x27;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line"><span class="comment">#                无嵌套结构的json</span></span><br><span class="line"><span class="comment"># ==========================================</span></span><br><span class="line">jsonString = [</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01001&quot;, &quot;city&quot; : &quot;AGAWAM&quot;,  &quot;pop&quot; : 15338, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span>,</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01002&quot;, &quot;city&quot; : &quot;CUSHMAN&quot;, &quot;pop&quot; : 36963, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span></span><br><span class="line">]</span><br></pre></td></tr></table></figure>

<p><strong>从json字符串数组得到DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从json字符串数组得到rdd有两种方法</span></span><br><span class="line"><span class="comment"># 1. 转换为rdd，再从rdd到DataFrame</span></span><br><span class="line"><span class="comment"># 2. 直接利用spark.createDataFrame()，见后面例子</span></span><br><span class="line"></span><br><span class="line">jsonRDD = sc.parallelize(jsonString)   <span class="comment"># stringJSONRDD</span></span><br><span class="line">jsonDF =  spark.read.json(jsonRDD)  <span class="comment"># convert RDD into DataFrame</span></span><br><span class="line">jsonDF.printSchema()</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>

<p><strong>直接从文件生成DataFrame</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -- 直接从文件生成DataFrame</span></span><br><span class="line"><span class="comment">#只有被压缩后的json文件内容，才能被spark-sql正确读取，否则格式化后的数据读取会出现问题</span></span><br><span class="line">jsonDF = spark.read.json(<span class="string">&quot;xxx.json&quot;</span>)</span><br><span class="line"><span class="comment"># or</span></span><br><span class="line"><span class="comment"># jsonDF = spark.read.format(&#x27;json&#x27;).load(&#x27;xxx.json&#x27;)</span></span><br><span class="line"></span><br><span class="line">jsonDF.printSchema()</span><br><span class="line">jsonDF.show()</span><br><span class="line"></span><br><span class="line">jsonDF.<span class="built_in">filter</span>(jsonDF.pop&gt;<span class="number">4000</span>).show(<span class="number">10</span>)</span><br><span class="line"><span class="comment">#依照已有的DataFrame，创建一个临时的表(相当于mysql数据库中的一个表)，这样就可以用纯sql语句进行数据操作</span></span><br><span class="line">jsonDF.createOrReplaceTempView(<span class="string">&quot;tmp_table&quot;</span>)</span><br><span class="line"></span><br><span class="line">resultDF = spark.sql(<span class="string">&quot;select * from tmp_table where pop&gt;4000&quot;</span>)</span><br><span class="line">resultDF.show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="动态json数据的读取和操作"><a href="#动态json数据的读取和操作" class="headerlink" title="动态json数据的读取和操作"></a>动态json数据的读取和操作</h3><p><strong>指定DataFrame的Schema</strong></p>
<p>3.1节中的例子为通过反射自动推断schema，适合静态数据</p>
<p>下面我们来讲解如何进行程序指定schema</p>
<p><strong>没有嵌套结构的json</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">jsonString = [</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01001&quot;, &quot;city&quot; : &quot;AGAWAM&quot;,  &quot;pop&quot; : 15338, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span>,</span><br><span class="line"><span class="string">&quot;&quot;&quot;&#123; &quot;id&quot; : &quot;01002&quot;, &quot;city&quot; : &quot;CUSHMAN&quot;, &quot;pop&quot; : 36963, &quot;state&quot; : &quot;MA&quot; &#125;&quot;&quot;&quot;</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">jsonRDD = sc.parallelize(jsonString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义结构类型</span></span><br><span class="line"><span class="comment">#StructType：schema的整体结构，表示JSON的对象结构</span></span><br><span class="line"><span class="comment">#XXXStype:指的是某一列的数据类型</span></span><br><span class="line">jsonSchema = StructType() \</span><br><span class="line">  .add(<span class="string">&quot;id&quot;</span>, StringType(),<span class="literal">True</span>) \</span><br><span class="line">  .add(<span class="string">&quot;city&quot;</span>, StringType()) \</span><br><span class="line">  .add(<span class="string">&quot;pop&quot;</span> , LongType()) \</span><br><span class="line">  .add(<span class="string">&quot;state&quot;</span>,StringType())</span><br><span class="line"></span><br><span class="line">jsonSchema = StructType() \</span><br><span class="line">  .add(<span class="string">&quot;id&quot;</span>, LongType(),<span class="literal">True</span>) \</span><br><span class="line">  .add(<span class="string">&quot;city&quot;</span>, StringType()) \</span><br><span class="line">  .add(<span class="string">&quot;pop&quot;</span> , DoubleType()) \</span><br><span class="line">  .add(<span class="string">&quot;state&quot;</span>,StringType())</span><br><span class="line"></span><br><span class="line">reader = spark.read.schema(jsonSchema)</span><br><span class="line"></span><br><span class="line">jsonDF = reader.json(jsonRDD)</span><br><span class="line">jsonDF.printSchema()</span><br><span class="line">jsonDF.show()</span><br></pre></td></tr></table></figure>

<p><strong>带有嵌套结构的json</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql.types <span class="keyword">import</span> *</span><br><span class="line">jsonSchema = StructType([</span><br><span class="line">    StructField(<span class="string">&quot;id&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;city&quot;</span>, StringType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;loc&quot;</span> , ArrayType(DoubleType())),</span><br><span class="line">    StructField(<span class="string">&quot;pop&quot;</span>, LongType(), <span class="literal">True</span>),</span><br><span class="line">    StructField(<span class="string">&quot;state&quot;</span>, StringType(), <span class="literal">True</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">reader = spark.read.schema(jsonSchema)</span><br><span class="line">jsonDF = reader.json(<span class="string">&#x27;data/nest.json&#x27;</span>)</span><br><span class="line">jsonDF.printSchema()</span><br><span class="line">jsonDF.show(<span class="number">2</span>)</span><br><span class="line">jsonDF.<span class="built_in">filter</span>(jsonDF.pop&gt;<span class="number">4000</span>).show(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h1 id="数据清洗"><a href="#数据清洗" class="headerlink" title="数据清洗"></a>数据清洗</h1><p>前面我们处理的数据实际上都是已经被处理好的规整数据，但是在大数据整个生产过程中，需要先对数据进行数据清洗，将杂乱无章的数据整理为符合后面处理要求的规整数据。</p>
<p><strong>数据去重</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">1.删除重复数据</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">groupby().count()：可以看到数据的重复情况</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df = spark.createDataFrame([</span><br><span class="line">  (<span class="number">1</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">3</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="number">23</span>, <span class="string">&#x27;F&#x27;</span>),</span><br><span class="line">  (<span class="number">5</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">&#x27;M&#x27;</span>),</span><br><span class="line">], [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看重复记录</span></span><br><span class="line"><span class="comment">#无意义重复数据去重：数据中行与行完全重复</span></span><br><span class="line"><span class="comment"># 1.首先删除完全一样的记录</span></span><br><span class="line">df2 = df.dropDuplicates()</span><br><span class="line"></span><br><span class="line"><span class="comment">#有意义去重：删除除去无意义字段之外的完全重复的行数据</span></span><br><span class="line"><span class="comment"># 2.其次，关键字段值完全一模一样的记录（在这个例子中，是指除了id之外的列一模一样）</span></span><br><span class="line"><span class="comment"># 删除某些字段值完全一样的重复记录，subset参数定义这些字段</span></span><br><span class="line">df3 = df2.dropDuplicates(subset = [c <span class="keyword">for</span> c <span class="keyword">in</span> df2.columns <span class="keyword">if</span> c!=<span class="string">&#x27;id&#x27;</span>])</span><br><span class="line"><span class="comment"># 3.有意义的重复记录去重之后，再看某个无意义字段的值是否有重复（在这个例子中，是看id是否重复）</span></span><br><span class="line"><span class="comment"># 查看某一列是否有重复值</span></span><br><span class="line"><span class="keyword">import</span> pyspark.sql.functions <span class="keyword">as</span> fn</span><br><span class="line">df3.agg(fn.count(<span class="string">&#x27;id&#x27;</span>).alias(<span class="string">&#x27;id_count&#x27;</span>),fn.countDistinct(<span class="string">&#x27;id&#x27;</span>).alias(<span class="string">&#x27;distinct_id_count&#x27;</span>)).collect()</span><br><span class="line"><span class="comment"># 4.对于id这种无意义的列重复，添加另外一列自增id</span></span><br><span class="line"></span><br><span class="line">df3.withColumn(<span class="string">&#x27;new_id&#x27;</span>,fn.monotonically_increasing_id()).show()</span><br></pre></td></tr></table></figure>

<p><strong>缺失值处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">2.处理缺失值</span></span><br><span class="line"><span class="string">2.1 对缺失值进行删除操作(行，列)</span></span><br><span class="line"><span class="string">2.2 对缺失值进行填充操作(列的均值)</span></span><br><span class="line"><span class="string">2.3 对缺失值对应的行或列进行标记</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df_miss = spark.createDataFrame([</span><br><span class="line">(<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.6</span>, <span class="number">28</span>,<span class="string">&#x27;M&#x27;</span>, <span class="number">100000</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="number">167.2</span>, <span class="number">5.4</span>, <span class="number">45</span>,<span class="string">&#x27;M&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="literal">None</span> , <span class="number">5.2</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.9</span>, <span class="number">33</span>, <span class="string">&#x27;M&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.7</span>, <span class="number">54</span>, <span class="string">&#x27;F&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.2</span>, <span class="literal">None</span>, <span class="string">&#x27;F&#x27;</span>, <span class="literal">None</span>),</span><br><span class="line">(<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>, <span class="string">&#x27;M&#x27;</span>, <span class="number">76000</span>),],</span><br><span class="line"> [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>, <span class="string">&#x27;gender&#x27;</span>, <span class="string">&#x27;income&#x27;</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.计算每条记录的缺失值情况</span></span><br><span class="line"></span><br><span class="line">df_miss.rdd.<span class="built_in">map</span>(<span class="keyword">lambda</span> row:(row[<span class="string">&#x27;id&#x27;</span>],<span class="built_in">sum</span>([c==<span class="literal">None</span> <span class="keyword">for</span> c <span class="keyword">in</span> row]))).collect()</span><br><span class="line">[(<span class="number">1</span>, <span class="number">0</span>), (<span class="number">2</span>, <span class="number">1</span>), (<span class="number">3</span>, <span class="number">4</span>), (<span class="number">4</span>, <span class="number">1</span>), (<span class="number">5</span>, <span class="number">1</span>), (<span class="number">6</span>, <span class="number">2</span>), (<span class="number">7</span>, <span class="number">0</span>)]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.计算各列的缺失情况百分比</span></span><br><span class="line">df_miss.agg(*[(<span class="number">1</span> - (fn.count(c) / fn.count(<span class="string">&#x27;*&#x27;</span>))).alias(c + <span class="string">&#x27;_missing&#x27;</span>) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns]).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、删除缺失值过于严重的列</span></span><br><span class="line"><span class="comment"># 其实是先建一个DF，不要缺失值的列</span></span><br><span class="line">df_miss_no_income = df_miss.select([</span><br><span class="line">c <span class="keyword">for</span> c <span class="keyword">in</span> df_miss.columns <span class="keyword">if</span> c != <span class="string">&#x27;income&#x27;</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、按照缺失值删除行（threshold是根据一行记录中，缺失字段的百分比的定义）</span></span><br><span class="line">df_miss_no_income.dropna(thresh=<span class="number">3</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、填充缺失值，可以用fillna来填充缺失值，</span></span><br><span class="line"><span class="comment"># 对于bool类型、或者分类类型，可以为缺失值单独设置一个类型，missing</span></span><br><span class="line"><span class="comment"># 对于数值类型，可以用均值或者中位数等填充</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># fillna可以接收两种类型的参数：</span></span><br><span class="line"><span class="comment"># 一个数字、字符串，这时整个DataSet中所有的缺失值都会被填充为相同的值。</span></span><br><span class="line"><span class="comment"># 也可以接收一个字典｛列名：值｝这样</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 先计算均值，并组织成一个字典</span></span><br><span class="line">means = df_miss_no_income.agg( *[fn.mean(c).alias(c) <span class="keyword">for</span> c <span class="keyword">in</span> df_miss_no_income.columns <span class="keyword">if</span> c != <span class="string">&#x27;gender&#x27;</span>]).toPandas().to_dict(<span class="string">&#x27;records&#x27;</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 然后添加其它的列</span></span><br><span class="line">means[<span class="string">&#x27;gender&#x27;</span>] = <span class="string">&#x27;missing&#x27;</span></span><br><span class="line"></span><br><span class="line">df_miss_no_income.fillna(means).show()</span><br></pre></td></tr></table></figure>

<p><strong>异常值处理</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line"><span class="string">3、异常值处理</span></span><br><span class="line"><span class="string">异常值：不属于正常的值 包含：缺失值，超过正常范围内的较大值或较小值</span></span><br><span class="line"><span class="string">分位数去极值</span></span><br><span class="line"><span class="string">中位数绝对偏差去极值</span></span><br><span class="line"><span class="string">正态分布去极值</span></span><br><span class="line"><span class="string">上述三种操作的核心都是：通过原始数据设定一个正常的范围，超过此范围的就是一个异常值</span></span><br><span class="line"><span class="string">&#x27;&#x27;&#x27;</span></span><br><span class="line">df_outliers = spark.createDataFrame([</span><br><span class="line">(<span class="number">1</span>, <span class="number">143.5</span>, <span class="number">5.3</span>, <span class="number">28</span>),</span><br><span class="line">(<span class="number">2</span>, <span class="number">154.2</span>, <span class="number">5.5</span>, <span class="number">45</span>),</span><br><span class="line">(<span class="number">3</span>, <span class="number">342.3</span>, <span class="number">5.1</span>, <span class="number">99</span>),</span><br><span class="line">(<span class="number">4</span>, <span class="number">144.5</span>, <span class="number">5.5</span>, <span class="number">33</span>),</span><br><span class="line">(<span class="number">5</span>, <span class="number">133.2</span>, <span class="number">5.4</span>, <span class="number">54</span>),</span><br><span class="line">(<span class="number">6</span>, <span class="number">124.1</span>, <span class="number">5.1</span>, <span class="number">21</span>),</span><br><span class="line">(<span class="number">7</span>, <span class="number">129.2</span>, <span class="number">5.3</span>, <span class="number">42</span>),</span><br><span class="line">], [<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>])</span><br><span class="line"><span class="comment"># 设定范围 超出这个范围的 用边界值替换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># approxQuantile方法接收三个参数：参数1，列名；参数2：想要计算的分位点，可以是一个点，也可以是一个列表（0和1之间的小数），第三个参数是能容忍的误差，如果是0，代表百分百精确计算。</span></span><br><span class="line"></span><br><span class="line">cols = [<span class="string">&#x27;weight&#x27;</span>, <span class="string">&#x27;height&#x27;</span>, <span class="string">&#x27;age&#x27;</span>]</span><br><span class="line"></span><br><span class="line">bounds = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    quantiles = df_outliers.approxQuantile(col, [<span class="number">0.25</span>, <span class="number">0.75</span>], <span class="number">0.05</span>)</span><br><span class="line">    IQR = quantiles[<span class="number">1</span>] - quantiles[<span class="number">0</span>]</span><br><span class="line">    bounds[col] = [</span><br><span class="line">        quantiles[<span class="number">0</span>] - <span class="number">1.5</span> * IQR,</span><br><span class="line">        quantiles[<span class="number">1</span>] + <span class="number">1.5</span> * IQR</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>bounds</span><br><span class="line">&#123;<span class="string">&#x27;age&#x27;</span>: [-<span class="number">11.0</span>, <span class="number">93.0</span>], <span class="string">&#x27;height&#x27;</span>: [<span class="number">4.499999999999999</span>, <span class="number">6.1000000000000005</span>], <span class="string">&#x27;weight&#x27;</span>: [<span class="number">91.69999999999999</span>, <span class="number">191.7</span>]&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为异常值字段打标志</span></span><br><span class="line">outliers = df_outliers.select(*[<span class="string">&#x27;id&#x27;</span>] + [( (df_outliers[c] &lt; bounds[c][<span class="number">0</span>]) | (df_outliers[c] &gt; bounds[c][<span class="number">1</span>]) ).alias(c + <span class="string">&#x27;_o&#x27;</span>) <span class="keyword">for</span> c <span class="keyword">in</span> cols ])</span><br><span class="line">outliers.show()</span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># +---+--------+--------+-----+</span></span><br><span class="line"><span class="comment"># | id|weight_o|height_o|age_o|</span></span><br><span class="line"><span class="comment"># +---+--------+--------+-----+</span></span><br><span class="line"><span class="comment"># |  1|   false|   false|false|</span></span><br><span class="line"><span class="comment"># |  2|   false|   false|false|</span></span><br><span class="line"><span class="comment"># |  3|    true|   false| true|</span></span><br><span class="line"><span class="comment"># |  4|   false|   false|false|</span></span><br><span class="line"><span class="comment"># |  5|   false|   false|false|</span></span><br><span class="line"><span class="comment"># |  6|   false|   false|false|</span></span><br><span class="line"><span class="comment"># |  7|   false|   false|false|</span></span><br><span class="line"><span class="comment"># +---+--------+--------+-----+</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再回头看看这些异常值的值，重新和原始数据关联</span></span><br><span class="line"></span><br><span class="line">df_outliers = df_outliers.join(outliers, on=<span class="string">&#x27;id&#x27;</span>)</span><br><span class="line">df_outliers.<span class="built_in">filter</span>(<span class="string">&#x27;weight_o&#x27;</span>).select(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;weight&#x27;</span>).show()</span><br><span class="line"><span class="comment"># +---+------+</span></span><br><span class="line"><span class="comment"># | id|weight|</span></span><br><span class="line"><span class="comment"># +---+------+</span></span><br><span class="line"><span class="comment"># |  3| 342.3|</span></span><br><span class="line"><span class="comment"># +---+------+</span></span><br><span class="line"></span><br><span class="line">df_outliers.<span class="built_in">filter</span>(<span class="string">&#x27;age_o&#x27;</span>).select(<span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;age&#x27;</span>).show()</span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># | id|age|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br><span class="line"><span class="comment"># |  3| 99|</span></span><br><span class="line"><span class="comment"># +---+---+</span></span><br></pre></td></tr></table></figure>

<h1 id="sparkStreaming"><a href="#sparkStreaming" class="headerlink" title="sparkStreaming"></a>sparkStreaming</h1><h2 id="sparkStreaming概述"><a href="#sparkStreaming概述" class="headerlink" title="sparkStreaming概述"></a>sparkStreaming概述</h2><h3 id="SparkStreaming是什么"><a href="#SparkStreaming是什么" class="headerlink" title="SparkStreaming是什么"></a>SparkStreaming是什么</h3><ul>
<li><p>它是一个可扩展，高吞吐具有容错性的流式计算框架</p>
<p>吞吐量：单位时间内成功传输数据的数量</p>
</li>
</ul>
<p>之前我们接触的spark-core和spark-sql都是处理属于离线批处理任务，数据一般都是在固定位置上，通常我们写好一个脚本，每天定时去处理数据，计算，保存数据结果。这类任务通常是T+1(一天一个任务)，对实时性要求不高。</p>
<p><img src="ss1.png" alt="ss1"></p>
<p>但在企业中存在很多实时性处理的需求，例如：双十一的京东阿里，通常会做一个实时的数据大屏，显示实时订单。这种情况下，对数据实时性要求较高，仅仅能够容忍到延迟1分钟或几秒钟。</p>
<p><img src="ss2.png" alt="ss2"></p>
<p><strong>实时计算框架对比</strong></p>
<p>Storm</p>
<ul>
<li>流式计算框架</li>
<li>以record为单位处理数据</li>
<li>也支持micro-batch方式（Trident）</li>
</ul>
<p>Spark</p>
<ul>
<li>批处理计算框架</li>
<li>以RDD为单位处理数据</li>
<li>支持micro-batch流式处理数据（Spark Streaming）</li>
</ul>
<p>对比：</p>
<ul>
<li>吞吐量：Spark Streaming优于Storm</li>
<li>延迟：Spark Streaming差于Storm</li>
</ul>
<h3 id="SparkStreaming的组件"><a href="#SparkStreaming的组件" class="headerlink" title="SparkStreaming的组件"></a>SparkStreaming的组件</h3><ul>
<li>Streaming Context<ul>
<li>一旦一个Context已经启动(调用了Streaming Context的start()),就不能有新的流算子(Dstream)建立或者是添加到context中</li>
<li>一旦一个context已经停止,不能重新启动(Streaming Context调用了stop方法之后 就不能再次调 start())</li>
<li>在JVM(java虚拟机)中, 同一时间只能有一个Streaming Context处于活跃状态, 一个SparkContext创建一个Streaming Context</li>
<li>在Streaming Context上调用Stop方法, 也会关闭SparkContext对象, 如果只想仅关闭Streaming Context对象,设置stop()的可选参数为false</li>
<li>一个SparkContext对象可以重复利用去创建多个Streaming Context对象(不关闭SparkContext前提下), 但是需要关一个再开下一个</li>
</ul>
</li>
<li>DStream (离散流)<ul>
<li>代表一个连续的数据流</li>
<li>在内部, DStream由一系列连续的RDD组成</li>
<li>DStreams中的每个RDD都包含确定时间间隔内的数据</li>
<li>任何对DStreams的操作都转换成了对DStreams隐含的RDD的操作</li>
<li>数据源<ul>
<li>基本源<ul>
<li>TCP/IP Socket</li>
<li>FileSystem</li>
</ul>
</li>
<li>高级源<ul>
<li>Kafka</li>
<li>Flume</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
<h2 id="Spark-Streaming编码实践"><a href="#Spark-Streaming编码实践" class="headerlink" title="Spark Streaming编码实践"></a>Spark Streaming编码实践</h2><p><strong>Spark Streaming编码步骤：</strong></p>
<ul>
<li>创建一个StreamingContext</li>
<li>从StreamingContext中创建一个数据对象</li>
<li>对数据对象进行Transformations操作</li>
<li>输出结果</li>
<li>开始和停止</li>
</ul>
<p><strong>利用Spark Streaming实现WordCount</strong></p>
<p>需求：监听某个端口上的网络数据，实时统计出现的不同单词个数。</p>
<ol>
<li>需要安装一个nc工具：sudo yum install -y nc</li>
<li>执行指令：nc -lk 9999 -v</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">    </span><br><span class="line">    sc = SparkContext(<span class="string">&quot;local[2]&quot;</span>,appName=<span class="string">&quot;NetworkWordCount&quot;</span>)</span><br><span class="line">    <span class="comment">#参数2：指定执行计算的时间间隔</span></span><br><span class="line">    ssc = StreamingContext(sc, <span class="number">1</span>)</span><br><span class="line">    <span class="comment">#监听ip，端口上的上的数据</span></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>,<span class="number">9999</span>)</span><br><span class="line">    <span class="comment">#将数据按空格进行拆分为多个单词</span></span><br><span class="line">    words = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">    <span class="comment">#将单词转换为(单词，1)的形式</span></span><br><span class="line">    pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">    <span class="comment">#统计单词个数</span></span><br><span class="line">    wordCounts = pairs.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)</span><br><span class="line">    <span class="comment">#打印结果信息，会使得前面的transformation操作执行</span></span><br><span class="line">    wordCounts.pprint()</span><br><span class="line">    <span class="comment">#启动StreamingContext</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">#等待计算结束</span></span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>可视化查看效果：<a target="_blank" rel="noopener" href="http://192.168.199.188:4040/">http://192.168.199.188:4040</a></p>
<p>点击streaming，查看效果</p>
<h2 id="Spark-Streaming的状态操作"><a href="#Spark-Streaming的状态操作" class="headerlink" title="Spark Streaming的状态操作"></a>Spark Streaming的状态操作</h2><p>在Spark Streaming中存在两种状态操作</p>
<ul>
<li>UpdateStateByKey</li>
<li>Windows操作</li>
</ul>
<p>使用有状态的transformation，需要开启Checkpoint</p>
<ul>
<li>spark streaming 的容错机制</li>
<li>它将足够多的信息checkpoint到某些具备容错性的存储系统如hdfs上，以便出错时能够迅速恢复</li>
</ul>
<h3 id="updateStateByKey"><a href="#updateStateByKey" class="headerlink" title="updateStateByKey"></a>updateStateByKey</h3><p>Spark Streaming实现的是一个实时批处理操作，每隔一段时间将数据进行打包，封装成RDD，是无状态的。</p>
<p>无状态：指的是每个时间片段的数据之间是没有关联的。</p>
<p>需求：想要将一个大时间段（1天），即多个小时间段的数据内的数据持续进行累积操作</p>
<p>一般超过一天都是用RDD或Spark SQL来进行离线批处理</p>
<p>如果没有UpdateStateByKey，我们需要将每一秒的数据计算好放入mysql中取，再用mysql来进行统计计算</p>
<p>Spark Streaming中提供这种状态保护机制，即updateStateByKey</p>
<p>步骤：</p>
<ul>
<li>首先，要定义一个state，可以是任意的数据类型</li>
<li>其次，要定义state更新函数–指定一个函数如何使用之前的state和新值来更新state</li>
<li>对于每个batch，Spark都会为每个之前已经存在的key去应用一次state更新函数，无论这个key在batch中是否有新的数据。如果state更新函数返回none，那么key对应的state就会被删除</li>
<li>对于每个新出现的key，也会执行state更新函数</li>
</ul>
<p>举例：词统计。</p>
<h3 id="案例：updateStateByKey"><a href="#案例：updateStateByKey" class="headerlink" title="案例：updateStateByKey"></a>案例：updateStateByKey</h3><p>需求：监听网络端口的数据，获取到每个批次的出现的单词数量，并且需要把每个批次的信息保留下来</p>
<p><strong>代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建SparkContext</span></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">ssc = StreamingContext(sc, <span class="number">3</span>)</span><br><span class="line"><span class="comment">#开启检查点</span></span><br><span class="line">ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义state更新函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">updateFunc</span>(<span class="params">new_values, last_sum</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(new_values) + (last_sum <span class="keyword">or</span> <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">lines = ssc.socketTextStream(<span class="string">&quot;localhost&quot;</span>, <span class="number">9999</span>)</span><br><span class="line"><span class="comment"># 对数据以空格进行拆分，分为多个单词</span></span><br><span class="line">counts = lines.flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">    .<span class="built_in">map</span>(<span class="keyword">lambda</span> word: (word, <span class="number">1</span>)) \</span><br><span class="line">    .updateStateByKey(updateFunc=updateFunc)<span class="comment">#应用updateStateByKey函数</span></span><br><span class="line">    </span><br><span class="line">counts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<h3 id="Windows"><a href="#Windows" class="headerlink" title="Windows"></a>Windows</h3><ul>
<li>窗口长度L：运算的数据量</li>
<li>滑动间隔G：控制每隔多长时间做一次运算</li>
</ul>
<p>每隔G秒，统计最近L秒的数据</p>
<p><img src="ss14.png" alt="ss14"></p>
<p><strong>操作细节</strong></p>
<ul>
<li>Window操作是基于窗口长度和滑动间隔来工作的</li>
<li>窗口的长度控制考虑前几批次数据量</li>
<li>默认为批处理的滑动间隔来确定计算结果的频率</li>
</ul>
<p><strong>相关函数</strong></p>
<p><img src="ss15.png" alt="ss15"></p>
<ul>
<li>Smart computation</li>
<li>invAddFunc</li>
</ul>
<p>reduceByKeyAndWindow(func,invFunc,windowLength,slideInterval,[num,Tasks])</p>
<p>func:正向操作，类似于updateStateByKey</p>
<p>invFunc：反向操作</p>
<p><img src="ss16.png" alt="ss16"></p>
<p>例如在热词时，在上一个窗口中可能是热词，这个一个窗口中可能不是热词，就需要在这个窗口中把该次剔除掉</p>
<p>典型案例：热点搜索词滑动统计，每隔10秒，统计最近60秒钟的搜索词的搜索频次，并打印出最靠前的3个搜索词出现次数。</p>
<p><img src="ss17.png" alt="ss17"></p>
<p><strong>案例</strong></p>
<p>监听网络端口的数据，每隔3秒统计前6秒出现的单词数量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_countryname</span>(<span class="params">line</span>):</span></span><br><span class="line">    country_name = line.strip()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> country_name == <span class="string">&#x27;usa&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;USA&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;ind&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;India&#x27;</span></span><br><span class="line">    <span class="keyword">elif</span> country_name == <span class="string">&#x27;aus&#x27;</span>:</span><br><span class="line">        output = <span class="string">&#x27;Australia&#x27;</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        output = <span class="string">&#x27;Unknown&#x27;</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> (output, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&quot;__main__&quot;</span>:</span><br><span class="line">	<span class="comment">#定义处理的时间间隔</span></span><br><span class="line">    batch_interval = <span class="number">1</span> <span class="comment"># base time unit (in seconds)</span></span><br><span class="line">    <span class="comment">#定义窗口长度</span></span><br><span class="line">    window_length = <span class="number">6</span> * batch_interval</span><br><span class="line">    <span class="comment">#定义滑动时间间隔</span></span><br><span class="line">    frequency = <span class="number">3</span> * batch_interval</span><br><span class="line"></span><br><span class="line">    <span class="comment">#获取StreamingContext</span></span><br><span class="line">    spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">	sc = spark.sparkContext</span><br><span class="line">	ssc = StreamingContext(sc, batch_interval)</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#需要设置检查点</span></span><br><span class="line">    ssc.checkpoint(<span class="string">&quot;checkpoint&quot;</span>)</span><br><span class="line"></span><br><span class="line">    lines = ssc.socketTextStream(<span class="string">&#x27;localhost&#x27;</span>, <span class="number">9999</span>)</span><br><span class="line">    addFunc = <span class="keyword">lambda</span> x, y: x + y</span><br><span class="line">    invAddFunc = <span class="keyword">lambda</span> x, y: x - y</span><br><span class="line">    <span class="comment">#调用reduceByKeyAndWindow，来进行窗口函数的调用</span></span><br><span class="line">    window_counts = lines.<span class="built_in">map</span>(get_countryname) \</span><br><span class="line">        .reduceByKeyAndWindow(addFunc, invAddFunc, window_length, frequency)</span><br><span class="line">	<span class="comment">#输出处理结果信息</span></span><br><span class="line">    window_counts.pprint()</span><br><span class="line"></span><br><span class="line">    ssc.start()</span><br><span class="line">    ssc.awaitTermination()</span><br></pre></td></tr></table></figure>


<h2 id="Spark-Streaming对接Kafka"><a href="#Spark-Streaming对接Kafka" class="headerlink" title="Spark Streaming对接Kafka"></a>Spark Streaming对接Kafka</h2><h3 id="对接数据的两种方式"><a href="#对接数据的两种方式" class="headerlink" title="对接数据的两种方式"></a>对接数据的两种方式</h3><p>在前面的案例中，我们监听了来自网络端口的数据，实现了WordCount，但是在实际开发中并不是这样。我们更多的是接收来自高级数据源的数据，例如Kafka。</p>
<p>下面我们来介绍如何利用Spark Streaming对接Kafka</p>
<p><img src="ss13.png"></p>
<p>以下两种方式都是为了数据可靠性：</p>
<ul>
<li>Receiver-based Approach：由Receiver来对接数据，Receiver接收到数据后会将日志预先写入到hdfs上（WAL），同时也会将数据做副本传输到其他的Worker节点。在读取数据的过程中，Receiver是从Zookeeper中获取数据的偏移信息。</li>
<li>Direct Approach（No Receivers）：没有Receiver接收信息，由Spark Streaming直接对接Kafka的broker，获取数据和数据的偏移信息。</li>
</ul>
<p>上述两种方式中，Direct Approach方式更加可靠，不需要Spark Streaming自己去保证维护数据的可靠性，而是由善于处理这类工作的Kafka来做。</p>
<p><strong>对应代码</strong></p>
<ul>
<li>KafkaUtils.createStream(ssc,zkQuorum,”spark-streaming-consumer”,{topic:1})</li>
<li>KafkaUtils.createDirectStream(ssc,[topic],{“metadata.broker.list”:’localhost:9092’})</li>
</ul>
<p><strong>Direct API的好处</strong></p>
<ul>
<li><strong>简化的并行</strong>：在Receiver的方式中我们提到创建多个Receiver之后利用union来合并成一个Dstream的方式提高数据传输并行度。而在Direct方式中，<strong>Kafka中的partition与RDD中的partition是一一对应</strong>的并行读取Kafka数据，这种映射关系也更利于理解和优化。</li>
<li><strong>高效</strong>：在Receiver的方式中，为了达到0数据丢失需要将数据存入Write Ahead Log中，这样在Kafka和日志中就保存了两份数据，浪费！而第二种方式不存在这个问题，只要我们Kafka的数据保留时间足够长，我们都能够从Kafka进行数据恢复。</li>
<li><strong>精确一次</strong>：在Receiver的方式中，使用的是Kafka的高阶API接口从Zookeeper中获取offset值，这也是传统的从Kafka中读取数据的方式，但由于Spark Streaming消费的数据和Zookeeper中记录的offset不同步，这种方式偶尔会造成数据重复消费。而第二种方式，直接使用了简单的低阶Kafka API，Offsets则利用Spark Streaming的checkpoints进行记录，消除了这种不一致性。</li>
</ul>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p><strong>步骤：</strong></p>
<ul>
<li><p>配置spark streaming kafka开发环境</p>
<ul>
<li><ol>
<li>下载spark streaming集成kafka的jar包<br>spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar</li>
</ol>
</li>
<li><ol start="2">
<li>将jar包放置到spark的jars目录下</li>
</ol>
</li>
<li><ol start="3">
<li>编辑spark/conf目录下的spark-defaults.conf，添加如下两条配置</li>
</ol>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">spark.driver.extraClassPath</span>=<span class="string">$SPAKR_HOME/jars/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar</span></span><br><span class="line"><span class="meta">spark.executor.extraClassPath</span>=<span class="string">$SPARK_HOME/jars/spark-streaming-kafka-0-8-assembly_2.11-2.3.0.jar</span></span><br><span class="line"><span class="comment">#driver和executor对应的两个路径一致</span></span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>测试配置是否成功</p>
<ul>
<li><p>启动zookeeper</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure></li>
<li><p>启动kafka</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kafka-server-start.sh config/server.properties</span><br></pre></td></tr></table></figure></li>
<li><p>创建topic</p>
<ul>
<li><p>bin/kafka-topics.sh –create –zookeeper localhost:2181 –replication-factor 1 –partitions 1 –topic test</p>
<p>replication-factor：副本数量</p>
<p>partitions：分区数量</p>
<p>出现Created topic “test”，说明创建成功</p>
</li>
</ul>
</li>
<li><p>查看所有topic</p>
<ul>
<li>bin/kafka-topics.sh –list –zookeeper localhost:2181</li>
</ul>
</li>
<li><p>通过Pycharm远程连接Centos 创建代码</p>
</li>
<li><p>通过KafkaUtils 成功连接Kafka 创建DStream对象说明连接成功</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="comment"># 配置spark driver和pyspark运行时，所使用的python解释器路径</span></span><br><span class="line">PYSPARK_PYTHON = <span class="string">&quot;/home/hadoop/miniconda3/envs/datapy365spark23/bin/python&quot;</span></span><br><span class="line">JAVA_HOME=<span class="string">&#x27;/home/hadoop/app/jdk1.8.0_191&#x27;</span></span><br><span class="line">SPARK_HOME = <span class="string">&quot;/home/hadoop/app/spark-2.3.0-bin-2.6.0-cdh5.7.0&quot;</span></span><br><span class="line"><span class="comment"># 当存在多个版本时，不指定很可能会导致出错</span></span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&quot;PYSPARK_DRIVER_PYTHON&quot;</span>] = PYSPARK_PYTHON</span><br><span class="line">os.environ[<span class="string">&#x27;JAVA_HOME&#x27;</span>]=JAVA_HOME</span><br><span class="line">os.environ[<span class="string">&quot;SPARK_HOME&quot;</span>] = SPARK_HOME</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">sc = sparkContext（<span class="string">&#x27;master[2]&#x27;</span>,<span class="string">&#x27;kafkastreamingtest&#x27;</span></span><br><span class="line">ssc = StreamingContext(sc,<span class="number">3</span>)</span><br><span class="line"><span class="comment">#createDirectStream 连接kafka数据源获取数据</span></span><br><span class="line"><span class="comment"># 参数1 streamingcontext</span></span><br><span class="line"><span class="comment">#参数2 topic的名字</span></span><br><span class="line"><span class="comment"># 参数3 kafka broker地址</span></span><br><span class="line">ks = KafkaUtils.createDirectStream(ssc,[<span class="string">&quot;topic1&quot;</span>],&#123;<span class="string">&quot;metadata.broker.list&quot;</span>:<span class="string">&quot;localhost:9092&quot;</span>&#125;)</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="案例实现"><a href="#案例实现" class="headerlink" title="案例实现"></a>案例实现</h3><p>需求：利用Spark Streaming不断处理来自Kafka生产者生产的数据，并统计出现的单词数量</p>
<ul>
<li><ol>
<li>编写producer.py，用于生产数据</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> kafka <span class="keyword">import</span> KafkaProducer</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建KafkaProducer，连接broker</span></span><br><span class="line">producer = KafkaProducer(bootstrap_servers=<span class="string">&#x27;localhost:9092&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#每隔一段时间发送一端字符串数据到broker</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">send_data</span>():</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">60</span>):</span><br><span class="line">        <span class="comment"># (key,value) 参数2 是value </span></span><br><span class="line">        producer.send(<span class="string">&#x27;topic_name&#x27;</span>,<span class="string">&quot;hello,kafka,spark,streaming,kafka&quot;</span>)</span><br><span class="line">        time.sleep(<span class="number">2</span>)</span><br><span class="line">send_data()</span><br></pre></td></tr></table></figure></li>
<li><ol start="2">
<li>编辑Spark Streaming代码，统计单词出现的数量</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.kafka <span class="keyword">import</span> KafkaUtils</span><br><span class="line"><span class="keyword">from</span> pyspark.sql.session <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">topic=<span class="string">&quot;topic_name&quot;</span></span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.master(<span class="string">&quot;local[2]&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">ssc = StreamingContext(sc,<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#创建direct连接，指定要连接的topic和broker地址</span></span><br><span class="line">ks = KafkaUtils.createDirectStream(ssc,[topic],&#123;<span class="string">&quot;metadata.broker.list&quot;</span>:<span class="string">&quot;localhost:9092&quot;</span>&#125;)</span><br><span class="line"><span class="comment">#(None,内容)</span></span><br><span class="line">ks.pprint()</span><br><span class="line"><span class="comment">#（key,value)</span></span><br><span class="line"><span class="comment">#以下代码每操作一次，就打印输出一次</span></span><br><span class="line">lines = ks.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">lines.pprint()</span><br><span class="line"></span><br><span class="line">words = lines.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot;,&quot;</span>))</span><br><span class="line"><span class="comment">#words.pprint()</span></span><br><span class="line"></span><br><span class="line">pairs = words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line"><span class="comment">#pairs.pprint()</span></span><br><span class="line"></span><br><span class="line">counts = pairs.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)</span><br><span class="line">counts.pprint()</span><br><span class="line"></span><br><span class="line">ssc.start()</span><br><span class="line"><span class="comment">#等待计算结束</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></li>
<li><ol start="3">
<li>开启Spark Streaming消费数据，将产生的日志结果存储到日志中</li>
</ol>
<p>spark-submit xxx.py&gt;a.log</p>
</li>
<li><ol start="4">
<li>开启producer.py，生产数据</li>
</ol>
<p>python3 producer.py</p>
</li>
<li><ol start="5">
<li>通过浏览器观察运算过程</li>
</ol>
<p><a target="_blank" rel="noopener" href="http://node-teach:4040/">http://node-teach:4040</a></p>
</li>
<li><ol start="6">
<li>分析生成的日志内容</li>
</ol>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:31:21</span><br><span class="line">-------------------------------------------</span><br><span class="line">(None, &#x27;hello,kafka,spark,streaming,kafka&#x27;)</span><br><span class="line">(None, &#x27;hello,kafka,spark,streaming,kafka&#x27;)</span><br><span class="line">(None, &#x27;hello,kafka,spark,streaming,kafka&#x27;)</span><br><span class="line">(None, &#x27;hello,kafka,spark,streaming,kafka&#x27;)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:33</span><br><span class="line">-------------------------------------------</span><br><span class="line">hello,kafka,spark,streaming,kafka</span><br><span class="line">hello,kafka,spark,streaming,kafka</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:33</span><br><span class="line">-------------------------------------------</span><br><span class="line">hello</span><br><span class="line">kafka</span><br><span class="line">spark</span><br><span class="line">streaming</span><br><span class="line">kafka</span><br><span class="line">hello</span><br><span class="line">kafka</span><br><span class="line">spark</span><br><span class="line">streaming</span><br><span class="line">kafka</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:33</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;hello&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line">(&#x27;spark&#x27;, 1)</span><br><span class="line">(&#x27;streaming&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line">(&#x27;hello&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line">(&#x27;spark&#x27;, 1)</span><br><span class="line">(&#x27;streaming&#x27;, 1)</span><br><span class="line">(&#x27;kafka&#x27;, 1)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:33</span><br><span class="line">-------------------------------------------</span><br><span class="line">(&#x27;streaming&#x27;, 2)</span><br><span class="line">(&#x27;hello&#x27;, 2)</span><br><span class="line">(&#x27;kafka&#x27;, 4)</span><br><span class="line">(&#x27;spark&#x27;, 2)</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:36</span><br><span class="line">-------------------------------------------</span><br><span class="line"></span><br><span class="line">-------------------------------------------</span><br><span class="line">Time: 2018-12-11 01:02:36</span><br><span class="line">-------------------------------------------</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="Spark-Streaming对接flume"><a href="#Spark-Streaming对接flume" class="headerlink" title="Spark Streaming对接flume"></a>Spark Streaming对接flume</h2><p>flume作为日志实时采集的框架，可以与SparkStreaming实时处理框架进行对接，flume实时产生数据，sparkStreaming做实时处理。</p>
<p>Spark Streaming对接FlumeNG有两种方式，一种是FlumeNG将消息<strong>Push</strong>推给Spark Streaming，还有一种是Spark Streaming从flume 中<strong>Pull</strong>拉取数据。</p>
<h3 id="Pull方式"><a href="#Pull方式" class="headerlink" title="Pull方式"></a>Pull方式</h3><ul>
<li><ol>
<li>安装flume1.6以上</li>
</ol>
</li>
<li><ol start="2">
<li>下载依赖包</li>
</ol>
<p>spark-streaming-flume-assembly_2.11-2.3.0.jar放入到flume的lib目录下</p>
</li>
<li><ol start="3">
<li>写flume的agent，注意既然是拉取的方式，那么flume向自己所在的机器上产数据就行</li>
</ol>
</li>
<li><ol start="4">
<li>编写flume-pull.conf配置文件</li>
</ol>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">simple-agent.sources</span> = <span class="string">netcat-source</span></span><br><span class="line"><span class="meta">simple-agent.sinks</span> = <span class="string">spark-sink</span></span><br><span class="line"><span class="meta">simple-agent.channels</span> = <span class="string">memory-channel</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"># source</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.bind</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.port</span> = <span class="string">44444</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line"><span class="meta">simple-agent.sinks.spark-sink.type</span> = <span class="string">org.apache.spark.streaming.flume.sink.SparkSink</span></span><br><span class="line"><span class="meta">simple-agent.sinks.spark-sink.hostname</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">simple-agent.sinks.spark-sink.port</span> = <span class="string">41414</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory</span></span><br><span class="line"><span class="meta">simple-agent.channels.memory-channel.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="comment"> </span></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.channels</span> = <span class="string">memory-channel</span></span><br><span class="line"><span class="meta">simple-agent.sinks.spark-sink.channel</span>=<span class="string">memory-channel</span></span><br></pre></td></tr></table></figure></li>
<li><p>5，启动flume</p>
<p>flume-ng agent -n simple-agent -f flume-pull.conf -Dflume.root.logger=INFO,console</p>
</li>
<li><p>6，编写word count代码</p>
<p>代码：</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.flume <span class="keyword">import</span> FlumeUtils</span><br><span class="line"></span><br><span class="line">sc=SparkContext(<span class="string">&quot;local[2]&quot;</span>,<span class="string">&quot;FlumeWordCount_Pull&quot;</span>)</span><br><span class="line"><span class="comment">#处理时间间隔为2s</span></span><br><span class="line">ssc=StreamingContext(sc,<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#利用flume工具类创建pull方式的流</span></span><br><span class="line">lines = FlumeUtils.createPollingStream(ssc, [(<span class="string">&quot;localhost&quot;</span>,<span class="number">41414</span>)])</span><br><span class="line"></span><br><span class="line">lines1=lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">1</span>])</span><br><span class="line">counts = lines1.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>))\</span><br><span class="line">        .<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))\</span><br><span class="line">        .reduceByKey(<span class="keyword">lambda</span> a,b:a+b)</span><br><span class="line">counts.pprint()</span><br><span class="line"><span class="comment">#启动spark streaming应用</span></span><br><span class="line">ssc.start()</span><br><span class="line"><span class="comment">#等待计算终止</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure>

<p>启动</p>
<p><code>bin/spark-submit --jars xxx/spark-streaming-flume-assembly_2.11-2.3.0.jar xxx/flume_pull.py</code></p>
<h2 id="push方式"><a href="#push方式" class="headerlink" title="push方式"></a>push方式</h2><p>大部分操作和之前一致</p>
<p>flume配置</p>
<figure class="highlight properties"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">simple-agent.sources</span> = <span class="string">netcat-source</span></span><br><span class="line"><span class="meta">simple-agent.sinks</span> = <span class="string">avro-sink</span></span><br><span class="line"><span class="meta">simple-agent.channels</span> = <span class="string">memory-channel</span></span><br><span class="line"></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.type</span> = <span class="string">netcat</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.bind</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.port</span> = <span class="string">44444</span></span><br><span class="line"></span><br><span class="line"><span class="meta">simple-agent.sinks.avro-sink.type</span> = <span class="string">avro</span></span><br><span class="line"><span class="meta">simple-agent.sinks.avro-sink.hostname</span> = <span class="string">localhost</span></span><br><span class="line"><span class="meta">simple-agent.sinks.avro-sink.port</span> = <span class="string">41414</span></span><br><span class="line"><span class="meta">simple-agent.channels.memory-channel.type</span> = <span class="string">memory</span></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.channels</span> = <span class="string">memory-channel</span></span><br><span class="line"></span><br><span class="line"><span class="meta">simple-agent.sources.netcat-source.channels</span> = <span class="string">memory-channel</span></span><br><span class="line"><span class="meta">simple-agent.sinks.avro-sink.channel</span>=<span class="string">memory-channel</span></span><br></pre></td></tr></table></figure>

<p>代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark <span class="keyword">import</span> SparkContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming <span class="keyword">import</span> StreamingContext</span><br><span class="line"><span class="keyword">from</span> pyspark.streaming.flume <span class="keyword">import</span> FlumeUtils</span><br><span class="line"></span><br><span class="line">sc=SparkContext(<span class="string">&quot;local[2]&quot;</span>,<span class="string">&quot;FlumeWordCount_Push&quot;</span>)</span><br><span class="line"><span class="comment">#处理时间间隔为2s</span></span><br><span class="line">ssc=StreamingContext(sc,<span class="number">2</span>)</span><br><span class="line"><span class="comment">#创建push方式的DStream</span></span><br><span class="line">lines = FlumeUtils.createStream(ssc, <span class="string">&quot;localhost&quot;</span>,<span class="number">41414</span>)</span><br><span class="line">lines1=lines.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">1</span>].strip())</span><br><span class="line"><span class="comment">#对1s内收到的字符串进行分割</span></span><br><span class="line">words=lines1.flatMap(<span class="keyword">lambda</span> line:line.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line"><span class="comment">#映射为（word，1）元祖</span></span><br><span class="line">pairs=words.<span class="built_in">map</span>(<span class="keyword">lambda</span> word:(word,<span class="number">1</span>))</span><br><span class="line">wordcounts=pairs.reduceByKey(<span class="keyword">lambda</span> x,y:x+y)</span><br><span class="line">wordcounts.pprint()</span><br><span class="line"><span class="comment">#启动spark streaming应用</span></span><br><span class="line">ssc.start()</span><br><span class="line"><span class="comment">#等待计算终止</span></span><br><span class="line">ssc.awaitTermination()</span><br></pre></td></tr></table></figure></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">贪钱算法还我头发</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/">https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/01/18/5.7-RS-case/"><img class="prev-cover" src="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Recommendation System case</div></div></a></div><div class="next-post pull-right"><a href="/2022/01/18/5.5-Spark-core/"><img class="next-cover" src="http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Spark core</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/09/Technical-Summary-Deep-Learning/" title="Technical Summary —— Pytorch & Numpy"><img class="cover" src="http://img.shijue.me/21cdab996c944862bac3947c8c7197f1_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-09</div><div class="title">Technical Summary —— Pytorch & Numpy</div></div></a></div><div><a href="/2024/07/02/Technical-Summary-Necessary/" title="Technical Summary —— Common Command"><img class="cover" src="http://img.shijue.me/82cad50b40214d388d246ae4847e8bec_d.jpeg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Technical Summary —— Common Command</div></div></a></div><div><a href="/2024/04/15/Technical-Summary-BigData/" title="Technical Summary —— Big Data Processing"><img class="cover" src="http://img.shijue.me/886e06c4e9b04b8a836a1b6ec00a0b17_d.jpeg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-15</div><div class="title">Technical Summary —— Big Data Processing</div></div></a></div><div><a href="/2024/04/07/Technical-Summary-Database/" title="Technical Summary —— Database Usage"><img class="cover" src="http://img.shijue.me/078729117f75472bae4bbc684d2b714c_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-07</div><div class="title">Technical Summary —— Database Usage</div></div></a></div><div><a href="/2023/05/19/Summary-NG/" title="Summary NG"><img class="cover" src="http://img.shijue.me/0a336cc060e74ca29f9cf862eb2ee8cf_d.png!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Summary NG</div></div></a></div><div><a href="/2023/05/19/Experimental-Technique/" title="Experimental Technique"><img class="cover" src="http://img.shijue.me/c94bd493552d441ab4c647296a6913b5_d.jpg!dp6" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Experimental Technique</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">贪钱算法还我头发</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">56</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xfliu1998"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xfliu1998" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuxiaofei_7@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://blog.csdn.net/keiven_" target="_blank" title="CSDN"><i class="fa fa-address-card"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">欢迎来这里掉头发</div></div><div class="card-widget" id="newYear"><div class="item-headline"><i></i><span></span></div><div class="item-content"><div id="newYear-main"><div class="mask"></div> <p class="title"></p> <div class="newYear-time"></div> <p class="today" style="text-align: right;"></p> </div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Spark-SQL"><span class="toc-number">1.</span> <span class="toc-text">Spark SQL</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#DataFrame"><span class="toc-number">2.</span> <span class="toc-text">DataFrame</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D"><span class="toc-number">2.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%88%9B%E5%BB%BADataFrame"><span class="toc-number">2.2.</span> <span class="toc-text">创建DataFrame</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#JSON%E6%95%B0%E6%8D%AE%E7%9A%84%E5%A4%84%E7%90%86"><span class="toc-number">3.</span> <span class="toc-text">JSON数据的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E4%BB%8B%E7%BB%8D-1"><span class="toc-number">3.1.</span> <span class="toc-text">介绍</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%AE%9E%E8%B7%B5"><span class="toc-number">3.2.</span> <span class="toc-text">实践</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%9D%99%E6%80%81json%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">3.2.1.</span> <span class="toc-text">静态json数据的读取和操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%8A%A8%E6%80%81json%E6%95%B0%E6%8D%AE%E7%9A%84%E8%AF%BB%E5%8F%96%E5%92%8C%E6%93%8D%E4%BD%9C"><span class="toc-number">3.2.2.</span> <span class="toc-text">动态json数据的读取和操作</span></a></li></ol></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97"><span class="toc-number">4.</span> <span class="toc-text">数据清洗</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#sparkStreaming"><span class="toc-number">5.</span> <span class="toc-text">sparkStreaming</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#sparkStreaming%E6%A6%82%E8%BF%B0"><span class="toc-number">5.1.</span> <span class="toc-text">sparkStreaming概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E6%98%AF%E4%BB%80%E4%B9%88"><span class="toc-number">5.1.1.</span> <span class="toc-text">SparkStreaming是什么</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#SparkStreaming%E7%9A%84%E7%BB%84%E4%BB%B6"><span class="toc-number">5.1.2.</span> <span class="toc-text">SparkStreaming的组件</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming%E7%BC%96%E7%A0%81%E5%AE%9E%E8%B7%B5"><span class="toc-number">5.2.</span> <span class="toc-text">Spark Streaming编码实践</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming%E7%9A%84%E7%8A%B6%E6%80%81%E6%93%8D%E4%BD%9C"><span class="toc-number">5.3.</span> <span class="toc-text">Spark Streaming的状态操作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#updateStateByKey"><span class="toc-number">5.3.1.</span> <span class="toc-text">updateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%EF%BC%9AupdateStateByKey"><span class="toc-number">5.3.2.</span> <span class="toc-text">案例：updateStateByKey</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Windows"><span class="toc-number">5.3.3.</span> <span class="toc-text">Windows</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming%E5%AF%B9%E6%8E%A5Kafka"><span class="toc-number">5.4.</span> <span class="toc-text">Spark Streaming对接Kafka</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AF%B9%E6%8E%A5%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%A4%E7%A7%8D%E6%96%B9%E5%BC%8F"><span class="toc-number">5.4.1.</span> <span class="toc-text">对接数据的两种方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E5%87%86%E5%A4%87%E5%B7%A5%E4%BD%9C"><span class="toc-number">5.5.</span> <span class="toc-text">准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A1%88%E4%BE%8B%E5%AE%9E%E7%8E%B0"><span class="toc-number">5.5.1.</span> <span class="toc-text">案例实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Spark-Streaming%E5%AF%B9%E6%8E%A5flume"><span class="toc-number">5.6.</span> <span class="toc-text">Spark Streaming对接flume</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pull%E6%96%B9%E5%BC%8F"><span class="toc-number">5.6.1.</span> <span class="toc-text">Pull方式</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#push%E6%96%B9%E5%BC%8F"><span class="toc-number">5.7.</span> <span class="toc-text">push方式</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('http://img.shijue.me/d667362f0f6b42bcb21d4a4fc167e93a_d.jpg!dp6')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 贪钱算法还我头发</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '1b0c10ce649501ea4a72',
      clientSecret: '741b5e861137e3d5a482bba272c8201b78da6cb0',
      repo: 'xfliu1998.github.io',
      owner: 'xfliu1998',
      admin: ['xfliu1998'],
      id: 'f470850948e6f8786d54762d55f13dcd',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: true,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/script.js?v1"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><script async data-pjax src="/js/newYear.js"></script><script async src="//at.alicdn.com/t/font_2264842_b004iy0kk2b.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='all'|| 'all' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Machine-Learning-and-Deep-Learning/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习与深度学习 (11)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Structures-and-Algorithms/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">😼 数据结构与算法 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Search-Advertisement-Recommendation-Causal/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗂️ 搜索/广告/推荐/因果 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Analysis-and-Processing/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数据分析与处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Reading-Notes/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 阅读笔记 (8)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Daily/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 日常随想 (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://xfliu1998.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<container><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></container>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.fomal.cc/api?xfliu1998",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xfliu1998')
    }
  </script><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-NLP/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-NLP/" alt="">Papers Reading about NLP</a><div class="blog-slider__text">自然语言处理论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-NLP/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/10/01/9-3D-Construction/" alt=""><img width="48" height="48" src="https://img.zcool.cn/community/017d495d0e6a9ea801205e4b7fa13c.png@1280w_1l_2o_100sh.png" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-10-01</span><a class="blog-slider__title" href="2022/10/01/9-3D-Construction/" alt="">3D Construction</a><div class="blog-slider__text">三维重建基础</div><a class="blog-slider__button" href="2022/10/01/9-3D-Construction/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/01/11/SfM-SLAM/" alt=""><img width="48" height="48" src="https://tse2-mm.cn.bing.net/th/id/OIP-C.V5uTTQ6LTBHc42xoBPG8hAHaEm?pid=ImgDet&amp;rs=1" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-11</span><a class="blog-slider__title" href="2023/01/11/SfM-SLAM/" alt="">SfM &amp; SLAM</a><div class="blog-slider__text">SfM和SLAM系统</div><a class="blog-slider__button" href="2023/01/11/SfM-SLAM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/03/25/Papers-Ideas/" alt=""><img width="48" height="48" src="https://tse2-mm.cn.bing.net/th/id/OIP-C.Mmv8iGEVFxSQII6QH0BG9QHaEJ?w=302&amp;h=180&amp;c=7&amp;r=0&amp;o=5&amp;dpr=2&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-25</span><a class="blog-slider__title" href="2023/03/25/Papers-Ideas/" alt="">Papers Ideas</a><div class="blog-slider__text">大模型时代下的科研思路</div><a class="blog-slider__button" href="2023/03/25/Papers-Ideas/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Summary/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Summary/" alt="">Papers Summary</a><div class="blog-slider__text">论文总结笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Summary/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-CV/" alt=""><img width="48" height="48" src="https://tse1-mm.cn.bing.net/th/id/OIP-C.KNjcp6IetyzFaIaSc8-eKAHaE8?w=303&amp;h=201&amp;c=7&amp;r=0&amp;o=5&amp;dpr=1.88&amp;pid=1.7" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-CV/" alt="">Papers Reading about CV</a><div class="blog-slider__text">计算机视觉论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-CV/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/06/05/Interview-Experience/" alt=""><img width="48" height="48" src="http://img.shijue.me/00964c481ad34d78acbf148d2b391c9e_d.jpg!dp6" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-05</span><a class="blog-slider__title" href="2023/06/05/Interview-Experience/" alt="">Interview Experience</a><div class="blog-slider__text">面经八股</div><a class="blog-slider__button" href="2023/06/05/Interview-Experience/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/08/16/Papers-Reading-about-LLM/" alt=""><img width="48" height="48" src="http://img.shijue.me/9ce88483789847cebe8d38fd7a77f7c7_d.jpg!dp6" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-08-16</span><a class="blog-slider__title" href="2024/08/16/Papers-Reading-about-LLM/" alt="">Papers Reading about LLM</a><div class="blog-slider__text">LLM论文阅读笔记</div><a class="blog-slider__button" href="2024/08/16/Papers-Reading-about-LLM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/01/21/Learning-Framework/" alt=""><img width="48" height="48" src="http://img.shijue.me/78ae8b05a73444cd9643a8312abc0d43.jpg!dp6" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-21</span><a class="blog-slider__title" href="2022/01/21/Learning-Framework/" alt="">Learning Framework</a><div class="blog-slider__text">学习大纲</div><a class="blog-slider__button" href="2022/01/21/Learning-Framework/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>