<!DOCTYPE html><html lang="en" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width,initial-scale=1"><title>Spark core | 一直进步 做喜欢的</title><meta name="keywords" content="python"><meta name="author" content="贪钱算法还我头发"><meta name="copyright" content="贪钱算法还我头发"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta name="description" content="推荐系统学习笔记五——Spark core">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark core">
<meta property="og:url" content="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/index.html">
<meta property="og:site_name" content="一直进步 做喜欢的">
<meta property="og:description" content="推荐系统学习笔记五——Spark core">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/cover.jpeg">
<meta property="article:published_time" content="2022-01-18T13:44:29.000Z">
<meta property="article:modified_time" content="2025-07-20T07:30:57.188Z">
<meta property="article:author" content="贪钱算法还我头发">
<meta property="article:tag" content="python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/cover.jpeg"><link rel="shortcut icon" href="/img/favicon.png"><link rel="canonical" href="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = { 
  root: '/',
  algolia: undefined,
  localSearch: {"path":"search.xml","languages":{"hits_empty":"We didn't find any results for the search: ${query}"}},
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":false},
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: 'days',
  date_suffix: {
    just: 'Just',
    min: 'minutes ago',
    hour: 'hours ago',
    day: 'days ago',
    month: 'months ago'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  source: {
    jQuery: 'https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js',
    justifiedGallery: {
      js: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/js/jquery.justifiedGallery.min.js',
      css: 'https://cdn.jsdelivr.net/npm/justifiedGallery/dist/css/justifiedGallery.min.css'
    },
    fancybox: {
      js: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js',
      css: 'https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.css'
    }
  },
  isPhotoFigcaption: false,
  islazyload: false,
  isanchor: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: 'Spark core',
  isPost: true,
  isHome: false,
  isHighlightShrink: false,
  isToc: true,
  postUpdate: '2025-07-20 15:30:57'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  .justified-gallery img {
    opacity: 1
  }

  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
    win.saveToLocal = {
      set: function setWithExpiry(key, value, ttl) {
        if (ttl === 0) return
        const now = new Date()
        const expiryDay = ttl * 86400000
        const item = {
          value: value,
          expiry: now.getTime() + expiryDay,
        }
        localStorage.setItem(key, JSON.stringify(item))
      },

      get: function getWithExpiry(key) {
        const itemStr = localStorage.getItem(key)

        if (!itemStr) {
          return undefined
        }
        const item = JSON.parse(itemStr)
        const now = new Date()

        if (now.getTime() > item.expiry) {
          localStorage.removeItem(key)
          return undefined
        }
        return item.value
      }
    }
  
    win.getScript = url => new Promise((resolve, reject) => {
      const script = document.createElement('script')
      script.src = url
      script.async = true
      script.onerror = reject
      script.onload = script.onreadystatechange = function() {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    })
  
      win.activateDarkMode = function () {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = function () {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
          if (t === 'dark') activateDarkMode()
          else if (t === 'light') activateLightMode()
        
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
    const detectApple = () => {
      if (GLOBAL_CONFIG_SITE.isHome && /iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
        document.documentElement.classList.add('apple')
      }
    }
    detectApple()
    })(window)</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/pool.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/css/iconfont.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/StaticFile_HEXO@latest/butterfly/js/pool.min.js"><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/sviptzk/HexoStaticFile@latest/Hexo/js/mouse_snow.min.js"><link rel="stylesheet" href="/css/custom.css?v1"><link rel="stylesheet" href="//at.alicdn.com/t/font_2264842_b004iy0kk2b.css" media="defer" onload="this.media='all'"><!-- hexo injector head_end start --><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiperstyle.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.css" media="print" onload="this.media='all'"><!-- hexo injector head_end end --><meta name="generator" content="Hexo 5.4.0"></head><body><div id="web_bg"></div><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/images/head.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="site-data"><div class="data-item is-center"><div class="data-item-link"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">58</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div></div><div class="data-item is-center"><div class="data-item-link"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div></div><hr/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div></div></div><div class="post" id="body-wrap"><header class="post-bg" id="page-header" style="background-image: url('/2022/01/18/5.5-Spark-core/cover.jpeg')"><nav id="nav"><span id="blog_name"><a id="site-name" href="/">一直进步 做喜欢的</a></span><div id="menus"><div id="search-button"><a class="site-page social-icon search"><i class="fas fa-search fa-fw"></i><span> Search</span></a></div><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> Home</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> Archives</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> Tags</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> Categories</span></a></div></div><div id="toggle-menu"><a class="site-page"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="post-info"><h1 class="post-title">Spark core</h1><div id="post-meta"><div class="meta-firstline"><span class="post-meta-date"><i class="far fa-calendar-alt fa-fw post-meta-icon"></i><span class="post-meta-label">Created</span><time class="post-meta-date-created" datetime="2022-01-18T13:44:29.000Z" title="Created 2022-01-18 21:44:29">2022-01-18</time><span class="post-meta-separator">|</span><i class="fas fa-history fa-fw post-meta-icon"></i><span class="post-meta-label">Updated</span><time class="post-meta-date-updated" datetime="2025-07-20T07:30:57.188Z" title="Updated 2025-07-20 15:30:57">2025-07-20</time></span><span class="post-meta-categories"><span class="post-meta-separator">|</span><i class="fas fa-inbox fa-fw post-meta-icon"></i><a class="post-meta-categories" href="/categories/Search-Advertisement-Recommendation-Causal/">Search / Advertisement / Recommendation / Causal</a></span></div><div class="meta-secondline"><span class="post-meta-separator">|</span><span class="post-meta-wordcount"><i class="far fa-file-word fa-fw post-meta-icon"></i><span class="post-meta-label">Word count:</span><span class="word-count">5.4k</span><span class="post-meta-separator">|</span><i class="far fa-clock fa-fw post-meta-icon"></i><span class="post-meta-label">Reading time:</span><span>24min</span></span><span class="post-meta-separator">|</span><span class="post-meta-pv-cv" id="" data-flag-title="Spark core"><i class="far fa-eye fa-fw post-meta-icon"></i><span class="post-meta-label">Post View:</span><span id="busuanzi_value_page_pv"></span></span><span class="post-meta-separator">|</span><span class="post-meta-commentcount"><i class="far fa-comments fa-fw post-meta-icon"></i><span class="post-meta-label">Comments:</span><a href="/2022/01/18/5.5-Spark-core/#post-comment"><span class="gitalk-comment-count"></span></a></span></div></div></div></header><main class="layout" id="content-inner"><div id="post"><article class="post-content" id="article-container"><p><strong>推荐系统学习笔记目录</strong></p>
<ol>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.1-Recommendation-System-Introduction/">推荐系统介绍</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.2-RS-Algorithm/">推荐算法</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.3-Hadoop/">Hadoop</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.4-Hive/">Hive &amp; HBase</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/">Spark core</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.6-Spark-SQL/">Spark SQL &amp; Spark streaming</a></li>
<li><a href="https://xfliu1998.github.io/2022/01/18/5.7-RS-case/">推荐系统案例</a></li>
</ol>
<h2 id="spark-入门"><a href="#spark-入门" class="headerlink" title="spark 入门"></a>spark 入门</h2><h3 id="spark概述"><a href="#spark概述" class="headerlink" title="spark概述"></a>spark概述</h3><ul>
<li><p>1、什么是spark</p>
<ul>
<li>基于内存的计算引擎，它的计算速度非常快。但是仅仅只涉及到数据的计算，并没有涉及到数据的存储。</li>
</ul>
</li>
<li><p>2、为什么要学习spark<br><strong>MapReduce框架局限性</strong></p>
<ul>
<li><ol>
<li>Map结果写磁盘，Reduce写HDFS，多个MR之间通过HDFS交换数据</li>
</ol>
</li>
<li><ol start="2">
<li>任务调度和启动开销大</li>
</ol>
</li>
<li><ol start="3">
<li>无法充分利用内存</li>
</ol>
</li>
<li><ol start="4">
<li>不适合迭代计算（如机器学习、图计算等等），交互式处理（数据挖掘）</li>
</ol>
</li>
<li><ol start="5">
<li>不适合流式处理（点击日志分析）</li>
</ol>
</li>
<li><ol start="6">
<li>MapReduce编程不够灵活，仅支持Map和Reduce两种操作</li>
</ol>
</li>
</ul>
<p><strong>Hadoop生态圈</strong></p>
<ul>
<li>批处理：MapReduce、Hive、Pig</li>
<li>流式计算：Storm</li>
<li>交互式计算：Impala、presto</li>
</ul>
<p>需要一种灵活的框架可同时进行批处理、流式计算、交互式计算</p>
<ul>
<li>内存计算引擎，提供cache机制来支持需要反复迭代计算或者多次数据共享，减少数据读取的IO开销</li>
<li>DAG引擎，较少多次计算之间中间结果写到HDFS的开销</li>
<li>使用多线程模型来减少task启动开销，shuffle过程中避免不必要的sort操作以及减少磁盘IO</li>
</ul>
<p>spark的缺点是：吃内存，不太稳定</p>
</li>
<li><p>3、spark特点</p>
<ul>
<li>1、速度快（比mapreduce在内存中快100倍，在磁盘中快10倍）<ul>
<li>spark中的job中间结果可以不落地，可以存放在内存中。</li>
<li>mapreduce中map和reduce任务都是以进程的方式运行着，而spark中的job是以线程方式运行在进程中。</li>
</ul>
</li>
<li>2、易用性（可以通过java/scala/python/R开发spark应用程序）</li>
<li>3、通用性（可以使用spark sql/spark streaming/mlib/Graphx）</li>
<li>4、兼容性（spark程序可以运行在standalone/yarn/mesos）</li>
</ul>
</li>
</ul>
<h3 id="spark启动（local模式）和WordCount-演示"><a href="#spark启动（local模式）和WordCount-演示" class="headerlink" title="spark启动（local模式）和WordCount(演示)"></a>spark启动（local模式）和WordCount(演示)</h3><ul>
<li><p>启动pyspark</p>
<ul>
<li><p>在$SPARK_HOME/sbin目录下执行</p>
<ul>
<li>./pyspark<br><img src="pyspark.png"></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sc = spark.sparkContext</span><br><span class="line">words = sc.textFile(<span class="string">&#x27;file:///home/hadoop/tmp/word.txt&#x27;</span>) \</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">            .<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a + b).collect()</span><br></pre></td></tr></table></figure></li>
<li><p>输出结果：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[(&#x27;python&#x27;, 2), (&#x27;hadoop&#x27;, 1), (&#x27;bc&#x27;, 1), (&#x27;foo&#x27;, 4), (&#x27;test&#x27;, 2), (&#x27;bar&#x27;, 2), (&#x27;quux&#x27;, 2), (&#x27;abc&#x27;, 2), (&#x27;ab&#x27;, 1), (&#x27;you&#x27;, 1), (&#x27;ac&#x27;, 1), (&#x27;bec&#x27;, 1), (&#x27;by&#x27;, 1), (&#x27;see&#x27;, 1), (&#x27;labs&#x27;, 2), (&#x27;me&#x27;, 1), (&#x27;welcome&#x27;, 1)]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="spark-core概述"><a href="#spark-core概述" class="headerlink" title="spark-core概述"></a>spark-core概述</h2><h3 id="什么是RDD"><a href="#什么是RDD" class="headerlink" title="什么是RDD"></a>什么是RDD</h3><ul>
<li>RDD（Resilient Distributed Dataset）叫做弹性分布式数据集，是Spark中最基本的数据抽象，它代表一个不可变、可分区、里面的元素可并行计算的集合.<ul>
<li>Dataset:一个数据集，简单的理解为集合，用于存放数据的</li>
<li>Distributed：它的数据是分布式存储，并且可以做分布式的计算</li>
<li>Resilient：弹性的<ul>
<li>它表示的是数据可以保存在磁盘，也可以保存在内存中</li>
<li>数据分布式也是弹性的</li>
<li>弹性:并不是指他可以动态扩展，而是容错机制。<ul>
<li>RDD会在多个节点上存储，就和hdfs的分布式道理是一样的。hdfs文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同的节点上</li>
<li>spark读取hdfs的场景下，spark把hdfs的block读到内存就会抽象为spark的partition。</li>
<li>spark计算结束，一般会把数据做持久化到hive，hbase，hdfs等等。我们就拿hdfs举例，将RDD持久化到hdfs上，RDD的每个partition就会存成一个文件，如果文件小于128M，就可以理解为一个partition对应hdfs的一个block。反之，如果大于128M，就会被且分为多个block，这样，一个partition就会对应多个block。</li>
</ul>
</li>
</ul>
</li>
<li>不可变</li>
<li>可分区</li>
<li>并行计算</li>
</ul>
</li>
</ul>
<h3 id="RDD的创建"><a href="#RDD的创建" class="headerlink" title="RDD的创建"></a>RDD的创建</h3><ul>
<li><p>第一步 创建sparkContext</p>
<ul>
<li>SparkContext, Spark程序的入口. SparkContext代表了和Spark集群的链接, 在Spark集群中通过SparkContext来创建RDD</li>
<li>SparkConf  创建SparkContext的时候需要一个SparkConf， 用来传递Spark应用的基本信息</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conf = SparkConf().setAppName(appName).setMaster(master)</span><br><span class="line">sc = SparkContext(conf=conf)</span><br></pre></td></tr></table></figure></li>
<li><p>创建RDD</p>
<ul>
<li>进入pyspark环境</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop000 ~]$ pyspark</span><br><span class="line">Python 3.5.0 (default, Nov 13 2018, 15:43:53)</span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-28)] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">19/03/08 12:19:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &#x27;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.3.0</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.5.0 (default, Nov 13 2018 15:43:53)</span><br><span class="line">SparkSession available as &#x27;spark&#x27;.</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sc</span></span><br><span class="line">&lt;SparkContext master=local[*] appName=PySparkShell&gt;</span><br></pre></td></tr></table></figure>

<ul>
<li>在spark shell中 已经为我们创建好了 SparkContext 通过sc直接使用</li>
<li>可以在spark UI中看到当前的Spark作业 在浏览器访问当前centos的4040端口</li>
</ul>
<p><img src="sparkui.png"></p>
<ul>
<li><p>Parallelized Collections方式创建RDD</p>
<ul>
<li>调用<code>SparkContext</code>的 <code>parallelize</code> 方法并且传入已有的可迭代对象或者集合</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">data = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]</span><br><span class="line">distData = sc.parallelize(data)</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; data = [1, 2, 3, 4, 5]</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; distData = sc.parallelize(data)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; data</span></span><br><span class="line">[1, 2, 3, 4, 5]</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; distData</span></span><br><span class="line">ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:175</span><br></pre></td></tr></table></figure>

<ul>
<li>在spark ui中观察执行情况</li>
</ul>
<p><img src="createrdd.png" alt="createrdd"></p>
<ul>
<li>在通过<code>parallelize</code>方法创建RDD 的时候可以指定分区数量</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; distData = sc.parallelize(data,5)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; distData.reduce(lambda a, b: a + b)</span></span><br><span class="line">15</span><br></pre></td></tr></table></figure>

<ul>
<li>在spark ui中观察执行情况</li>
</ul>
<p><img src="createrdd2.png"></p>
<ul>
<li> Spark将为群集的每个分区（partition）运行一个任务（task）。 通常，可以根据CPU核心数量指定分区数量（每个CPU有2-4个分区）如未指定分区数量，Spark会自动设置分区数。</li>
</ul>
</li>
<li><p>通过外部数据创建RDD</p>
<ul>
<li>PySpark可以从Hadoop支持的任何存储源创建RDD，包括本地文件系统，HDFS，Cassandra，HBase，Amazon S3等</li>
<li>支持整个目录、多文件、通配符</li>
<li>支持压缩文件</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.textFile(<span class="string">&#x27;file:///home/hadoop/tmp/word.txt&#x27;</span>)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1.collect()</span></span><br><span class="line">[&#x27;foo foo quux labs foo bar quux abc bar see you by test welcome test&#x27;, &#x27;abc labs foo me python hadoop ab ac bc bec python&#x27;]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h2 id="spark-core-RDD常用算子练习"><a href="#spark-core-RDD常用算子练习" class="headerlink" title="spark-core RDD常用算子练习"></a>spark-core RDD常用算子练习</h2><h3 id="RDD-常用操作"><a href="#RDD-常用操作" class="headerlink" title="RDD 常用操作"></a>RDD 常用操作</h3><ul>
<li><p>RDD 支持两种类型的操作：</p>
<ul>
<li>transformation<ul>
<li>从一个已经存在的数据集创建一个新的数据集<ul>
<li>rdd a —–&gt;transformation —-&gt; rdd b</li>
</ul>
</li>
<li>比如， map就是一个transformation 操作，把数据集中的每一个元素传给一个函数并<strong>返回一个新的RDD</strong>，代表transformation操作的结果 </li>
</ul>
</li>
<li>action<ul>
<li>获取对数据进行运算操作之后的结果</li>
<li>比如， reduce 就是一个action操作，使用某个函数聚合RDD所有元素的操作，并<strong>返回最终计算结果</strong></li>
</ul>
</li>
</ul>
</li>
<li><p>所有的transformation操作都是惰性的（lazy）</p>
<ul>
<li>不会立即计算结果</li>
<li>只记下应用于数据集的transformation操作</li>
<li>只有调用action一类的操作之后才会计算所有transformation</li>
<li>这种设计使Spark运行效率更高</li>
<li>例如map reduce 操作，map创建的数据集将用于reduce，map阶段的结果不会返回，仅会返回reduce结果。</li>
</ul>
</li>
<li><p><em>persist</em> 操作</p>
<ul>
<li><em>persist</em>操作用于将数据缓存 可以缓存在内存中 也可以缓存到磁盘上， 也可以复制到磁盘的其它节点上</li>
</ul>
</li>
</ul>
<h3 id="RDD-Transformation算子"><a href="#RDD-Transformation算子" class="headerlink" title="RDD Transformation算子"></a>RDD Transformation算子</h3><ul>
<li><p>map: map(func)</p>
<ul>
<li>将func函数作用到数据集的每一个元素上，生成一个新的RDD返回</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = rdd1.map(lambda x: x+1)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2.collect()</span></span><br><span class="line">[2, 3, 4, 5, 6, 7, 8, 9, 10]</span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; def add(x):</span></span><br><span class="line">...     return x+1</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = rdd1.map(add)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2.collect()</span></span><br><span class="line">[2, 3, 4, 5, 6, 7, 8, 9, 10]</span><br></pre></td></tr></table></figure>

<p><img src="rdd_map.png"></p>
</li>
<li><p>filter</p>
<ul>
<li>filter(func) 选出所有func返回值为true的元素，生成一个新的RDD返回</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5,6,7,8,9],3)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = rdd1.map(lambda x:x*2)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd3 = rdd2.filter(lambda x:x&gt;4)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd3.collect()</span></span><br><span class="line">[6, 8, 10, 12, 14, 16, 18]</span><br></pre></td></tr></table></figure></li>
<li><p>flatmap</p>
<ul>
<li>flatMap会先执行map的操作，再将所有对象合并为一个对象</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([<span class="string">&quot;a b c&quot;</span>,<span class="string">&quot;d e f&quot;</span>,<span class="string">&quot;h i j&quot;</span>])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = rdd1.flatMap(lambda x:x.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2.collect()</span></span><br><span class="line">[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;, &#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;, &#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;]</span><br></pre></td></tr></table></figure>

<ul>
<li>flatMap和map的区别：flatMap在map的基础上将结果合并到一个list中</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([<span class="string">&quot;a b c&quot;</span>,<span class="string">&quot;d e f&quot;</span>,<span class="string">&quot;h i j&quot;</span>])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = rdd1.map(lambda x:x.split(<span class="string">&quot; &quot;</span>))</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2.collect()</span></span><br><span class="line">[[&#x27;a&#x27;, &#x27;b&#x27;, &#x27;c&#x27;], [&#x27;d&#x27;, &#x27;e&#x27;, &#x27;f&#x27;], [&#x27;h&#x27;, &#x27;i&#x27;, &#x27;j&#x27;]]</span><br></pre></td></tr></table></figure></li>
<li><p>union</p>
<ul>
<li>对两个RDD求并集</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>,1),(<span class="string">&quot;b&quot;</span>,2)])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>,1),(<span class="string">&quot;b&quot;</span>,3)])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd3 = rdd1.union(rdd2)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd3.collect()</span></span><br><span class="line">[(&#x27;a&#x27;, 1), (&#x27;b&#x27;, 2), (&#x27;c&#x27;, 1), (&#x27;b&#x27;, 3)]</span><br></pre></td></tr></table></figure></li>
<li><p>intersection</p>
<ul>
<li>对两个RDD求交集</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">2</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>,<span class="number">1</span>),(<span class="string">&quot;b&quot;</span>,<span class="number">3</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd3 = rdd1.union(rdd2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd4 = rdd3.intersection(rdd2)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd4.collect()</span><br><span class="line">[(<span class="string">&#x27;c&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">3</span>)]</span><br></pre></td></tr></table></figure></li>
<li><p>groupByKey</p>
<ul>
<li>以元组中的第0个元素作为key，进行分组，返回一个新的RDD</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([(<span class="string">&quot;a&quot;</span>,1),(<span class="string">&quot;b&quot;</span>,2)])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd2 = sc.parallelize([(<span class="string">&quot;c&quot;</span>,1),(<span class="string">&quot;b&quot;</span>,3)])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd3 = rdd1.union(rdd2)</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd4 = rdd3.groupByKey()</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd4.collect()</span></span><br><span class="line">[(&#x27;a&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5898&gt;), (&#x27;c&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5518&gt;), (&#x27;b&#x27;, &lt;pyspark.resultiterable.ResultIterable object at 0x7fba6a5e5f28&gt;)]</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<ul>
<li>groupByKey之后的结果中 value是一个Iterable</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">2</span>]</span><br><span class="line">(<span class="string">&#x27;b&#x27;</span>, &lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6c18e518</span>&gt;)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>result[<span class="number">2</span>][<span class="number">1</span>]</span><br><span class="line">&lt;pyspark.resultiterable.ResultIterable <span class="built_in">object</span> at <span class="number">0x7fba6c18e518</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">list</span>(result[<span class="number">2</span>][<span class="number">1</span>])</span><br><span class="line">[<span class="number">2</span>, <span class="number">3</span>]</span><br></pre></td></tr></table></figure>

<ul>
<li><p>reduceByKey</p>
<ul>
<li>将key相同的键值对，按照Function进行计算</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd = sc.parallelize([(<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">1</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>rdd.reduceByKey(<span class="keyword">lambda</span> x,y:x+y).collect()</span><br><span class="line">[(<span class="string">&#x27;b&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">2</span>)]</span><br></pre></td></tr></table></figure></li>
<li><p>sortByKey</p>
<ul>
<li><p><code>sortByKey</code>(<em>ascending=True</em>, <em>numPartitions=None</em>, <em>keyfunc=&lt;function RDD.<lambda>&gt;</em>)</p>
<p>Sorts this RDD, which is assumed to consist of (key, value) pairs.</p>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>tmp = [(<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize(tmp).sortByKey().first()</span><br><span class="line">(<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize(tmp).sortByKey(<span class="literal">True</span>, <span class="number">1</span>).collect()</span><br><span class="line">[(<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize(tmp).sortByKey(<span class="literal">True</span>, <span class="number">2</span>).collect()</span><br><span class="line">[(<span class="string">&#x27;1&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;2&#x27;</span>, <span class="number">5</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;b&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;d&#x27;</span>, <span class="number">4</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tmp2 = [(<span class="string">&#x27;Mary&#x27;</span>, <span class="number">1</span>), (<span class="string">&#x27;had&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;a&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;little&#x27;</span>, <span class="number">4</span>), (<span class="string">&#x27;lamb&#x27;</span>, <span class="number">5</span>)]</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tmp2.extend([(<span class="string">&#x27;whose&#x27;</span>, <span class="number">6</span>), (<span class="string">&#x27;fleece&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;was&#x27;</span>, <span class="number">8</span>), (<span class="string">&#x27;white&#x27;</span>, <span class="number">9</span>)])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize(tmp2).sortByKey(<span class="literal">True</span>, <span class="number">3</span>, keyfunc=<span class="keyword">lambda</span> k: k.lower()).collect()</span><br><span class="line">[(<span class="string">&#x27;a&#x27;</span>, <span class="number">3</span>), (<span class="string">&#x27;fleece&#x27;</span>, <span class="number">7</span>), (<span class="string">&#x27;had&#x27;</span>, <span class="number">2</span>), (<span class="string">&#x27;lamb&#x27;</span>, <span class="number">5</span>),...(<span class="string">&#x27;white&#x27;</span>, <span class="number">9</span>), (<span class="string">&#x27;whose&#x27;</span>, <span class="number">6</span>)]</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
</ul>
<h3 id="RDD-Action算子"><a href="#RDD-Action算子" class="headerlink" title="RDD Action算子"></a>RDD Action算子</h3><ul>
<li><p>collect</p>
<ul>
<li>返回一个list，list中包含 RDD中的所有元素</li>
<li>只有当数据量较小的时候使用Collect 因为所有的结果都会加载到内存中</li>
</ul>
</li>
<li><p>reduce</p>
<ul>
<li><strong>reduce</strong>将<strong>RDD</strong>中元素两两传递给输入函数，同时产生一个新的值，新产生的值与RDD中下一个元素再被传递给输入函数直到最后只有一个值为止。</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1 = sc.parallelize([1,2,3,4,5])</span></span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; rdd1.reduce(lambda x,y : x+y)</span></span><br><span class="line">15</span><br></pre></td></tr></table></figure></li>
<li><p>first</p>
<ul>
<li>返回RDD的第一个元素</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>sc.parallelize([<span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]).first()</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure></li>
<li><p>take</p>
<ul>
<li>返回RDD的前N个元素</li>
<li><code>take</code>(<em>num</em>)</li>
</ul>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(2)</span></span><br><span class="line">[2, 3]</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sc.parallelize([2, 3, 4, 5, 6]).take(10)</span></span><br><span class="line">[2, 3, 4, 5, 6]</span><br><span class="line"><span class="meta">&gt;</span><span class="bash">&gt;&gt; sc.parallelize(range(100), 100).filter(lambda x: x &gt; 90).take(3)</span></span><br><span class="line">[91, 92, 93]</span><br></pre></td></tr></table></figure></li>
<li><p>count</p>
<p>返回RDD中元素的个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; sc.parallelize([2, 3, 4]).count()</span><br><span class="line">3</span><br></pre></td></tr></table></figure></li>
</ul>
<h3 id="Spark-RDD两类算子执行示意"><a href="#Spark-RDD两类算子执行示意" class="headerlink" title="Spark RDD两类算子执行示意"></a>Spark RDD两类算子执行示意</h3><p><img src="s5.png" alt="s5"><br><img src="s6.png" alt="s6"></p>
<h2 id="spark-core-实战案例"><a href="#spark-core-实战案例" class="headerlink" title="spark-core 实战案例"></a>spark-core 实战案例</h2><h3 id="Pycharm编写spark代码环境配置"><a href="#Pycharm编写spark代码环境配置" class="headerlink" title="Pycharm编写spark代码环境配置"></a>Pycharm编写spark代码环境配置</h3><p>准备pycharm环境</p>
<ul>
<li><ol>
<li>对接到centos服务器，下载环境</li>
</ol>
<ul>
<li>1.1 选择Tools –&gt;Deployment–&gt;Configuration<br>注：选择Type为SFTP，写入主机名，登陆的用户名和密码<br>注：选择Deployment目录为基准的根目录</li>
<li>1.2 选择File–&gt;settings–&gt;Project xxx–&gt;Project Interpreter<br>注：输入远程连接的主机名，登陆的用户名和密码，进行远程python环境的对接。</li>
</ul>
</li>
</ul>
<h3 id="利用PyCharm编写spark-wordcount程序"><a href="#利用PyCharm编写spark-wordcount程序" class="headerlink" title="利用PyCharm编写spark wordcount程序"></a>利用PyCharm编写spark wordcount程序</h3><ul>
<li><p>环境配置<br>将spark目录下的python目录下的pyspark整体拷贝到pycharm使用的python环境下<br>将pyspark拷贝到pycharm使用的：xxx\Python\Python36\Lib\site-packages目录下</p>
</li>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(sys.argv) != <span class="number">2</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">&quot;Usage: avg &lt;input&gt;&quot;</span>, file=sys.stderr)</span><br><span class="line">        sys.exit(-<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&quot;test&quot;</span>).getOrCreate()</span><br><span class="line">	sc = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    counts = sc.textFile(sys.argv[<span class="number">1</span>]) \</span><br><span class="line">            .flatMap(<span class="keyword">lambda</span> line: line.split(<span class="string">&quot; &quot;</span>)) \</span><br><span class="line">            .<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x, <span class="number">1</span>)) \</span><br><span class="line">            .reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line"></span><br><span class="line">    output = counts.collect()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (word, count) <span class="keyword">in</span> output:</span><br><span class="line">    	<span class="built_in">print</span>(<span class="string">&quot;%s: %i&quot;</span> % (word, count))</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br></pre></td></tr></table></figure></li>
<li><p>将代码上传到远程cent-os系统上</p>
</li>
<li><p>在系统上执行指令</p>
<p><code>spark-submit --master local wc.py file:///root/bigdata/data/spark_test.log</code></p>
</li>
</ul>
<h3 id="通过spark实现点击流日志分析"><a href="#通过spark实现点击流日志分析" class="headerlink" title="通过spark实现点击流日志分析"></a>通过spark实现点击流日志分析</h3><p>在新闻类网站中，经常要衡量一条网络新闻的页面访问量，最常见的就是uv和pv，如果在所有新闻中找到访问最多的前几条新闻，topN是最常见的指标。</p>
<ul>
<li><p>数据示例</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">每条数据代表一次访问记录 包含了ip 访问时间 访问的请求方式 访问的地址...信息</span></span><br><span class="line">194.237.142.21 - - [18/Sep/2013:06:49:18 +0000] &quot;GET /wp-content/uploads/2013/07/rstudio-git3.png HTTP/1.1&quot; 304 0 &quot;-&quot; &quot;Mozilla/4.0 (compatible;)&quot;</span><br><span class="line">183.49.46.228 - - [18/Sep/2013:06:49:23 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;</span><br><span class="line">163.177.71.12 - - [18/Sep/2013:06:49:33 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</span><br><span class="line">163.177.71.12 - - [18/Sep/2013:06:49:36 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</span><br><span class="line">101.226.68.137 - - [18/Sep/2013:06:49:42 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</span><br><span class="line">101.226.68.137 - - [18/Sep/2013:06:49:45 +0000] &quot;HEAD / HTTP/1.1&quot; 200 20 &quot;-&quot; &quot;DNSPod-Monitor/1.0&quot;</span><br><span class="line">60.208.6.156 - - [18/Sep/2013:06:49:48 +0000] &quot;GET /wp-content/uploads/2013/07/rcassandra.png HTTP/1.0&quot; 200 185524 &quot;http://cos.name/category/software/packages/&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;</span><br><span class="line">222.68.172.190 - - [18/Sep/2013:06:49:57 +0000] &quot;GET /images/my.jpg HTTP/1.1&quot; 200 19939 &quot;http://www.angularjs.cn/A00n&quot; &quot;Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/29.0.1547.66 Safari/537.36&quot;</span><br><span class="line">222.68.172.190 - - [18/Sep/2013:06:50:08 +0000] &quot;-&quot; 400 0 &quot;-&quot; &quot;-&quot;</span><br></pre></td></tr></table></figure></li>
<li><p>访问的pv</p>
<p>pv：网站的总访问量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;pv&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;file:///root/bigdata/data/access.log&quot;</span>)</span><br><span class="line"><span class="comment">#把每一行数据记为(&quot;pv&quot;,1)</span></span><br><span class="line">rdd2 = rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(<span class="string">&quot;pv&quot;</span>,<span class="number">1</span>)).reduceByKey(<span class="keyword">lambda</span> a,b:a+b)</span><br><span class="line">rdd2.collect()</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li>
<li><p>访问的uv</p>
<p>uv：网站的独立用户访问量</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;pv&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;file:///root/bigdata/data/access.log&quot;</span>)</span><br><span class="line"><span class="comment">#对每一行按照空格拆分，将ip地址取出</span></span><br><span class="line">rdd2 = rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&quot; &quot;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line"><span class="comment">#把每个ur记为1</span></span><br><span class="line">rdd3 = rdd2.distinct().<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(<span class="string">&quot;uv&quot;</span>,<span class="number">1</span>))</span><br><span class="line">rdd4 = rdd3.reduceByKey(<span class="keyword">lambda</span> a,b:a+b)</span><br><span class="line">rdd4.saveAsTextFile(<span class="string">&quot;hdfs:///uv/result&quot;</span>)</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li>
<li><p>访问的topN</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"></span><br><span class="line">spark = SparkSession.builder.appName(<span class="string">&quot;topN&quot;</span>).getOrCreate()</span><br><span class="line">sc = spark.sparkContext</span><br><span class="line">rdd1 = sc.textFile(<span class="string">&quot;file:///root/bigdata/data/access.log&quot;</span>)</span><br><span class="line"><span class="comment">#对每一行按照空格拆分，将url数据取出，把每个url记为1</span></span><br><span class="line">rdd2 = rdd1.<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&quot; &quot;</span>)).<span class="built_in">filter</span>(<span class="keyword">lambda</span> x:<span class="built_in">len</span>(x)&gt;<span class="number">10</span>).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:(x[<span class="number">10</span>],<span class="number">1</span>))</span><br><span class="line"><span class="comment">#对数据进行累加，按照url出现次数的降序排列</span></span><br><span class="line">rdd3 = rdd2.reduceByKey(<span class="keyword">lambda</span> a,b:a+b).sortBy(<span class="keyword">lambda</span> x:x[<span class="number">1</span>],ascending=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#取出序列数据中的前n个</span></span><br><span class="line">rdd4 = rdd3.take(<span class="number">5</span>)</span><br><span class="line">rdd4.collect()</span><br><span class="line">sc.stop()</span><br></pre></td></tr></table></figure></li>
</ul>
<h2 id="spark-core实战"><a href="#spark-core实战" class="headerlink" title="spark-core实战"></a>spark-core实战</h2><h3 id="通过spark实现ip地址查询"><a href="#通过spark实现ip地址查询" class="headerlink" title="通过spark实现ip地址查询"></a>通过spark实现ip地址查询</h3><p><strong>需求</strong><br>在互联网中，我们经常会见到城市热点图这样的报表数据，例如在百度统计中，会统计今年的热门旅游城市、热门报考学校等，会将这样的信息显示在热点图中。</p>
<p>因此，我们需要通过日志信息（运行商或者网站自己生成）和城市ip段信息来判断用户的ip段，统计热点经纬度。</p>
<p><strong>ip日志信息</strong><br>在ip日志信息中，我们只需要关心ip这一个维度就可以了，其他的不做介绍</p>
<p><strong>思路</strong><br>1、 加载城市ip段信息，获取ip起始数字和结束数字，经度，纬度<br>2、 加载日志数据，获取ip信息，然后转换为数字，和ip段比较<br>3、 比较的时候采用二分法查找，找到对应的经度和纬度<br>4，对相同的经度和纬度做累计求和</p>
<p><strong>启动Spark集群</strong></p>
<ul>
<li><p>进入到$SPARK_HOME/sbin目录</p>
<ul>
<li>启动Master    <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-master.sh -h 192.168.199.188</span><br></pre></td></tr></table></figure></li>
<li>启动Slave<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-slave.sh spark://192.168.199.188:7077</span><br></pre></td></tr></table></figure></li>
<li>jps查看进程<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">27073 Master</span><br><span class="line">27151 Worker</span><br></pre></td></tr></table></figure></li>
<li>关闭防火墙<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systemctl stop firewalld</span><br></pre></td></tr></table></figure></li>
<li>通过SPARK WEB UI查看Spark集群及Spark<ul>
<li><a target="_blank" rel="noopener" href="http://192.168.199.188:8080/">http://192.168.199.188:8080/</a>  监控Spark集群</li>
<li><a target="_blank" rel="noopener" href="http://192.168.199.188:4040/">http://192.168.199.188:4040/</a>  监控Spark Job</li>
</ul>
</li>
</ul>
</li>
<li><p>代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pyspark.sql <span class="keyword">import</span> SparkSession</span><br><span class="line"><span class="comment"># 255.255.255.255 0~255 256  2^8 8位2进制数</span></span><br><span class="line"><span class="comment">#将ip转换为特殊的数字形式  223.243.0.0|223.243.191.255|  255 2^8</span></span><br><span class="line"><span class="comment">#‭11011111‬</span></span><br><span class="line"><span class="comment">#00000000</span></span><br><span class="line"><span class="comment">#1101111100000000</span></span><br><span class="line"><span class="comment">#‭        11110011‬</span></span><br><span class="line"><span class="comment">#11011111111100110000000000000000</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ip_transform</span>(<span class="params">ip</span>):</span>     </span><br><span class="line">    ips = ip.split(<span class="string">&quot;.&quot;</span>)<span class="comment">#[223,243,0,0] 32位二进制数</span></span><br><span class="line">    ip_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> ips:</span><br><span class="line">        ip_num = <span class="built_in">int</span>(i) | ip_num &lt;&lt; <span class="number">8</span></span><br><span class="line">    <span class="keyword">return</span> ip_num</span><br><span class="line"></span><br><span class="line"><span class="comment">#二分法查找ip对应的行的索引</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search</span>(<span class="params">ip_num, broadcast_value</span>):</span></span><br><span class="line">    start = <span class="number">0</span></span><br><span class="line">    end = <span class="built_in">len</span>(broadcast_value) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> (start &lt;= end):</span><br><span class="line">        mid = <span class="built_in">int</span>((start + end) / <span class="number">2</span>)</span><br><span class="line">        <span class="keyword">if</span> ip_num &gt;= <span class="built_in">int</span>(broadcast_value[mid][<span class="number">0</span>]) <span class="keyword">and</span> ip_num &lt;= <span class="built_in">int</span>(broadcast_value[mid][<span class="number">1</span>]):</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="keyword">if</span> ip_num &lt; <span class="built_in">int</span>(broadcast_value[mid][<span class="number">0</span>]):</span><br><span class="line">            end = mid</span><br><span class="line">        <span class="keyword">if</span> ip_num &gt; <span class="built_in">int</span>(broadcast_value[mid][<span class="number">1</span>]):</span><br><span class="line">            start = mid</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span>():</span></span><br><span class="line">    spark = SparkSession.builder.appName(<span class="string">&quot;test&quot;</span>).getOrCreate()</span><br><span class="line">    sc = spark.sparkContext</span><br><span class="line">    city_id_rdd = sc.textFile(<span class="string">&quot;file:///home/hadoop/app/tmp/data/ip.txt&quot;</span>).<span class="built_in">map</span>(<span class="keyword">lambda</span> x:x.split(<span class="string">&quot;|&quot;</span>)).<span class="built_in">map</span>(<span class="keyword">lambda</span> x: (x[<span class="number">2</span>], x[<span class="number">3</span>], x[<span class="number">13</span>], x[<span class="number">14</span>]))</span><br><span class="line">    <span class="comment">#创建一个广播变量</span></span><br><span class="line">    city_broadcast = sc.broadcast(city_id_rdd.collect())</span><br><span class="line">    dest_data = sc.textFile(<span class="string">&quot;file:///home/hadoop/app/tmp/data/20090121000132.394251.http.format&quot;</span>).<span class="built_in">map</span>(</span><br><span class="line">        <span class="keyword">lambda</span> x: x.split(<span class="string">&quot;|&quot;</span>)[<span class="number">1</span>])</span><br><span class="line">    <span class="comment">#根据取出对应的位置信息</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_pos</span>(<span class="params">x</span>):</span></span><br><span class="line">        city_broadcast_value = city_broadcast.value</span><br><span class="line">        <span class="comment">#根据单个ip获取对应经纬度信息</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">get_result</span>(<span class="params">ip</span>):</span></span><br><span class="line">            ip_num = ip_transform(ip)</span><br><span class="line">            index = binary_search(ip_num, city_broadcast_value)</span><br><span class="line">            <span class="comment">#((纬度,精度),1)</span></span><br><span class="line">            <span class="keyword">return</span> ((city_broadcast_value[index][<span class="number">2</span>], city_broadcast_value[index][<span class="number">3</span>]), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        x = <span class="built_in">map</span>(<span class="built_in">tuple</span>,[get_result(ip) <span class="keyword">for</span> ip <span class="keyword">in</span> x])</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    dest_rdd = dest_data.mapPartitions(<span class="keyword">lambda</span> x: get_pos(x)) <span class="comment">#((纬度,精度),1)</span></span><br><span class="line">    result_rdd = dest_rdd.reduceByKey(<span class="keyword">lambda</span> a, b: a + b)</span><br><span class="line">    <span class="built_in">print</span>(result_rdd.collect())</span><br><span class="line">    sc.stop()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">&#x27;__main__&#x27;</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure></li>
<li><p><strong>广播变量的使用</strong></p>
<ul>
<li>要统计Ip所对应的经纬度, 每一条数据都会去查询ip表</li>
<li>每一个task 都需要这一个ip表, 默认情况下, 所有task都会去复制ip表</li>
<li>实际上 每一个Worker上会有多个task, 数据也是只需要进行查询操作的, 所以这份数据可以共享,没必要每个task复制一份</li>
<li>可以通过广播变量, 通知当前worker上所有的task, 来共享这个数据,避免数据的多次复制,可以大大降低内存的开销</li>
<li>sparkContext.broadcast(要共享的数据)</li>
</ul>
</li>
</ul>
<h2 id="spark-相关概念补充"><a href="#spark-相关概念补充" class="headerlink" title="spark 相关概念补充"></a>spark 相关概念补充</h2><h3 id="spark的安装部署"><a href="#spark的安装部署" class="headerlink" title="spark的安装部署"></a>spark的安装部署</h3><ul>
<li><p>1、下载spark安装包<br><a target="_blank" rel="noopener" href="http://spark.apache.org/downloads.html">http://spark.apache.org/downloads.html</a></p>
<p>高版本不存在cdh的编译版本，可以从官网下载源码版本，指定高版本hadoop进行编译</p>
<p>编译步骤：</p>
<ul>
<li><ol>
<li>安装java(JDK 1.7及以上)<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">export JAVA_HOME=/xxx</span><br><span class="line">export JRE_HOME=/xxx</span><br><span class="line">export CLASSPATH=.:$JAVA_HOME/lib/dt.jar:$JAVA_HOME/lib/tools.jar:$JRE_HOME/lib:$CLASSPATH</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><ol start="2">
<li>安装Maven， 版本为3.3.9或者以上<br>下载地址：<a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries">https://mirrors.tuna.tsinghua.edu.cn/apache//maven/maven-3/3.3.9/binaries</a><br>配置MAVEN_HOME<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">export MAVEN_HOME=/xxx</span><br><span class="line">export PATH=$MAVEN_HOME/bin:$PATH</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><ol start="3">
<li>下载spark源码</li>
</ol>
</li>
<li><ol start="4">
<li>增加cdh的repository<br>解压spark的源码包，编辑pom.xml文件， 在repositories节点 加入如下配置：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&lt;repository&gt;</span><br><span class="line">        &lt;id&gt;cloudera&lt;/id&gt;</span><br><span class="line">        &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt;&lt;/repository&gt;</span><br></pre></td></tr></table></figure></li>
</ol>
</li>
<li><ol start="5">
<li>编译<br>设置内存：<br>export MAVEN_OPTS=”-Xmx2g -XX:ReservedCodeCacheSize=512m”<br>开始编译：<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./dev/make-distribution.sh --name 2.6.0-cdh5.7.0 --tgz  -Pyarn -Phadoop-2.6 -Phive -Phive-thriftserver -Dhadoop.version=2.6.0-cdh5.7.0 -DskipTests clean package</span><br></pre></td></tr></table></figure>
源码编译后，bin目录下的文件可能不存在可执行权限，需要通过chmod指令添加可执行权限<br>chmod +x xxx</li>
</ol>
</li>
</ul>
</li>
<li><p>2、规划spark安装目录</p>
</li>
<li><p>3、解压安装包</p>
</li>
<li><p>4、重命名安装目录</p>
</li>
<li><p>5、修改配置文件</p>
<ul>
<li>spark-env.sh(需要将spark-env.sh.template重命名)<ul>
<li>配置java环境变量<ul>
<li>export JAVA_HOME=java_home_path</li>
</ul>
</li>
<li>配置PYTHON环境<ul>
<li>export PYSPARK_PYTHON=/xx/pythonx_home/bin/pythonx</li>
</ul>
</li>
<li>配置master的地址<ul>
<li>export SPARK_MASTER_HOST=node-teach</li>
</ul>
</li>
<li>配置master的端口<ul>
<li>export SPARK_MASTER_PORT=7077</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li><p>6、配置spark环境变量</p>
<ul>
<li>export SPARK_HOME=/xxx/spark2.x</li>
<li>export PATH=$PATH:$SPARK_HOME/bin</li>
</ul>
</li>
</ul>
<h3 id="spark-集群相关概念"><a href="#spark-集群相关概念" class="headerlink" title="spark 集群相关概念"></a>spark 集群相关概念</h3><ul>
<li><p>spark集群架构(Standalone模式)<br><img src="spark1.png"></p>
<ul>
<li><p>Application</p>
<p>用户自己写的Spark应用程序，批处理作业的集合。Application的main方法为应用程序的入口，用户通过Spark的API，定义了RDD和对RDD的操作。</p>
</li>
<li><p>Master和Worker</p>
<p>整个集群分为 Master 节点和 Worker 节点，相当于 Hadoop 的 Master 和 Slave 节点。</p>
<ul>
<li>Master：Standalone模式中主控节点，负责接收Client提交的作业，管理Worker，并命令Worker启动Driver和Executor。</li>
<li>Worker：Standalone模式中slave节点上的守护进程，负责管理本节点的资源，定期向Master汇报心跳，接收Master的命令，启动Driver和Executor。</li>
</ul>
</li>
<li><p>Client：客户端进程，负责提交作业到Master。</p>
</li>
<li><p>Driver： 一个Spark作业运行时包括一个Driver进程，也是作业的主进程，负责作业的解析、生成Stage并调度Task到Executor上。包括DAGScheduler，TaskScheduler。</p>
</li>
<li><p>Executor：即真正执行作业的地方，一个集群一般包含多个Executor，每个Executor接收Driver的命令Launch Task，一个Executor可以执行一到多个Task。</p>
</li>
</ul>
</li>
<li><p>Spark作业相关概念</p>
<ul>
<li><p>Stage：一个Spark作业一般包含一到多个Stage。</p>
</li>
<li><p>Task：一个Stage包含一到多个Task，通过多个Task实现并行运行的功能。</p>
</li>
<li><p>DAGScheduler： 实现将Spark作业分解成一到多个Stage，每个Stage根据RDD的Partition个数决定Task的个数，然后生成相应的Task set放到TaskScheduler中。</p>
</li>
<li><p>TaskScheduler：实现Task分配到Executor上执行。<br><img src="spark2.png"><br><img src="spark3.png"></p>
</li>
</ul>
</li>
</ul>
</article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">贪钱算法还我头发</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/">https://xfliu1998.github.io/2022/01/18/5.5-Spark-core/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="tag_share"><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/python/">python</a></div><div class="post_share"><div class="social-share" data-image="/2022/01/18/5.5-Spark-core/cover.jpeg" data-sites="facebook,twitter,wechat,weibo,qq"></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js/dist/css/share.min.css" media="print" onload="this.media='all'"><script src="https://cdn.jsdelivr.net/npm/social-share.js/dist/js/social-share.min.js" defer></script></div></div><nav class="pagination-post" id="pagination"><div class="prev-post pull-left"><a href="/2022/01/18/5.6-Spark-SQL/"><img class="prev-cover" src="/2022/01/18/5.6-Spark-SQL/cover.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of previous post"><div class="pagination-info"><div class="label">Previous Post</div><div class="prev_info">Spark SQL &amp; Spark streaming</div></div></a></div><div class="next-post pull-right"><a href="/2022/01/18/5.4-Hive/"><img class="next-cover" src="/2022/01/18/5.4-Hive/cover.jpeg" onerror="onerror=null;src='/img/404.jpg'" alt="cover of next post"><div class="pagination-info"><div class="label">Next Post</div><div class="next_info">Hive &amp; HBase</div></div></a></div></nav><div class="relatedPosts"><div class="headline"><i class="fas fa-thumbs-up fa-fw"></i><span>Related Articles</span></div><div class="relatedPosts-list"><div><a href="/2024/07/09/Technical-Summary-Deep-Learning/" title="Technical Summary —— Pytorch & Numpy"><img class="cover" src="/2024/07/09/Technical-Summary-Deep-Learning/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-09</div><div class="title">Technical Summary —— Pytorch & Numpy</div></div></a></div><div><a href="/2024/07/02/Technical-Summary-Necessary/" title="Technical Summary —— Common Command"><img class="cover" src="/2024/07/02/Technical-Summary-Necessary/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-07-02</div><div class="title">Technical Summary —— Common Command</div></div></a></div><div><a href="/2024/04/15/Technical-Summary-BigData/" title="Technical Summary —— Big Data Processing"><img class="cover" src="/2024/04/15/Technical-Summary-BigData/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-15</div><div class="title">Technical Summary —— Big Data Processing</div></div></a></div><div><a href="/2024/04/07/Technical-Summary-Database/" title="Technical Summary —— Database Usage"><img class="cover" src="/2024/04/07/Technical-Summary-Database/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2024-04-07</div><div class="title">Technical Summary —— Database Usage</div></div></a></div><div><a href="/2023/05/19/Summary-NG/" title="Summary NG"><img class="cover" src="/2023/05/19/Summary-NG/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Summary NG</div></div></a></div><div><a href="/2023/05/19/Experimental-Technique/" title="Experimental Technique"><img class="cover" src="/2023/05/19/Experimental-Technique/cover.jpeg" alt="cover"><div class="content is-center"><div class="date"><i class="far fa-calendar-alt fa-fw"></i> 2023-05-19</div><div class="title">Experimental Technique</div></div></a></div></div></div><hr/><div id="post-comment"><div class="comment-head"><div class="comment-headline"><i class="fas fa-comments fa-fw"></i><span> Comment</span></div></div><div class="comment-wrap"><div><div id="gitalk-container"></div></div></div></div></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/images/head.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">贪钱算法还我头发</div><div class="author-info__description"></div></div><div class="card-info-data"><div class="card-info-data-item is-center"><a href="/archives/"><div class="headline">Articles</div><div class="length-num">58</div></a></div><div class="card-info-data-item is-center"><a href="/tags/"><div class="headline">Tags</div><div class="length-num">15</div></a></div><div class="card-info-data-item is-center"><a href="/categories/"><div class="headline">Categories</div><div class="length-num">6</div></a></div></div><a class="button--animated" id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/xfliu1998"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://github.com/xfliu1998" target="_blank" title="Github"><i class="fab fa-github"></i></a><a class="social-icon" href="mailto:liuxiaofei_7@163.com" target="_blank" title="Email"><i class="fas fa-envelope"></i></a><a class="social-icon" href="https://www.zhihu.com/people/fan-xu-15-35/posts" target="_blank" title="Zhihu"><i class="fa fa-address-card"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn card-announcement-animation"></i><span>Announcement</span></div><div class="announcement_content">搭建这个小站源于一个朴素的愿望：对抗遗忘，沉淀思考。期待在代码与逻辑的世界里探索技术的深度与广度，永远保持热情与好奇。</div></div><div class="card-widget" id="newYear"><div class="item-headline"><i></i><span></span></div><div class="item-content"><div id="newYear-main"><div class="mask"></div> <p class="title"></p> <div class="newYear-time"></div> <p class="today" style="text-align: right;"></p> </div></div></div><div class="sticky_layout"><div class="card-widget" id="card-toc"><div class="item-headline"><i class="fas fa-stream"></i><span>Catalog</span></div><div class="toc-content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-%E5%85%A5%E9%97%A8"><span class="toc-number">1.</span> <span class="toc-text">spark 入门</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark%E6%A6%82%E8%BF%B0"><span class="toc-number">1.1.</span> <span class="toc-text">spark概述</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark%E5%90%AF%E5%8A%A8%EF%BC%88local%E6%A8%A1%E5%BC%8F%EF%BC%89%E5%92%8CWordCount-%E6%BC%94%E7%A4%BA"><span class="toc-number">1.2.</span> <span class="toc-text">spark启动（local模式）和WordCount(演示)</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-core%E6%A6%82%E8%BF%B0"><span class="toc-number">2.</span> <span class="toc-text">spark-core概述</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%80%E4%B9%88%E6%98%AFRDD"><span class="toc-number">2.1.</span> <span class="toc-text">什么是RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD%E7%9A%84%E5%88%9B%E5%BB%BA"><span class="toc-number">2.2.</span> <span class="toc-text">RDD的创建</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-core-RDD%E5%B8%B8%E7%94%A8%E7%AE%97%E5%AD%90%E7%BB%83%E4%B9%A0"><span class="toc-number">3.</span> <span class="toc-text">spark-core RDD常用算子练习</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C"><span class="toc-number">3.1.</span> <span class="toc-text">RDD 常用操作</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-Transformation%E7%AE%97%E5%AD%90"><span class="toc-number">3.2.</span> <span class="toc-text">RDD Transformation算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD-Action%E7%AE%97%E5%AD%90"><span class="toc-number">3.3.</span> <span class="toc-text">RDD Action算子</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-RDD%E4%B8%A4%E7%B1%BB%E7%AE%97%E5%AD%90%E6%89%A7%E8%A1%8C%E7%A4%BA%E6%84%8F"><span class="toc-number">3.4.</span> <span class="toc-text">Spark RDD两类算子执行示意</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-core-%E5%AE%9E%E6%88%98%E6%A1%88%E4%BE%8B"><span class="toc-number">4.</span> <span class="toc-text">spark-core 实战案例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Pycharm%E7%BC%96%E5%86%99spark%E4%BB%A3%E7%A0%81%E7%8E%AF%E5%A2%83%E9%85%8D%E7%BD%AE"><span class="toc-number">4.1.</span> <span class="toc-text">Pycharm编写spark代码环境配置</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%88%A9%E7%94%A8PyCharm%E7%BC%96%E5%86%99spark-wordcount%E7%A8%8B%E5%BA%8F"><span class="toc-number">4.2.</span> <span class="toc-text">利用PyCharm编写spark wordcount程序</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87spark%E5%AE%9E%E7%8E%B0%E7%82%B9%E5%87%BB%E6%B5%81%E6%97%A5%E5%BF%97%E5%88%86%E6%9E%90"><span class="toc-number">4.3.</span> <span class="toc-text">通过spark实现点击流日志分析</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-core%E5%AE%9E%E6%88%98"><span class="toc-number">5.</span> <span class="toc-text">spark-core实战</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E9%80%9A%E8%BF%87spark%E5%AE%9E%E7%8E%B0ip%E5%9C%B0%E5%9D%80%E6%9F%A5%E8%AF%A2"><span class="toc-number">5.1.</span> <span class="toc-text">通过spark实现ip地址查询</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#spark-%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%E8%A1%A5%E5%85%85"><span class="toc-number">6.</span> <span class="toc-text">spark 相关概念补充</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#spark%E7%9A%84%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2"><span class="toc-number">6.1.</span> <span class="toc-text">spark的安装部署</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#spark-%E9%9B%86%E7%BE%A4%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5"><span class="toc-number">6.2.</span> <span class="toc-text">spark 集群相关概念</span></a></li></ol></li></ol></div></div></div></div></main><footer id="footer" style="background-image: url('/2022/01/18/5.5-Spark-core/cover.jpeg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025  <i id="heartbeat" class="fa fas fa-heartbeat"></i> 贪钱算法还我头发</div><div class="framework-info"><span>Framework </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>Theme </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div></div><head><link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/HCLonely/images@master/others/heartbeat.min.css"></head></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="readmode" type="button" title="Read Mode"><i class="fas fa-book-open"></i></button><button id="darkmode" type="button" title="Switch Between Light And Dark Mode"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="Toggle between single-column and double-column"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside_config" type="button" title="Setting"><i class="fas fa-cog fa-spin"></i></button><button class="close" id="mobile-toc-button" type="button" title="Table Of Contents"><i class="fas fa-list-ul"></i></button><a id="to_comment" href="#post-comment" title="Scroll To Comments"><i class="fas fa-comments"></i></a><button id="go-up" type="button" title="Back To Top"><i class="fas fa-arrow-up"></i></button></div></div><div id="local-search"><div class="search-dialog"><div class="search-dialog__title" id="local-search-title">Local search</div><div id="local-input-panel"><div id="local-search-input"><div class="local-search-box"><input class="local-search-box--input" placeholder="Search for Posts" type="text"/></div></div></div><hr/><div id="local-search-results"></div><span class="search-close-button"><i class="fas fa-times"></i></span></div><div id="search-mask"></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="/js/search/local-search.js"></script><div class="js-pjax"><script>if (!window.MathJax) {
  window.MathJax = {
    tex: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]],
      tags: 'ams'
    },
    chtml: {
      scale: 1.2
    },
    options: {
      renderActions: {
        findScript: [10, doc => {
          for (const node of document.querySelectorAll('script[type^="math/tex"]')) {
            const display = !!node.type.match(/; *mode=display/)
            const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display)
            const text = document.createTextNode('')
            node.parentNode.replaceChild(text, node)
            math.start = {node: text, delim: '', n: 0}
            math.end = {node: text, delim: '', n: 0}
            doc.math.push(math)
          }
        }, ''],
        insertScript: [200, () => {
          document.querySelectorAll('mjx-container:not\([display]\)').forEach(node => {
            const target = node.parentNode
            if (target.nodeName.toLowerCase() === 'li') {
              target.parentNode.classList.add('has-jax')
            } else {
              target.classList.add('has-jax')
            }
          });
        }, '', false]
      }
    }
  }
  
  const script = document.createElement('script')
  script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js'
  script.id = 'MathJax-script'
  script.async = true
  document.head.appendChild(script)
} else {
  MathJax.startup.document.state(0)
  MathJax.texReset()
  MathJax.typeset()
}</script><script>function addGitalkSource () {
  const ele = document.createElement('link')
  ele.rel = 'stylesheet'
  ele.href= 'https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css'
  document.getElementsByTagName('head')[0].appendChild(ele)
}

function loadGitalk () {
  function initGitalk () {
    var gitalk = new Gitalk(Object.assign({
      clientID: '1b0c10ce649501ea4a72',
      clientSecret: '741b5e861137e3d5a482bba272c8201b78da6cb0',
      repo: 'xfliu1998.github.io',
      owner: 'xfliu1998',
      admin: ['xfliu1998'],
      id: '847854bd2a47a6409927d4b427d61086',
      language: 'en',
      perPage: 10,
      distractionFreeMode: false,
      pagerDirection: 'last',
      createIssueManually: true,
      updateCountCallback: commentCount
    },null))

    gitalk.render('gitalk-container')
  }

  if (typeof Gitalk === 'function') initGitalk()
  else {
    addGitalkSource()
    getScript('https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js').then(initGitalk)
  }
}

function commentCount(n){
  let isCommentCount = document.querySelector('#post-meta .gitalk-comment-count')
  if (isCommentCount) {
    isCommentCount.innerHTML= n
  }
}

if ('Gitalk' === 'Gitalk' || !false) {
  if (false) btf.loadComment(document.getElementById('gitalk-container'), loadGitalk)
  else loadGitalk()
} else {
  function loadOtherComment () {
    loadGitalk()
  }
}</script></div><script src="/js/script.js?v1"></script><script src="https://cdn.staticfile.org/jquery/3.6.3/jquery.min.js"></script><script async data-pjax src="https://cdn.wpon.cn/2022-sucai/Gold-ingot.js"></script><script async data-pjax src="/js/newYear.js"></script><script async src="//at.alicdn.com/t/font_2264842_b004iy0kk2b.js"></script><script src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/activate-power-mode.min.js"></script><script>POWERMODE.colorful = true;
POWERMODE.shake = true;
POWERMODE.mobile = false;
document.body.addEventListener('input', POWERMODE);
</script><script id="click-heart" src="https://cdn.jsdelivr.net/npm/butterfly-extsrc@1/dist/click-heart.min.js" async="async" mobile="false"></script><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div><!-- hexo injector body_end start --> <script data-pjax>if(document.getElementById('recent-posts') && (location.pathname ==='all'|| 'all' ==='all')){
    var parent = document.getElementById('recent-posts');
    var child = '<div class="recent-post-item" style="width:100%;height: auto"><div id="catalog_magnet"><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Machine-Learning-and-Deep-Learning/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">👩‍💻 机器学习与深度学习 (14)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Structures-and-Algorithms/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">😼 数据结构与算法 (16)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Search-Advertisement-Recommendation-Causal/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">🗂️ 搜索/广告/推荐/因果 (10)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Data-Analysis-and-Processing/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📒 数据分析与处理 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Reading-Notes/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">📚 阅读笔记 (7)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><div class="magnet_item"><a class="magnet_link" href="https://xfliu1998.github.io/categories/Daily/"><div class="magnet_link_context" style=""><span style="font-weight:500;flex:1">💡 日常随笔 (4)</span><span style="padding:0px 4px;border-radius: 8px;"><i class="fas fa-arrow-circle-right"></i></span></div></a></div><a class="magnet_link_more"  href="https://xfliu1998.github.io/categories" style="flex:1;text-align: center;margin-bottom: 10px;">查看更多...</a></div></div>';
    console.log('已挂载magnet')
    parent.insertAdjacentHTML("afterbegin",child)}
     </script><style>#catalog_magnet{flex-wrap: wrap;display: flex;width:100%;justify-content:space-between;padding: 10px 10px 0 10px;align-content: flex-start;}.magnet_item{flex-basis: calc(50% - 5px);background: #f2f2f2;margin-bottom: 10px;border-radius: 8px;transition: all 0.2s ease-in-out;}.magnet_item:hover{background: #b30070}.magnet_link_more{color:#555}.magnet_link{color:black}.magnet_link:hover{color:white}@media screen and (max-width: 600px) {.magnet_item {flex-basis: 100%;}}.magnet_link_context{display:flex;padding: 10px;font-size:16px;transition: all 0.2s ease-in-out;}.magnet_link_context:hover{padding: 10px 20px;}</style>
    <style></style><script data-pjax>
  function butterfly_swiper_injector_config(){
    var parent_div_git = document.getElementById('recent-posts');
    var item_html = '<div class="recent-post-item" style="height: auto;width: 100%"><div class="blog-slider swiper-container-fade swiper-container-horizontal" id="swiper_container"><div class="blog-slider__wrp swiper-wrapper" style="transition-duration: 0ms;"><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-NLP/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Reading-about-NLP/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-NLP/" alt="">Papers Reading about NLP</a><div class="blog-slider__text">自然语言处理论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-NLP/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/10/01/9-3D-Construction/" alt=""><img width="48" height="48" src="2022/10/01/9-3D-Construction/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-10-01</span><a class="blog-slider__title" href="2022/10/01/9-3D-Construction/" alt="">3D Construction</a><div class="blog-slider__text">三维重建基础</div><a class="blog-slider__button" href="2022/10/01/9-3D-Construction/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/01/11/SfM-SLAM/" alt=""><img width="48" height="48" src="2023/01/11/SfM-SLAM/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-01-11</span><a class="blog-slider__title" href="2023/01/11/SfM-SLAM/" alt="">SfM &amp; SLAM</a><div class="blog-slider__text">SfM和SLAM系统</div><a class="blog-slider__button" href="2023/01/11/SfM-SLAM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/03/25/Papers-Ideas/" alt=""><img width="48" height="48" src="2023/03/25/Papers-Ideas/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-03-25</span><a class="blog-slider__title" href="2023/03/25/Papers-Ideas/" alt="">Papers Ideas</a><div class="blog-slider__text">大模型时代下的科研思路</div><a class="blog-slider__button" href="2023/03/25/Papers-Ideas/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Summary/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Summary/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Summary/" alt="">Papers Summary</a><div class="blog-slider__text">论文总结笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Summary/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/09/17/Papers-Reading-about-CV/" alt=""><img width="48" height="48" src="2022/09/17/Papers-Reading-about-CV/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-09-17</span><a class="blog-slider__title" href="2022/09/17/Papers-Reading-about-CV/" alt="">Papers Reading about CV</a><div class="blog-slider__text">计算机视觉论文阅读笔记</div><a class="blog-slider__button" href="2022/09/17/Papers-Reading-about-CV/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2023/06/05/Interview-Experience/" alt=""><img width="48" height="48" src="2023/06/05/Interview-Experience/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2023-06-05</span><a class="blog-slider__title" href="2023/06/05/Interview-Experience/" alt="">Interview Experience</a><div class="blog-slider__text">面经八股</div><a class="blog-slider__button" href="2023/06/05/Interview-Experience/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2024/08/16/Papers-Reading-about-LLM/" alt=""><img width="48" height="48" src="2024/08/16/Papers-Reading-about-LLM/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2024-08-16</span><a class="blog-slider__title" href="2024/08/16/Papers-Reading-about-LLM/" alt="">Papers Reading about LLM</a><div class="blog-slider__text">LLM论文阅读笔记</div><a class="blog-slider__button" href="2024/08/16/Papers-Reading-about-LLM/" alt="">详情   </a></div></div><div class="blog-slider__item swiper-slide" style="width: 750px; opacity: 1; transform: translate3d(0px, 0px, 0px); transition-duration: 0ms;"><a class="blog-slider__img" href="2022/01/21/Learning-Framework/" alt=""><img width="48" height="48" src="2022/01/21/Learning-Framework/cover.jpeg" alt="" onerror="this.src=https://unpkg.zhimg.com/akilar-candyassets/image/loading.gif; this.onerror = null;"/></a><div class="blog-slider__content"><span class="blog-slider__code">2022-01-21</span><a class="blog-slider__title" href="2022/01/21/Learning-Framework/" alt="">学习大纲</a><div class="blog-slider__text">目录</div><a class="blog-slider__button" href="2022/01/21/Learning-Framework/" alt="">详情   </a></div></div></div><div class="blog-slider__pagination swiper-pagination-clickable swiper-pagination-bullets"></div></div></div>';
    console.log('已挂载butterfly_swiper')
    parent_div_git.insertAdjacentHTML("afterbegin",item_html)
    }
  var elist = 'undefined'.split(',');
  var cpage = location.pathname;
  var epage = '/';
  var flag = 0;

  for (var i=0;i<elist.length;i++){
    if (cpage.includes(elist[i])){
      flag++;
    }
  }

  if ((epage ==='all')&&(flag == 0)){
    butterfly_swiper_injector_config();
  }
  else if (epage === cpage){
    butterfly_swiper_injector_config();
  }
  </script><script defer src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper.min.js"></script><script defer data-pjax src="https://npm.elemecdn.com/hexo-butterfly-swiper/lib/swiper_init.js"></script><script data-pjax src="https://npm.elemecdn.com/hexo-filter-gitcalendar/lib/gitcalendar.js"></script><script data-pjax>
  function gitcalendar_injector_config(){
      var parent_div_git = document.getElementById('recent-posts');
      var item_html = '<container><style>#git_container{min-height: 280px}@media screen and (max-width:650px) {#git_container{min-height: 0px}}</style><div id="git_loading" style="width:10%;height:100%;margin:0 auto;display: block;"><svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 50 50" style="enable-background:new 0 0 50 50" xml:space="preserve"><path fill="#d0d0d0" d="M25.251,6.461c-10.318,0-18.683,8.365-18.683,18.683h4.068c0-8.071,6.543-14.615,14.615-14.615V6.461z" transform="rotate(275.098 25 25)"><animatetransform attributeType="xml" attributeName="transform" type="rotate" from="0 25 25" to="360 25 25" dur="0.6s" repeatCount="indefinite"></animatetransform></path></svg><style>#git_container{display: none;}</style></div><div id="git_container"></div></container>';
      parent_div_git.insertAdjacentHTML("afterbegin",item_html)
      console.log('已挂载gitcalendar')
      }

    if( document.getElementById('recent-posts') && (location.pathname ==='/'|| '/' ==='all')){
        gitcalendar_injector_config()
        GitCalendarInit("https://gitcalendar.fomal.cc/api?xfliu1998",['#d9e0df', '#c6e0dc', '#a8dcd4', '#9adcd2', '#89ded1', '#77e0d0', '#5fdecb', '#47dcc6', '#39dcc3', '#1fdabe', '#00dab9'],'xfliu1998')
    }
  </script><!-- hexo injector body_end end --><script src="/live2dw/lib/L2Dwidget.min.js?094cbace49a39548bed64abff5988b05"></script><script>L2Dwidget.init({"model":{"jsonPath":"/live2dw/assets/haruto.model.json"},"display":{"position":"right","width":150,"height":300},"mobile":{"show":false},"log":false,"pluginJsPath":"lib/","pluginModelPath":"assets/","pluginRootPath":"live2dw/","tagMode":false});</script></body></html>